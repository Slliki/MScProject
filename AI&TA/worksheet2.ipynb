{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This worksheet covers the three supervised learning algorithms we looked at: k-nearest neighbours, linear regression, and the naive Bayes classifier. Similar to last week, you will do some work implementing your own versions of these algorithms, to ensure that you understand the details of them. You will also compare them with the implementations in scikit-learn to test your implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries\n",
    "Import key packages: NumPy, matplotlib, and any others that you prefer to work with. In general, when writing code, you will put all your import statements at the top. However, for these worksheets we will import as we go along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T09:13:23.807702500Z",
     "start_time": "2024-01-22T09:13:23.766835700Z"
    }
   },
   "outputs": [],
   "source": [
    "#TODO: import NumPy and matplotlib here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: k-nearest neighbours classification\n",
    "In this question we will use the k-nearest neighbours algorithm to make predictions on the breast cancer Wisconsin dataset. This is a classification problem where the aim is to classify instances as either being malignant or benign based on the following 10 features:\n",
    "1. radius (mean of distances from center to points on the perimeter)\n",
    "2. texture (standard deviation of gray-scale values)\n",
    "3. perimeter\n",
    "4. area\n",
    "5. smoothness (local variation in radius lengths)\n",
    "6. compactness (perimeter squared/ area âˆ’1)\n",
    "7. concavity (severity of concave portions of the contour)\n",
    "8. concave points (number of concave portions of the contour)\n",
    "9. symmetry\n",
    "10. fractal dimension (â€˜coastline approximationâ€™ âˆ’1)\n",
    "\n",
    "In this question you will:\n",
    "    (a) download the dataset from sklearn and store the data and targets in suitable variables, \n",
    "    (b) separate your data into a training and test split, \n",
    "    (c) (Optional) write your own function to implement k-nearest neighbours, \n",
    "    (d) (Optional) check your implementation with that of sklearn. \n",
    "    (e) select the most appropriate value of $k$ using cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (a)\n",
    "Import the package `datasets` from `sklearn` and then load the breast cancer dataset (function is `load_breast_cancer()`). Save the data into a variable `X` and the targets into a variable `Y`. \n",
    "Take a look at the data in `X`. How many datapoints are there? How many features does each datapoint have? (Hint: use `np.shape`).\n",
    "Take a look at the targets. Is this suitable for a classification algorithm or a regression algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T09:13:23.863820100Z",
     "start_time": "2024-01-22T09:13:23.775806100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import suitable packages, load the dataset, and save data and targets into variables X and Y\n",
    "# import packages\n",
    "##TODO##\n",
    "\n",
    "# load dataset and save data and targets into X and Y\n",
    "##TODO##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (b)\n",
    "\n",
    "Use the function `train_test_split` from `sklearn.model_selection` to split your data into a training set and a held-out test set. Use a test set that is 0.2 of the original dataset. Set the parameter `random_state` to 10 to help with replication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T09:13:23.865932400Z",
     "start_time": "2024-01-22T09:13:23.790756300Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1551998961.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  Cell \u001B[1;32mIn[3], line 4\u001B[1;36m\u001B[0m\n\u001B[1;33m    Xtr, Xtest, Ytr, Ytest = ##TODO##\u001B[0m\n\u001B[1;37m                             ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Import the package train_test_split from sklearn.model_selection.\n",
    "# Split the dataset into Xtr, Xtest, Ytr, Ytest. Xtest and Ytest will form your held-out\n",
    "# test set. You will later split Xtr and Ytr into training and validation sets.\n",
    "Xtr, Xtest, Ytr, Ytest = ##TODO##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Part (c) \n",
    "Recall from the lecture that the k-nearest neighbours algorithm runs as follows:\n",
    "\n",
    "Training step: Simply store the dataset\n",
    "\n",
    "Prediction step: Given a datapoint $\\vec{x}$:\n",
    " - **Find** the k datapoints $(\\vec{x}_i, y_i)$ where the distance from $\\vec{x}$ to $\\vec{x}_i$ is smallest\n",
    " - **Return** the majority class from the $y_i$\n",
    "   \n",
    " \n",
    "What, if anything, do you need to do for the training step?\n",
    "\n",
    "Write function(s) to implement the k-nearest neighbours prediction step. You may wish to break the procedure down into two functions `predict_datapoint` that makes a prediction for one datapoint and `predict_data` that loops over the whole dataset.\n",
    "\n",
    "To select the majority class from the nearest neighbours, you can use the function `scipy.stats.mode()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-22T09:13:23.807702500Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "# Write function(s) to implement the prediction step in k-nearest neighbours. \n",
    "# You can use the suggested structure below if desired.\n",
    "\n",
    "\n",
    "# predict_datapoint takes 4 arguments. pt (type: numpy array) is the datapoint we are making a prediction about, \n",
    "# Xtrain and Ytrain (numpy arrays) are training data and targets, k (int) is the number of neighbours.\n",
    "# Returns an integer which is the predicted class for pt\n",
    "def predict_datapoint(pt, Xtrain, Ytrain, k):\n",
    "    # For each datapoint in Xtrain, calculate the distance to pt and store\n",
    "    ##TODO##\n",
    "        \n",
    "    # Sort the list of distances (hint: use np.argsort)\n",
    "    ##TODO##\n",
    "    \n",
    "    # obtain the classes (in Ytrain) of the datapoints with the smallest distance to pt\n",
    "    ##TODO##\n",
    "    \n",
    "    # return the mode of the classes\n",
    "    ##TODO##\n",
    "\n",
    "# predict_data takes 4 arguments: the test data Xtst (numpy array), the training data Xtrain (numpy array),\n",
    "# the training targets Ytrain (numpy array), and the number of neighbours k (int, default = 3). \n",
    "# Returns: predictions (array of int) for each point in Xtst\n",
    "def predict_data(Xtst, Xtrain, Ytrain, k=3):\n",
    "    #Loop over the datapoints in Xtst and store the prediction for that datapoint\n",
    "    ##TODO##\n",
    "    # Return the predictions\n",
    "    ##TODO##\n",
    "\n",
    "# Predict values for the TRAINING data (we will not look at the test set yet)\n",
    "##TODO##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (d)\n",
    "Now we can compare your implementation with the sklearn implementation (you should get the same results). Firstly import the classfifier `KNeighborsClassifier` from `sklearn.neighbors`. Instantiate the classifier with the same number of neighbours that you used previously. Fit the model and make a prediction on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-22T09:13:23.808698200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import KNeighborClassifier\n",
    "##TODO##\n",
    "\n",
    "# Instantiate the classifier with 3 neighbors\n",
    "##TODO##\n",
    "\n",
    "#Fit the classifier on the training data\n",
    "##TODO##\n",
    "\n",
    "#Make a prediction on the training data\n",
    "##TODO##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check whether your predictions are the same as the predictions from `KNeighborsClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-22T09:13:23.809694700Z"
    }
   },
   "outputs": [],
   "source": [
    "##TODO##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the built in metrics in sklearn to calculate the accuracy of your classifier on the TRAINING set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T09:13:23.999067Z",
     "start_time": "2024-01-22T09:13:23.809694700Z"
    }
   },
   "outputs": [],
   "source": [
    "##TODO## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part(e) Using cross-validation for model selection\n",
    "k-nearest neighbours has the parameter $k$, and we need to decide which is the best value of $k$ to use. Last week we talked about using cross-validation for model selection.\n",
    "\n",
    "We will use cross-validation on our training set to select the best value of $k$, in a range from 1 to 30.\n",
    "\n",
    "NB: use sklearn's version of k-NN rather than yours, since unless you have optimised yours it is probably too slow.\n",
    "\n",
    "Since we are using cross-validation for model selection we will cross-validate on the training set only.\n",
    "\n",
    "Procedure:\n",
    "        \n",
    " 1. Import `KFold` from `sklearn.model_selection`  \n",
    " 1. Instantiate `KFold` with 5 splits. Set the parameter `random_state` to help you reproduce your results if needed.\n",
    " 1. Set a variable `max_k` to 30  \n",
    " 1. Inititalise two variables to store the training accuracies and validation accuracies (these need to store max_k\\*5 accuracies)  \n",
    " 1. Loop over the values of k:  \n",
    "    1. Instantiate a k-nn classifier (Use the sklearn classifier) with the current value of k  \n",
    "    1. Loop over the cross-validation splits:  \n",
    "       1. fit the model on the current split of data  \n",
    "       1. make predictions  \n",
    "       1. calculate training and validation accuracy and store  \n",
    " 6. Calculate the mean training and validation accuracies across splits for each $k$\n",
    "\n",
    "Plot the mean training and validation accuracies. Which value of $k$ will you use? Why?\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T09:13:24.252693400Z",
     "start_time": "2024-01-22T09:13:23.890801700Z"
    }
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (951088511.py, line 39)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  Cell \u001B[1;32mIn[5], line 39\u001B[1;36m\u001B[0m\n\u001B[1;33m    ##TODO##\u001B[0m\n\u001B[1;37m            ^\u001B[0m\n\u001B[1;31mIndentationError\u001B[0m\u001B[1;31m:\u001B[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# Use cross-validation to select the value of k\n",
    "# You can use the structure below if desired\n",
    "\n",
    "# Import KFold from sklearn.model_selection\n",
    "##TODO##\n",
    "\n",
    "# Instantiate KFold with 5 splits. \n",
    "# Set the parameter random_state to help you reproduce your results if needed.\n",
    "##TODO##\n",
    "\n",
    "# Set a variable max_k to 30 \n",
    "##TODO##\n",
    "\n",
    "# Inititalise two variables to store the \n",
    "# training accuracies and validation accuracies \n",
    "# (these need to store max_k*5 accuracies) \n",
    "##TODO##\n",
    "\n",
    "# Loop over the values of k: \n",
    "for k in range(max_k):\n",
    "    \n",
    "    # Instantiate a k-nn classifier (Use the sklearn classifier)\n",
    "    # with the current value of k \n",
    "    ##TODO##\n",
    "    \n",
    "    # Loop over the cross-validation splits: \n",
    "    ##TODO##\n",
    "    \n",
    "        # fit the model on the current split of data \n",
    "        ##TODO##\n",
    "        \n",
    "        # make predictions \n",
    "        ##TODO##\n",
    "        \n",
    "        # calculate training and validation accuracy and store \n",
    "        ##TODO##\n",
    "        \n",
    "# Calculate the mean training and validation accuracies across splits for each ð‘˜\n",
    "##TODO##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T09:13:24.443721200Z",
     "start_time": "2024-01-22T09:13:24.253690800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the mean training and validation accuracies against each value of k. Which value of ð‘˜ will you use? Why?\n",
    "##TODO##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The naive Bayes classifier\n",
    "\n",
    "Recall from the lecture notes that the naive Bayes classifier works as follows. We are trying to approximate an unknown function $$f:V \\rightarrow \\mathcal{O}$$\n",
    "where $V$ is our feature space and our output space $\\mathcal{O} = \\{c_1, c_2, ... c_K\\}$ is a finite set of classes.\n",
    "\n",
    "The naive Bayes classifier does this by building a model that assigns the class label $\\hat{y} = c_k$ as follows:\n",
    "$$\n",
    "\\hat{y} = \\text{argmax}_k p(c_k)\\prod_i p(x_i| c_k)\n",
    "$$\n",
    "i.e., the $k$ that maximises this quantity.\n",
    "\n",
    "In practice, multiplying all the $p(x_i| c_k)$ together is going to give some very small values. Therefore, we can take the log to make it easier to compute:\n",
    "\\begin{align}\n",
    "\\hat{y} &= \\text{argmax}_k p(c_k)\\prod_i p(x_i| c_k)= \\text{argmax}_k log(p(c_k)\\prod_i p(x_i| c_k))\\\\\n",
    "&=\\text{argmax}_k log(p(c_k)) + \\sum_i log(p(x_i|c_k))\n",
    "\\end{align}\n",
    "\n",
    "If we choose that $p(x_i|c_k)$ is given by a normal distribution with mean $\\mu_k$ and variance $\\sigma_k^2$, then we obtain the following expression:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{y} &=\\text{argmax}_k \\log(p(c_k)) + \\sum_i \\log(p(x_i|c_k))\\\\\n",
    "&= \\text{argmax}_k \\log(p(c_k)) + \\sum_i \\log\\left(\\frac{1}{\\sigma_k\\sqrt{2\\pi}} exp\\left(\\frac{-(x-\\mu_k)^2}{2\\sigma_k}\\right)\\right)\\\\\n",
    "&= \\text{argmax}_k \\log(p(c_k)) - \\sum_i \\log\\left(\\sigma_k\\sqrt{2\\pi}\\right) - \\sum_i\\left(\\frac{(x-\\mu_k)^2}{2\\sigma_k}\\right) \\quad \\text{ log-likelihood}\n",
    "\\end{align}\n",
    "\n",
    "Expressing the values in terms of these sums means that they do not get so small, and it is less likely that there will be errors at the machine precision level.\n",
    "\n",
    "\n",
    "How do we implement this in practice? We assume that each probability $p(x_i| c_k)$ is given by some distribution, and then given a datapoint $\\vec{x}$, we plug the value into the equation for the distribution.\n",
    "\n",
    "In this question you will: \n",
    "    (a) (Optional) implement your own version of the Gaussian naive Bayes classifier, \n",
    "    (b) (Optional) check your classifier against the implementation in sci-kit learn, \n",
    "    (c) compare the accuracy of the scikit-learn naive Bayes classifier with the accuracy of the k-nearest neighbours classifier, and \n",
    "    (d) run cross-validation to verify whether the kNN classfier or the Gaussian naive Bayes classifier performs better on this dataset.\n",
    "\n",
    "## (Optional) Part (a) Implementing Gaussian naive Bayes\n",
    "For this question we will make the assumption that each feature is described by a normal (also called Gaussian) distribution. The procedure is as follows:\n",
    "1. Divide the training data by class\n",
    "2. Calculate mean and standard deviation per class and per feature\n",
    "4. For each datapoint in the validation set, calculate the log-likelihood for each class and for each feature (Hint: use the function `scipy.stats.norm.logpdf`)\n",
    "5. Combine these values together with the probability of the class according to the log-likelihood equation above\n",
    "6. Choose the class with the highest value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T09:13:24.785388Z",
     "start_time": "2024-01-22T09:13:24.352684300Z"
    }
   },
   "outputs": [],
   "source": [
    "##TODO##\n",
    "# Write your own implementation of naive Bayes applied to the breast cancer dataset.\n",
    "\n",
    "# If you wish you can follow the structure below\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Split the training data Xtr into training and validation sets with an 80:20 split. \n",
    "# Set the random state to help with reproducibility\n",
    "##TODO##\n",
    "\n",
    "#Separate the training set into classes, so you have one set of data for each class\n",
    "##TODO##\n",
    "\n",
    "# Calculate the means and standard deviations for each class, for each feature. \n",
    "# There are 30 features in the dataset, so you should have a 30-dimensional \n",
    "# array of means for each class and a 30-dimensional array of standard deviations\n",
    "# for each class. Remember that you can take the average across rows or columns of \n",
    "# a matrix by specifying axis = 1 or axis = 0\n",
    "##TODO##\n",
    "\n",
    "# Calculate the prior probability p(c_i) for each class\n",
    "##TODO##\n",
    "\n",
    "# Calculate the log-likelihood of each class for each datapoint in the validation set\n",
    "# Hint: you can use the function scipy.stats.norm.logpdf to help with this\n",
    "##TODO##\n",
    "\n",
    "# Your predicted class is 0 if class 0 has the highest log-likelihood, and 1 if class 1 \n",
    "# has the highest log-likelihood\n",
    "##TODO##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Part (b) Checking results\n",
    "We now compare our results with the sklearn implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T09:13:24.800945100Z",
     "start_time": "2024-01-22T09:13:24.727361500Z"
    }
   },
   "outputs": [],
   "source": [
    "##Import the classifier GaussianNB from sklearn.naive_bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# Instantiate the classifier (use the parameter var_smoothing=0.0),\n",
    "# fit, and predict the classes\n",
    "##TODO##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T09:13:24.826487100Z",
     "start_time": "2024-01-22T09:13:24.787722500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compare your predicted classes with those of the sklearn implementation.\n",
    "# If they are not identical, this may be due to some differences in parameter setting. \n",
    "# They should be almost all the same, however.\n",
    "##TODO##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (c) Comparing k-nearest neighbours and Gaussian naive Bayes\n",
    "Now, using the sklearn implementations of k-nearest neighbours and naive Bayes, retrain the naive Bayes classifier using the original training set `Xtr`, `Ytr`.\n",
    "Also retrain the k-nearest neighbours classifier using `Xtr` and `Ytr`. Use the value of $k$ that you decided on using cross-validation.\n",
    "\n",
    "Compute the accuracy of the naive Bayes classifier over the training set and the held-out test set.\n",
    "\n",
    "Compare with the accuracy of the k-nearest neighbours classifier on each set.\n",
    "\n",
    "Is it clear which classifier is the best on this dataset? why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false,
    "ExecuteTime": {
     "end_time": "2024-01-22T09:13:24.899690400Z",
     "start_time": "2024-01-22T09:13:24.804348900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate the knn classifer with your chosen value of k\n",
    "##TODO##\n",
    "\n",
    "# Fit the Gaussian naive Bayes classifier and the knn classifier on Xtr, Ytr\n",
    "##TODO##\n",
    "\n",
    "# Make predictions for the training set and the test set\n",
    "##TODO##\n",
    "\n",
    "# Take a look at the accuracy scores\n",
    "##TODO##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (d) Using cross-validation for statistical validation\n",
    "Earlier we used cross-validation to select the model parameters we would be using. We can also use it another way: to provide statistical information about which model is best. We will set up cross-validation on the whole dataset, with 10 folds.\n",
    "\n",
    " - Compute the accuracy for each model on the test set on each fold.\n",
    " - Calculate the mean accuracy across folds. Which model performs best?\n",
    " - Make a box-plot of the spread of scores of each model. Is there a clear difference between model performance?\n",
    " - Perform a paired t-test on the accuracy scores. What can you conclude about the performance of the two models?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T09:13:24.905970200Z",
     "start_time": "2024-01-22T09:13:24.818927600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set up a k-fold cross-validation with 10 folds\n",
    "##TODO##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T09:13:24.931323Z",
     "start_time": "2024-01-22T09:13:24.834305200Z"
    }
   },
   "outputs": [],
   "source": [
    "# For each fold, fit each model on the training data \n",
    "# and compute accuracy on the test data.\n",
    "##TODO##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T09:13:24.932387500Z",
     "start_time": "2024-01-22T09:13:24.850135800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute the mean and standard devation of the accuracies for each model.\n",
    "# Does one model perform better?\n",
    "##TODO##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T09:13:24.969989200Z",
     "start_time": "2024-01-22T09:13:24.865443900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make a boxplot of the accuracy scores. (Use plt.boxplot). \n",
    "# Is there a clear difference between the models?\n",
    "##TODO##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T09:13:25.013408100Z",
     "start_time": "2024-01-22T09:13:24.881945700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Perform a paired t-test (you can use the function scipy.stats.ttest_rel). \n",
    "# What do you conclude about the performance of the two models?\n",
    "##TODO##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "In linear regression we make the assumption that the data $(x_i, y_i)$ can be modelled by a function of the form\n",
    "$$ \\hat{y_i} = f(\\vec{x}_i)= \\sum_j a_j x_{ij}  + b_i$$\n",
    "\n",
    "Recall that we can express this in a matrix format by:\n",
    "$$ \\hat{\\vec{y}} = f(X)= X\\Theta$$\n",
    "\n",
    "where \n",
    "$$ X=\\begin{pmatrix}\n",
    "x_{1,1} & x_{1,2} & \\ldots & x_{1,n} &1 \\\\\n",
    "\\vdots & \\vdots & \\ldots & \\vdots & \\vdots \\\\\n",
    "x_{N,1} & x_{N,2} & \\ldots & x_{N,n} & 1\n",
    "\\end{pmatrix}, \\quad \\vec{y}=\\begin{pmatrix} y_1 \\\\ \\vdots \\\\y_N \\end{pmatrix}, \\quad \\Theta=\\begin{pmatrix} a_1 \\\\ \\vdots \\\\a_n\\\\b \\end{pmatrix}$$\n",
    "\n",
    "We saw in lectures that the optimal value of $\\Theta$ is given by setting\n",
    "$$ \\Theta = (X^T X)^{-1} X^T \\vec{y}$$\n",
    "\n",
    "The quantity $(X^T X)^{-1} X^T$ is called the psuedoinverse of X, and can be computed using the function `np.linalg.pinv`.\n",
    "\n",
    "We will (a) perform a linear regression on the diabetes dataset. You can load this dataset using the function `load_diabetes` from `sklearn.datasets`. (b) compute the mean squared error and the R^2, and (c) compare your results with the built in function in sklearn (`sklearn.linear_model.LinearRegresion()`). You should get the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T09:13:25.044937Z",
     "start_time": "2024-01-22T09:13:24.897160900Z"
    }
   },
   "outputs": [],
   "source": [
    "# import statments here\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (a) Implementing linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T09:13:25.045951400Z",
     "start_time": "2024-01-22T09:13:24.976012700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the diabetes dataset \n",
    "##TODO##\n",
    "\n",
    "# Split the dataset into training and test, using test_size=0.2\n",
    "##TODO##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T09:13:25.114461Z",
     "start_time": "2024-01-22T09:13:24.991689600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add a column of ones to Xtrain and Xtest for the intercept term\n",
    "##TODO##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T09:13:25.117450400Z",
     "start_time": "2024-01-22T09:13:25.006721400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the value of the coefficients theta. You can use the function np.linalg.pinv\n",
    "##TODO##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (b) Computing performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T09:13:25.136950900Z",
     "start_time": "2024-01-22T09:13:25.024418400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make a prediction on the test set by applying the coefficients theta to the test set\n",
    "##TODO##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T09:13:25.153403Z",
     "start_time": "2024-01-22T09:13:25.040790500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the mean squared error and the R^2. \n",
    "# You can use the built in functions from sklearn\n",
    "##TODO##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part (c) Checking results\n",
    "Compare your results with the built in function `sklearn.linear_model.LinearRegression()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T09:13:25.155455600Z",
     "start_time": "2024-01-22T09:13:25.054831500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Instantiate the linear regression\n",
    "##TODO##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T09:13:25.174446400Z",
     "start_time": "2024-01-22T09:13:25.072072100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit the model and make a prediction on the test set. Compare with your implementation\n",
    "##TODO##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise the perfomance of the regression by plotting your predicted values vs target values on a scatter plot, and drawing a line y=x. If all predictions were perfect, the predicted values would lie on the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T09:13:25.256097200Z",
     "start_time": "2024-01-22T09:13:25.140051100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot predicted values vs target values on a scatter plot, and drawing a line y=x\n",
    "## TODO##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Polynomial regression\n",
    "The term 'linear' in linear regression refers only to the coefficients $\\theta$. We can in fact compute polynomial terms in the data and perform linear regression over this extended dataset to get a better fit to the data.\n",
    "\n",
    "To compute polynomial terms in the data automatically, you can use the class `sklearn.preprocessing.PolynomialFeatures`. To find out how to use it, look at the guidance (you can type `help(PolynomialFeatures)` once you have imported it).\n",
    "\n",
    "The following small dataset (in the cell below) gives a relationship between temperature and yield for an experiment. Use cross-validation to select the degree of the polynomial that best fits this data.\n",
    "\n",
    "Plot the mean squared error against degree on the training set and on the validation set. Which degree of polynomial best fits this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T09:13:25.483788Z",
     "start_time": "2024-01-22T09:13:25.209792800Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[25], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Data\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241m.\u001B[39marray([\u001B[38;5;241m50\u001B[39m,\u001B[38;5;241m50\u001B[39m,\u001B[38;5;241m50\u001B[39m,\u001B[38;5;241m70\u001B[39m,\u001B[38;5;241m70\u001B[39m,\u001B[38;5;241m70\u001B[39m,\u001B[38;5;241m80\u001B[39m,\u001B[38;5;241m80\u001B[39m,\u001B[38;5;241m80\u001B[39m,\u001B[38;5;241m90\u001B[39m,\u001B[38;5;241m90\u001B[39m,\u001B[38;5;241m90\u001B[39m,\u001B[38;5;241m100\u001B[39m,\u001B[38;5;241m100\u001B[39m,\u001B[38;5;241m100\u001B[39m])\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m      3\u001B[0m y \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray([\u001B[38;5;241m3.3\u001B[39m,\u001B[38;5;241m2.8\u001B[39m,\u001B[38;5;241m2.9\u001B[39m,\u001B[38;5;241m2.3\u001B[39m,\u001B[38;5;241m2.6\u001B[39m,\u001B[38;5;241m2.1\u001B[39m,\u001B[38;5;241m2.5\u001B[39m,\u001B[38;5;241m2.9\u001B[39m,\u001B[38;5;241m2.4\u001B[39m,\u001B[38;5;241m3\u001B[39m,\u001B[38;5;241m3.1\u001B[39m,\u001B[38;5;241m2.8\u001B[39m,\u001B[38;5;241m3.3\u001B[39m,\u001B[38;5;241m3.5\u001B[39m,\u001B[38;5;241m3\u001B[39m])\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "X = np.array([50,50,50,70,70,70,80,80,80,90,90,90,100,100,100]).reshape(-1, 1)\n",
    "y = np.array([3.3,2.8,2.9,2.3,2.6,2.1,2.5,2.9,2.4,3,3.1,2.8,3.3,3.5,3]).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Layer Neural Network\n",
    "\n",
    "In this question we apply a single-layer neural network to a linearly separable toy data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-22T09:13:25.479354200Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier \n",
    "from sklearn.linear_model import Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-22T09:13:25.480587200Z"
    }
   },
   "outputs": [],
   "source": [
    "X1, y1 = make_classification(n_features=2, n_redundant=0, \\\n",
    "            n_informative=1, random_state=1,n_clusters_per_class=1)\n",
    "fig1, ax1 = plt.subplots();\n",
    "ax1.scatter(X1[:,0],X1[:,1],c=y1, cmap='rainbow')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-22T09:13:25.481699900Z"
    }
   },
   "outputs": [],
   "source": [
    "nn1=Perceptron(alpha=1, max_iter=1000) \n",
    "model=nn1.fit(X1,y1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional) Plot the line that the perceptron is modelling. Look at the documentation to find out how you can get the values of the weights and the bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-22T09:13:25.482726300Z"
    }
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We can visualise the performance of this network again using a scatter plot, and colour the points using predicted class. Is the network able to give the right classification for each point?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T09:13:25.599987900Z",
     "start_time": "2024-01-22T09:13:25.485833400Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[26], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m ypred1\u001B[38;5;241m=\u001B[39m\u001B[43mmodel\u001B[49m\u001B[38;5;241m.\u001B[39mpredict(X1)\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# TODO: Make a scatter plot of the predictions.\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# How do the predictions compare with the ground truth? Compute the accuracy to compare\u001B[39;00m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "ypred1=model.predict(X1)\n",
    "# TODO: Make a scatter plot of the predictions.\n",
    "\n",
    "# How do the predictions compare with the ground truth? Compute the accuracy to compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer NN on Toy Problems\n",
    "In this question we consider two toy problems in which the classes are not linearly separable. In the first example the two classes form moon shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-22T09:13:25.509657900Z"
    }
   },
   "outputs": [],
   "source": [
    "X2, y2 = make_moons()\n",
    "fig2, ax2 = plt.subplots()\n",
    "ax2.scatter(X2[:,0],X2[:,1],c=y2, cmap='rainbow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try fitting a single-layer neural network to the data. \n",
    "Comment on the performance and hypothesise what might be the source of the errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T09:13:25.651061300Z",
     "start_time": "2024-01-22T09:13:25.603206800Z"
    }
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now try to fit a multi-layer NN to the same data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T09:13:25.741776Z",
     "start_time": "2024-01-22T09:13:25.634037100Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MLPClassifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[28], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m nn2\u001B[38;5;241m=\u001B[39m\u001B[43mMLPClassifier\u001B[49m(alpha\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,hidden_layer_sizes\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m10\u001B[39m,\u001B[38;5;241m10\u001B[39m,\u001B[38;5;241m10\u001B[39m,\u001B[38;5;241m10\u001B[39m), max_iter\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1000\u001B[39m)\n\u001B[0;32m      2\u001B[0m model2\u001B[38;5;241m=\u001B[39mnn2\u001B[38;5;241m.\u001B[39mfit(X2,y2)\n\u001B[0;32m      3\u001B[0m ypred2\u001B[38;5;241m=\u001B[39mmodel2\u001B[38;5;241m.\u001B[39mpredict(X2)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'MLPClassifier' is not defined"
     ]
    }
   ],
   "source": [
    "nn2=MLPClassifier(alpha=1,hidden_layer_sizes=(10,10,10,10), max_iter=1000)\n",
    "model2=nn2.fit(X2,y2)\n",
    "ypred2=model2.predict(X2)\n",
    "ax2.scatter(X2[:,0],X2[:,1],c=ypred2, cmap='rainbow')\n",
    "fig2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This NN has 4 hidden layers each with 10 neurons. The parameter\n",
    "`hidden_layer_sizes=(x,y,z, ...)` identifies the number of neurons in each layer. Try experimenting with different configurations of hidden layers to see the effect on performance.\n",
    "\n",
    "Investigate the parameter `activation`. What are the different activation functions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The activation functions are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now introduce another toy classification problem based on classes in the form of two circles. Experiment with using different NN architectures to fit this data and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "start_time": "2024-01-22T09:13:25.656343300Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_circles' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[29], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m X3, y3 \u001B[38;5;241m=\u001B[39m \u001B[43mmake_circles\u001B[49m()\n\u001B[0;32m      2\u001B[0m fig3, ax3 \u001B[38;5;241m=\u001B[39m plt\u001B[38;5;241m.\u001B[39msubplots()\n\u001B[0;32m      3\u001B[0m ax3\u001B[38;5;241m.\u001B[39mscatter(X3[:,\u001B[38;5;241m0\u001B[39m],X3[:,\u001B[38;5;241m1\u001B[39m],c\u001B[38;5;241m=\u001B[39my3, cmap\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrainbow\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'make_circles' is not defined"
     ]
    }
   ],
   "source": [
    "X3, y3 = make_circles()\n",
    "fig3, ax3 = plt.subplots()\n",
    "ax3.scatter(X3[:,0],X3[:,1],c=y3, cmap='rainbow')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-22T09:13:25.917708500Z",
     "start_time": "2024-01-22T09:13:25.776813400Z"
    }
   },
   "outputs": [],
   "source": [
    "#TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "bc4778dd1211f1bf59455cd46029fa692529fbc0d32c0ba0949d8ddd432c76af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
