{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Convolutional Neural Network (CNN)\n",
    "---\n",
    "卷积神经网络（Convolutional Neural Network，CNN）是一种专门用于处理图像和图像类数据的深度学习模型。它在计算机视觉领域取得了巨大成功，被广泛应用于图像识别、物体检测、图像分割等任务。以下是对CNN的详细介绍：\n",
    "\n",
    "**卷积神经网络的结构特点：**\n",
    "\n",
    "1. **卷积层（Convolutional Layer）**：CNN最重要的特点之一是卷积层。卷积操作是通过滤波器（也称为卷积核）在输入图像上滑动并执行局部感知操作。卷积层可以自动学习图像中的特征，例如边缘、纹理和形状。通常，多个卷积核用于提取不同的特征。\n",
    "\n",
    "2. **池化层（Pooling Layer）**：池化层用于降低特征图的空间维度，减少参数数量和计算复杂性。常见的池化操作包括最大池化和平均池化，它们分别取池化窗口内的最大值或平均值。池化有助于提取最显著的特征。\n",
    "\n",
    "3. **全连接层（Fully Connected Layer）**：在CNN的顶部，通常有一个或多个全连接层，用于将卷积层和池化层提取的特征映射到最终的输出类别。这些层与传统的前馈神经网络相似。\n",
    "\n",
    "4. **卷积核共享参数**：CNN中的卷积核是共享参数的，这意味着它们在整个输入图像上执行相同的卷积操作。这有助于减少参数数量，并使网络更容易训练。\n",
    "\n",
    "5. **局部感知和平移不变性**：CNN通过卷积操作实现了局部感知，这意味着每个神经元只关注输入的局部区域，从而减少了计算复杂性。此外，卷积神经网络具有平移不变性，即对于不同位置的相似特征具有相同的响应。\n",
    "\n",
    "**CNN的应用领域：**\n",
    "\n",
    "CNN在计算机视觉领域的应用非常广泛，包括但不限于以下领域：\n",
    "\n",
    "1. **图像分类**：CNN可以用于图像分类任务，如将图像分为不同的类别，例如猫、狗、汽车等。\n",
    "\n",
    "2. **物体检测**：CNN能够检测图像中的物体并标出其位置，用于物体识别和定位。\n",
    "\n",
    "3. **图像分割**：CNN可用于图像分割，将图像分成不同的区域，每个区域对应一个对象或物体。\n",
    "\n",
    "4. **人脸识别**：CNN在人脸识别领域取得了显著进展，用于身份验证和安全应用。\n",
    "\n",
    "5. **自动驾驶**：卷积神经网络在自动驾驶系统中广泛应用，用于感知和决策。\n",
    "\n",
    "总的来说，卷积神经网络是一种强大的深度学习模型，特别适用于处理图像和空间数据，它已经在许多领域取得了显著的成就，并持续推动计算机视觉和图像处理技术的发展。\n",
    "\n",
    "与FNN对比，CNN在处理图像和空间数据时具有明显的优势，这些优势包括参数共享、局部感知、平移不变性和自动特征提取等，使其成为计算机视觉和图像处理任务中的首选模型。然而，在某些非图像数据上，FNN仍然具有一定的优势。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be8090a2ebceb0ad"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T12:17:42.301672500Z",
     "start_time": "2024-02-03T12:17:40.053800100Z"
    }
   },
   "id": "535ddda235eb8f97",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. 卷积\n",
    "---\n",
    "卷积是一种数学运算，通常在信号处理、图像处理和深度学习等领域中广泛应用。数学上，离散一维卷积和二维卷积可以定义如下：\n",
    "\n",
    "**一维卷积**：\n",
    "对于两个离散信号（序列）f和g，它们的一维卷积（也称为离散卷积）定义为：\n",
    "$$(f * g)[n] = \\sum_{m=-\\infty}^{\\infty} f[m] \\cdot g[n - m]$$\n",
    "其中，$*$ 表示卷积操作，n是输出的序列索引，m是卷积核（滤波器）g的索引。卷积核g通过滑动（翻转后）与信号f的不同位置进行卷积操作，最终得到卷积结果f * g。\n",
    "\n",
    "**二维卷积**：\n",
    "对于两个离散二维图像（矩阵）F和G，它们的二维卷积定义为：\n",
    "$$(F * G)[i, j] = \\sum_{m=-\\infty}^{\\infty} \\sum_{n=-\\infty}^{\\infty} F[m, n] \\cdot G[i - m, j - n]$$\n",
    "其中，$*$ 表示卷积操作，(i, j) 是输出图像的像素坐标，(m, n) 是卷积核G的像素坐标。卷积核G通过滑动（翻转后）与图像F的不同位置进行卷积操作，最终得到卷积结果F * G。\n",
    "\n",
    "要注意的是，卷积操作通常包括了翻转（或翻转后的滑动）卷积核，这是因为在信号处理和深度学习中，通常使用的卷积是“卷积-求和”操作，而不是“交叉相关-求和”操作。因此，在计算卷积时，卷积核通常会进行翻转以匹配卷积操作的定义。\n",
    "\n",
    "卷积操作在信号处理中用于滤波、特征提取等任务，在深度学习中用于卷积神经网络（CNN）的卷积层，用于提取图像和特征的重要操作。通过不同的卷积核，可以捕捉不同的图像特征，例如边缘、纹理、形状等。\n",
    "\n",
    "---\n",
    "在深度学习中，通常使用的是互相关（cross-correlation）操作而不是纯粹的数学卷积操作。尽管它们在数学上略有不同，但在深度学习中，这两者经常被混淆使用，因为它们在实际计算上是等效的。\n",
    "\n",
    "在数学上，卷积操作要求对卷积核进行翻转（旋转180度）后与输入信号进行卷积。而在深度学习中，通常使用的是互相关操作，不进行卷积核的翻转。这是因为在深度学习的卷积神经网络（CNN）中，卷积核的参数是可以学习的，因此网络可以自动学习正确的卷积核权重，而不需要手动翻转它们。\n",
    "\n",
    "在实际应用中，使用互相关操作更常见，因为它更直观并且与卷积核的学习方式更符合。因此，当我们谈论深度学习中的卷积操作时，通常是指互相关操作。\n",
    "\n",
    "总之，虽然在数学上存在区别，但在深度学习中，互相关操作通常用于代替卷积操作，因为它更适合神经网络的训练和应用。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c046e566056e8c90"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1 Stride & Padding\n",
    "Stride（步长）是卷积操作中的一个重要参数，用于控制卷积核在输入上滑动的步长。Stride的值越大，卷积核滑动的步长越大，输出的特征图尺寸越小。Stride的值越小，卷积核滑动的步长越小，输出的特征图尺寸越大。\n",
    "\n",
    "Padding(填充)是在输入数据的周围添加额外的像素值（通常是零）以扩展输入的尺寸。填充在卷积操作之前应用，可以用来控制输出特征图的尺寸。\n",
    "\n",
    "<img src=\"./images/img_2.png\">"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4fda4943cccb8086"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 卷积层\n",
    "FNN中，需要将输入数据展平为一维张量，然后通过全连接层进行处理。然而，对于图像数据，这样处理会导致参数数量过多，计算量过大，且无法利用图像的空间结构信息。CNN通过卷积层来解决这个问题，卷积层可以自动提取图像中的特征，从而减少参数数量和计算量。\n",
    "\n",
    "<img src=\"./images/img_3.png\">\n",
    "\n",
    "\n",
    "- 权重共享：卷积层中的卷积核是共享参数的，即在第k层，所有的卷积核都相同，为$W_k$，这样可以大大减少参数数量，使得网络更容易训练。\n",
    "- 局部连接：卷积层中的每个神经元只连接到输入数据的一个局部区域，这个局部区域称为感受野（receptive field）。这样可以大大减少参数数量，使得网络更容易训练。这个连接区域决定于卷积核的大小，例如，如果卷积核的大小为3x3，则每个神经元只连接到输入数据的3x3区域。\n",
    "\n",
    "当卷积核大小k=input_size时，卷积层可以视为全连接层。但是，卷积层通常用小于input_size的卷积核，以便提取局部特征。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26727d9364c77c23"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Convolutional Layer 卷积层\n",
    "\n",
    "卷积层是神经网络中用于处理图像数据的重要层，它通过卷积操作从图像中提取特征。在卷积层中，每个神经元不是连接到上一层的所有神经元，而是仅连接到输入中的一个局部区域，这个连接的局部区域称为卷积核或滤波器。多个卷积核允许网络同时学习多个特征。\n",
    "\n",
    "**特征映射**：卷积层的输出称为特征映射（Feature Map），它是通过卷积操作从输入数据中提取的特征。每个卷积核都可以提取输入数据中的一个特定特征，多个卷积核可以提取多个特征。如果为灰度图像，有一个特征映射；如果为彩色图像，有3个特征映射（分别对应RGB通道）。\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8c97dff5a5bf234"
  },
  {
   "cell_type": "markdown",
   "source": [
    "`torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)`\n",
    "- `in_channels`：输入数据的通道数，例如彩色图像的通道数为3（RGB）。\n",
    "- `out_channels`：卷积层中卷积核（滤波器）的数量，这也决定了输出数据的通道数。\n",
    "- `kernel_size`：卷积核的大小，可以是一个整数或者由两个整数构成的元组，分别代表卷积核的高度和宽度。若为n则表示卷积核的高度和宽度都为n；若为(n, m)则表示卷积核的高度为n，宽度为m。\n",
    "- `stride`：卷积时的步长，决定了卷积核滑动的距离，可以是一个整数或者由两个整数构成的元组，分别代表在高度和宽度方向上的步长。默认为1。\n",
    "- `padding`：在输入数据周围添加的零填充的层数，可以是一个整数或者由两个整数构成的元组，分别代表在高度和宽度方向上的填充。默认为0。\n",
    "- `dilation`：卷积核中元素之间的间距，用于定义膨胀卷积，可以增加感受野。默认为1。\n",
    "- `groups`：控制输入和输出之间的连接方式，用于实现分组卷积。默认为1，表示标准卷积。\n",
    "- `bias`：是否在卷积层的输出中添加偏置项。默认为True。\n",
    "\n",
    "nn.Conv2d的输入是一个4维张量，形状为$[batch_size, inchannels, height, width]$，其中batch_size表示批处理中的样本数量，in_channels表示输入通道数，height和width表示图像的高度和宽度。输出也是一个4维张量，形状为[batch_size, out_channels, output_height, output_width]，其中output_height和output_width取决于卷积层的参数（如kernel_size、stride、padding等）和输入尺寸。\n",
    "\n",
    "卷积操作本质上是在输入图像的局部区域上应用滤波器，通过滑动卷积核并计算点乘和来提取特征。每个卷积核负责检测输入中的某种特定模式或特征，多个卷积核允许网络学习多种特征。\n",
    "\n",
    "#### 计算公式\n",
    "对于每个维度（宽度和高度），输出大小（`output_size`）可以通过以下公式计算：\n",
    "\n",
    "$$\\[ \\text{output\\_size} = \\left\\lfloor \\frac{\\text{input\\_size} + 2 \\times \\text{padding} - \\text{dilation} \\times (\\text{kernel\\_size} - 1) - 1}{\\text{stride}} + 1 \\right\\rfloor \\]$$\n",
    "\n",
    "一般默认padding=0，dilation=1，stride=1，这时输出大小可以简化为：\n",
    "$$ output\\_size=input\\_size - kernel\\_size + 1$$\n",
    "\n",
    "其中：\n",
    "- `input_size` 是输入图像的尺寸（宽度或高度）。\n",
    "- `padding` 是在输入图像边缘添加的零填充的层数。\n",
    "- `dilation` 是卷积核中相邻元素之间的间距。当 `dilation=1` 时，表示没有膨胀，卷积核的元素是连续的。\n",
    "- `kernel_size` 是卷积核的尺寸（宽度或高度）。\n",
    "- `stride` 是卷积核移动的步长。\n",
    "- 符号 `⌊ ⌋` 表示向下取整。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d83bb0dedaf2c563"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 代码示例\n",
    "\n",
    "$\\[ \\text{output width} = \\left( \\frac{5 + 2 \\times 0 - 2 - 1}{1} \\right) + 1 = 3 \\]$\n",
    "$\\[ \\text{output height} = \\left( \\frac{5 + 2 \\times 0 - 2 - 1}{1} \\right) + 1 = 3 \\]$\n",
    "\n",
    "所以，每个输出特征图的大小是 $\\(3 \\times 3\\)$。这个计算表明，当卷积核滑过输入图像时，由于没有填充来扩展图像的边缘，且步长为1，每个方向上的有效滑动次数是输入尺寸减去卷积核尺寸加1。因此，如果你有一个 $\\(5 \\times 5\\)$ 的图像和一个 $\\(3 \\times 3\\)$ 的卷积核，那么卷积核可以在每个方向上滑动 $\\(5 - 3 + 1 = 3\\)$ 次，产生一个 $\\(3 \\times 3\\)$ 的输出特征图。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "39fcab7df048f6a4"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.2817,  0.1275, -0.0267],\n",
      "          [-0.4894, -0.6437, -0.7979],\n",
      "          [-1.2606, -1.4148, -1.5690]],\n",
      "\n",
      "         [[-0.7577, -1.1682, -1.5788],\n",
      "          [-2.8104, -3.2209, -3.6315],\n",
      "          [-4.8631, -5.2736, -5.6842]],\n",
      "\n",
      "         [[ 6.6946,  7.1855,  7.6765],\n",
      "          [ 9.1493,  9.6402, 10.1312],\n",
      "          [11.6040, 12.0949, 12.5859]]]], grad_fn=<ConvolutionBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 卷积层示例\n",
    "# 设置随机数种子\n",
    "torch.manual_seed(0)\n",
    "# 创建一个卷积层，输入通道数为1，输出通道数为3，卷积核大小为3*3\n",
    "layer = nn.Conv2d(1, 3, kernel_size=3)\n",
    "# 创建一个4维的单通道（灰度）图像，图像大小为5*5: [1, 1, 5, 5]\n",
    "tensor = torch.FloatTensor([[[[1, 2, 3, 4, 5],\n",
    "                              [6, 7, 8, 9, 10],\n",
    "                              [11, 12, 13, 14, 15],\n",
    "                              [16, 17, 18, 19, 20],\n",
    "                              [21, 22, 23, 24, 25]]]])\n",
    "# 卷积\n",
    "output = layer(tensor)\n",
    "print(output) \n",
    "# 输入size：[1, 1, 5, 5]\n",
    "# 输出size：[1, 3, 3, 3]\n",
    "# 一个卷积层将一个单通道的 5×5 图像转换成了具有3个通道的 3×3 图像。这里的“通道”可以被理解为独立的特征图，每个特征图是由卷积层中的一个独立卷积核通过卷积操作从输入图像中提取的特征。"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T18:51:31.112071700Z",
     "start_time": "2024-02-02T18:51:31.078247700Z"
    }
   },
   "id": "206cb7efd1b2dbce",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pooling Layer 池化层\n",
    "池化层（Pooling Layer）是卷积神经网络（CNN）中常用的一种层，主要用于特征选择，降低特征图的维度，减少计算量，同时保留重要信息。池化操作通过对输入特征图的局部区域进行下采样，从而减小特征图的尺寸。池化层通常跟在卷积层后面，用于减少卷积层输出的特征图的维度，使得网络对于输入的小变化更加不变（invariant），增强了网络的泛化能力。\n",
    "\n",
    "#### 主要类型\n",
    "池化层主要有两种类型：\n",
    "1. **最大池化（Max Pooling）**：在输入特征图的局部区域中取最大值作为该区域的输出。最大池化有助于提取图像中的纹理和形状信息，是最常用的池化方式。\n",
    "2. **平均池化（Average Pooling）**：计算输入特征图的局部区域中所有元素的平均值，并将该平均值作为输出。平均池化有助于平滑图像特征。\n",
    "\n",
    "<img src=\"./images/img_4.png\">\n",
    "\n",
    "#### 参数\n",
    "池化层的主要参数包括：\n",
    "- **池化核大小（Kernel Size）**：池化操作覆盖的区域大小。常见的选择有2x2或3x3。\n",
    "- **步长（Stride）**：池化窗口滑动的步长。如果步长等于池化核的大小，那么池化窗口不会重叠。\n",
    "- **填充（Padding）**：在输入特征图的边缘添加的零填充的层数，用于控制输出特征图的大小。在池化操作中，填充不如在卷积操作中常用。\n",
    "\n",
    "#### 作用\n",
    "池化层的主要作用包括：\n",
    "- **降维**：减少特征图的尺寸，从而减少后续层的参数数量和计算量，防止过拟合。\n",
    "- **增强特征不变性**：通过池化操作，网络能够对输入图像中的小的变化（如平移、旋转等）更加鲁棒。\n",
    "- **特征强化**：最大池化可以强化特征图中的显著特征，有助于模型捕捉关键信息。\n",
    "\n",
    "#### 使用场景\n",
    "池化层广泛应用于各种卷积神经网络架构中，尤其是在处理图像和视频数据时。在多层卷积和池化层的交替使用中，网络能够逐渐提取从低级到高级的特征，从而实现复杂任务的学习，如图像分类、物体检测和语义分割等。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1836116e8864427e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 计算公式\n",
    "池化层只会改变输入特征图的尺寸，不会改变通道数。即输入[batch_size, in_channels, height, width]，输出[batch_size, in_channels, output_height, output_width]。channels维度的大小保持不变，而height和width的大小取决于池化操作的参数（如kernel_size、stride、padding等）和输入尺寸。\n",
    "\n",
    "对于每个维度（宽度和高度），输出大小（`output_size`）可以通过以下公式计算：\n",
    "\n",
    " $$\\(output\\_size = \\left\\lfloor\\frac{{\\text{input\\_size} - \\text{kernel\\_size}}}{{\\text{stride}}} + 1\\right\\rfloor\\)$$\n",
    "\n",
    "$\\lfloor  \\rfloor$ 表示向下取整。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c97baf9d2044cbd7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 最大池化（Max Pooling）\n",
    "\n",
    "#### 优点：\n",
    "- **特征保留**：最大池化能够保留特征图中的最强特征，使模型对特征的位置变化更加鲁棒。\n",
    "- **防过拟合**：通过减少参数数量，有助于减轻过拟合问题。\n",
    "- **计算效率**：相对于平均池化，最大池化的计算通常更简单快速。\n",
    "\n",
    "#### 缺点：\n",
    "- **信息丢失**：只保留最大值可能导致其他有用信息的丢失。\n",
    "- **过于激进**：在某些情况下，最大值可能过于突出，导致模型对细微变化不够敏感。\n",
    "\n",
    "### 平均池化（Average Pooling）\n",
    "\n",
    "#### 优点：\n",
    "- **信息平滑**：平均池化考虑了所有特征，可以在特征图上提供更平滑的信息。\n",
    "- **防止过拟合**：和最大池化一样，通过降低特征维度，也有助于减轻过拟合问题。\n",
    "\n",
    "#### 缺点：\n",
    "- **特征模糊**：平均操作可能导致重要特征的稀释，尤其是当背景噪声较多时。\n",
    "- **反应迟钝**：由于取平均值，模型对于局部特征的反应可能不如最大池化敏感。\n",
    "\n",
    "### 选择依据：\n",
    "\n",
    "- **任务需求**：如果任务需要保留最显著的特征，如在图像识别和对象检测中，最大池化往往更为合适。如果任务需要保留背景信息或者对特征的整体分布敏感，平均池化可能更加适用。\n",
    "  \n",
    "- **数据特性**：对于包含大量背景噪声的数据，最大池化能够通过忽略噪声保留有用的信号。而对于特征分布相对均匀的数据，平均池化可能更能够保持数据的完整性。\n",
    "\n",
    "- **网络结构**：在一些深层网络中，可能会结合使用最大池化和平均池化，以利用两者的优点。例如，在网络的初级阶段使用最大池化来提取显著特征，在网络的后续阶段使用平均池化来平滑特征并减少参数量。\n",
    "\n",
    "总的来说，最大池化和平均池化各有优势，选择哪一种池化策略应根据具体任务的需求、数据的特性以及网络的结构来决定。实践中，最大池化因为其在特征提取方面的效率和效果，被更广泛地应用于各种深度学习模型中。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4df4ddd662578484"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 池化层类别\n",
    "`torch.nn.MaxPool2d(kernel_size, stride=None, padding=0)`\n",
    "最大池化层，用于提取特征图中的显著特征。ouput_size = (input_size - kernel_size) / stride + 1\n",
    "\n",
    "`torch.nn.AvgPool2d(kernel_size, stride=None, padding=0)`\n",
    "平均池化层，用于平滑特征图。ouput_size = (input_size - kernel_size) / stride + 1\n",
    "\n",
    "`torch.nn.AdaptiveMaxPool2d(output_size)`\n",
    "自适应最大池化层，根据输出大小自适应调整池化窗口大小。\n",
    "\n",
    "\n",
    "在一些现代的CNN架构中，全连接层有时会被全局平均池化层（Global Average Pooling Layer）替代，以减少模型参数和防止过拟合，同时保持网络对全局信息的整合能力。\n",
    "但是由于池化层输出仍然是张量，所以在输入到最终输出层（全连接）之前，需要将池化层的输出展平为一维张量。使用.view()方法可以实现这一操作。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b9c44c69ccecd5c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "`view`和`flatten`都是PyTorch中用于改变Tensor形状的方法，特别是在将多维数据展平为一维数据时。它们各自有不同的使用场景和优缺点。\n",
    "\n",
    "### `view`\n",
    "\n",
    "- **优点**:\n",
    "  - 非常灵活，可以用于多种形状的转换，不仅仅是展平操作。\n",
    "  - 在使用时可以通过`-1`参数让PyTorch自动计算某一维度的大小，这在处理不同批次大小的数据时非常有用。\n",
    "\n",
    "- **缺点**:\n",
    "  - 需要保证Tensor在内存中是连续的，否则在调用`view`之前需要使用`.contiguous()`。这一步可能会稍微增加代码的复杂性。\n",
    "  - 使用`view`进行展平操作时，需要手动计算展平后的维度大小（除非使用`-1`）。\n",
    "\n",
    "- **代码示例**:\n",
    "\n",
    "```python\n",
    "# 假设x是一个形状为[batch_size, channels, height, width]的Tensor\n",
    "x = torch.randn(64, 3, 28, 28)  # 例如，64张3通道的28x28图像\n",
    "\n",
    "# 使用view展平，除了第一维(batch_size)之外的所有维度\n",
    "x_flattened = x.view(x.size(0), -1)  # x.size(0)是batch_size\n",
    "```\n",
    "\n",
    "### `flatten`\n",
    "\n",
    "- **优点**:\n",
    "  - 直接语义，明确表示展平操作，代码更易读。\n",
    "  - 不需要担心Tensor是否连续，在内部处理。\n",
    "  - 可以指定开始和结束的维度，提供了一定的灵活性。\n",
    "\n",
    "- **缺点**:\n",
    "  - 相比于`view`，`flatten`的使用场景更专一，主要用于展平操作。\n",
    "\n",
    "- **代码示例**:\n",
    "\n",
    "```python\n",
    "# 同样假设x是一个形状为[batch_size, channels, height, width]的Tensor\n",
    "x = torch.randn(64, 3, 28, 28)\n",
    "\n",
    "# 使用flatten展平，从第1维开始到最后\n",
    "x_flattened = torch.flatten(x, start_dim=1)  # 这会保留batch_size维度，展平其余维度\n",
    "```\n",
    "\n",
    "### 总结\n",
    "\n",
    "- 当你的目的是仅仅展平Tensor，并且希望代码清晰易读时，使用`flatten`是更好的选择。\n",
    "- 如果你需要更广泛的形状改变操作，或者想要在一个步骤中同时改变多个维度的大小，`view`可能是更灵活的选择。\n",
    "- 在实际应用中，选择哪一个主要取决于具体任务的需求和个人偏好。两者在性能上的差异通常可以忽略不计。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "af5071043ad028b1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.3 卷积网络整体结构\n",
    "<img src=\"./images/img_5.png\">\n",
    "\n",
    "卷积块（Convolutional Block）：通常由卷积层、激活函数层和池化层组成，用于提取输入数据的特征。各层参数需要相互匹配，以便输入和输出的维度能够正确对齐。\n",
    "\n",
    "而最后全连接层的神经元数量通常与任务的类别数相匹配，这是基于网络架构的需求、目标任务的复杂度以及计算资源的限制等因素确定的。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "563d58f5ef9eee"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.3 正则化\n",
    "在卷积神经网络（CNN）中，正则化是一种减少过拟合、提高模型泛化能力的技术。过拟合是指模型在训练数据上表现很好，但在未见过的新数据上表现不佳的情况。正则化通过对网络的训练过程施加约束或惩罚来防止过拟合。以下是CNN中常用的几种正则化技术：\n",
    "\n",
    "### 1. 权重正则化\n",
    "- **L1正则化**：添加一个与权重绝对值成正比的惩罚项到损失函数中。它倾向于产生稀疏的权重矩阵，有的权重可能为零，有助于模型的可解释性。\n",
    "- **L2正则化**（权重衰减）：在损失函数中添加一个与权重平方成正比的惩罚项。L2正则化倾向于使权重值均匀地较小，但不会变为零，有助于控制模型的复杂度。\n",
    "\n",
    "### 2. Dropout\n",
    "Dropout是一种训练时使用的正则化技术，它随机地“丢弃”（即将输出设置为零）神经网络中的一部分神经元，防止它们在训练时过度依赖于彼此。这可以被看作是同时训练大量的神经网络（每个网络都是原始网络的一个子集）并共享权重的方式。在测试时不使用dropout，但会将神经元的输出按保留比例进行缩放，以保持总体激活水平。\n",
    "\n",
    "### 3. 批归一化（Batch Normalization）\n",
    "批归一化通过规范化层的输入来加速训练过程，减少所谓的内部协变量偏移问题。虽然其主要目的是加速训练，但它也有轻微的正则化效果，因为它向网络的每层添加了噪声。这意味着模型对于输入的小变化变得更加鲁棒，从而有助于泛化。\n",
    "\n",
    "### 4. 数据增强\n",
    "数据增强不直接通过修改损失函数或网络结构来正则化模型，而是通过人为增加训练数据的多样性（例如，通过旋转、缩放、裁剪或改变训练图像的颜色）来提高模型的泛化能力。虽然这不是正则化技术，但它有助于减少过拟合，因为模型现在必须学习识别更多变化的输入。\n",
    "\n",
    "### 5. 早停（Early Stopping）\n",
    "早停是一种基于验证集性能的正则化形式。在每个训练周期（epoch）后评估模型在一个独立的验证集上的性能，当性能开始下降时，即停止训练。这可以防止模型在训练数据上过度拟合。\n",
    "\n",
    "### 6. 稀疏性约束\n",
    "通过施加稀疏性约束来正则化模型，迫使网络学习只激活一小部分神经元，而其他神经元保持非激活状态。这可以通过在损失函数中添加一个使激活值接近零的惩罚项来实现。\n",
    "\n",
    "这些正则化技术可以单独使用，也可以组合使用，以根据特定任务和数据集优化CNN模型的性能和泛化能力。在实践中，需要通过实验来确定哪种正则化策略最适合特定"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63ce3a025a330a25"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 在PyTorch中实现正则化\n",
    "### 1. 权重正则化（L1/L2正则化）\n",
    "\n",
    "在PyTorch中，L1/L2正则化可以通过优化器（如`torch.optim.Adam`或`torch.optim.SGD`）中的`weight_decay`参数来实现，通常这对应于L2正则化。对于L1正则化，可能需要手动实现，比如在损失函数中添加权重的绝对值之和。\n",
    "\n",
    "```python\n",
    "# L2正则化示例（PyTorch）\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "# L1正则化示例（PyTorch，手动添加到损失函数）\n",
    "l1_lambda = 0.001\n",
    "l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "loss = criterion(output, target) + l1_lambda * l1_norm\n",
    "```\n",
    "\n",
    "### 2. Dropout\n",
    "\n",
    "在PyTorch中，Dropout可以通过`nn.Dropout`层在模型定义中直接使用。在训练时，Dropout层会随机将输入单元的一部分设置为0，而在测试时保持输入不变。\n",
    "\n",
    "```python\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.dropout = nn.Dropout(0.5)  # Dropout比例为0.5\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)  # 应用Dropout\n",
    "        # 其他层...\n",
    "```\n",
    "\n",
    "### 3. 批归一化（Batch Normalization）\n",
    "\n",
    "在PyTorch中，批归一化可以通过`nn.BatchNorm2d`（针对2D卷积层）或`nn.BatchNorm1d`（针对全连接层）层来实现。\n",
    "\n",
    "```python\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.bn = nn.BatchNorm2d(num_features=64)  # 64是特征图的数量\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn(x)  # 应用BatchNorm\n",
    "        # 其他层...\n",
    "```\n",
    "\n",
    "### 4. 数据增强\n",
    "\n",
    "数据增强通常在数据加载阶段实现，使用PyTorch时，可以通过`torchvision.transforms`模块实现。\n",
    "\n",
    "```python\n",
    "from torchvision import transforms\n",
    "\n",
    "# 定义数据增强转换\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# 应用于数据加载器\n",
    "train_dataset = datasets.ImageFolder(root='path/to/train/data', transform=transform)\n",
    "```\n",
    "\n",
    "### 5. 早停（Early Stopping）\n",
    "\n",
    "早停通常需要手动实现，通过监控验证集上的性能，在性能不再提高时停止训练。\n",
    "\n",
    "```python\n",
    "best_val_loss = None\n",
    "patience = 10  # 未见改善的训练轮数\n",
    "trigger_times = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 训练模型...\n",
    "    \n",
    "    # 验证集上的损失\n",
    "    val_loss = validate(model, val_loader)\n",
    "    \n",
    "    if best_val_loss is None or val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        trigger_times = 0\n",
    "    else:\n",
    "        trigger_times += 1\n",
    "    \n",
    "    if trigger_times >= patience:\n",
    "        print(\"Early stopping!\")\n",
    "        break\n",
    "```\n",
    "\n",
    "这些示例提供了在PyTorch中实现各种正则化技术的基本方法。根据实际需求，可以调整参数和实现细节。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ccd16f1b8d39f993"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 批归一化 Batch Normalization\n",
    "批归一化（Batch Normalization，简称BN）是一种广泛应用于深度学习模型中的技术，尤其是在卷积神经网络（CNN）和全连接网络中。它由 Sergey Ioffe 和 Christian Szegedy 在 2015 年提出，主要目的是解决深度网络训练过程中的“内部协变量偏移”问题。批归一化的作用包括：\n",
    "\n",
    "### 1. 加速训练过程\n",
    "批归一化通过规范化层的输入，使其均值接近0，标准差接近1，这有助于避免网络训练初期的梯度消失或梯度爆炸问题。标准化的输入允许使用更高的学习率，从而加快模型训练过程。\n",
    "\n",
    "### 2. 减少内部协变量偏移\n",
    "内部协变量偏移是指由于网络参数的更新，导致网络层激活分布发生变化的现象。这种变化迫使后续层不断适应新的分布，从而增加了训练过程的复杂度。批归一化通过规范化每一批数据来减少这种分布上的变化，使得模型训练更加稳定。\n",
    "\n",
    "### 3. 提高泛化能力\n",
    "批归一化在一定程度上可以起到正则化的作用，这意味着它可以帮助减少模型在训练数据上的过拟合。在批处理过程中，每个小批量数据的均值和方差的计算引入了噪声，这种噪声可以看作是一种轻微的数据增强技术，有助于提高模型的泛化能力。\n",
    "\n",
    "### 4. 允许更深的网络\n",
    "通过减少梯度消失问题，批归一化使得训练更深层的网络成为可能。深层网络通常能够学习更复杂和抽象的特征，但也更难训练。批归一化通过改善训练过程，帮助实现了更深网络的有效训练。\n",
    "\n",
    "### 实现细节\n",
    "在实践中，批归一化通常作为一个独立的层被加入到网络中，在卷积层或全连接层之后，激活函数之前。它计算当前批次数据的均值和标准差，然后使用下面的公式进行规范化：\n",
    "\n",
    "$$\\[ \\hat{x}^{(k)} = \\frac{x^{(k)} - \\mu_{\\mathcal{B}}}{\\sqrt{\\sigma_{\\mathcal{B}}^{2} + \\epsilon}} \\]$$\n",
    "\n",
    "其中，$\\(x^{(k)}\\)$ 是一个小批量数据中的一个数据点，$\\(\\mu_{\\mathcal{B}}\\)$ 和 $\\(\\sigma_{\\mathcal{B}}^{2}\\)$ 分别是这个批次数据的均值和方差，$\\(\\epsilon\\)$ 是一个很小的常数，用来避免除以零。之后，批归一化层会对规范化后的数据进行缩放和平移变换：\n",
    "\n",
    "$\\[ y^{(k)} = \\gamma \\hat{x}^{(k)} + \\beta \\]$\n",
    "\n",
    "其中，$\\(\\gamma\\)$ 和 $\\(\\beta\\)$ 是可学习的参数，允许网络学习恢复到最优的数据分布。通过这种方式，批归一化层不仅规范化了输入数据，还保持了网络的表达能力。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "713a3374213fc1a8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.4 完整CNN代码示例\n",
    "下面代码的CNN模型包括两个卷积层--两个最大池化层--两个全连接层，通过识别手写数字的MNIST数据集来展示CNN的应用。\n",
    "\n",
    "1. `datasets.MNIST('../data', train=True, download=True, ...)`\n",
    "   - `datasets.MNIST`：PyTorch提供的一个用于加载MNIST手写数字数据集的接口。\n",
    "   - `'../data'`：指定数据集下载（如果尚未下载）和存储的位置。这里是相对于当前工作目录的`data`文件夹。\n",
    "   - `train=True`：指定加载的是训练集。如果设置为`False`，则加载的是测试集。\n",
    "   - `download=True`：如果数据集尚未下载，则允许自动下载。\n",
    "\n",
    "2. `transform=transforms.Compose([...])`\n",
    "   - `transforms.Compose`：组合一系列的变换操作。\n",
    "   - `transforms.ToTensor()`：将PIL图像或NumPy数组转换为PyTorch张量，并且把像素值从[0, 255]缩放到[0.0, 1.0]的范围。\n",
    "   - `transforms.Normalize((0.1307,), (0.3081,))`：标准化张量图像，使用均值`0.1307`和标准差`0.3081`进行归一化。这些值通常是根据数据集预计算得出的。这里使用的是MNIST数据集的全局均值和标准差。\n",
    "\n",
    "3. `DataLoader(...)`\n",
    "   - `DataLoader`：PyTorch中用于加载数据的工具，可以迭代地提供给网络小批量的数据。\n",
    "        - `batch_size=64`：指定每个小批量的数据包含的样本数。这里设置为64，意味着每次迭代会提供64张图片给模型。\n",
    "        - `shuffle=True`：在每个epoch开始时，打乱数据。这有助于模型泛化，防止模型记忆训练数据的顺序。\n",
    "\n",
    "综上所述，这段代码配置了一个数据加载器，它会以64张图片为一组，从MNIST数据集中加载经过标准化处理的图像数据，用于训练过程。数据加载器在每个epoch开始时会打乱数据，以提高训练的效果。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d6b3751accf0d327"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 数据加载和预处理\n",
    "train_loader = DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True, \n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=64, shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=1000, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T18:51:31.178850Z",
     "start_time": "2024-02-02T18:51:31.116060900Z"
    }
   },
   "id": "2c2b9dde57439eac",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 定义CNN模型\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # 定义第一个卷积层，输入通道为1（灰度图），输出通道为10，卷积核大小为5\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        # 定义第二个卷积层，输入通道为10，输出通道为20，卷积核大小为5\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        # 定义一个最大池化层，窗口大小为2，即kernel_size=2\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2)\n",
    "        # 定义两个全连接层\n",
    "        self.fc1 = nn.Linear(20*4*4, 50)  # 320 = 20*4*4，4\n",
    "        self.fc2 = nn.Linear(50, 10)  # 10个输出对应于10个类别\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 通过第一个卷积层后使用ReLU激活函数，然后通过最大池化\n",
    "        # \n",
    "        x = F.relu(self.max_pool(self.conv1(x)))\n",
    "        # 通过第二个卷积层后使用ReLU激活函数，然后通过最大池化\n",
    "        x = F.relu(self.max_pool(self.conv2(x)))\n",
    "        # 将特征图展平\n",
    "        x = x.view(-1, 320)\n",
    "        # 通过第一个全连接层后使用ReLU激活函数\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # 通过第二个全连接层得到最终的输出\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T18:51:31.194797Z",
     "start_time": "2024-02-02T18:51:31.171874Z"
    }
   },
   "id": "cf5c3631da51a7e9",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.363403\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.668899\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.433514\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.278730\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.173540\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.193385\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.277416\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.383217\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.135102\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.148474\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 9590/10000 (96%)\n"
     ]
    }
   ],
   "source": [
    "# 设置设备\n",
    "device=torch.device(\"cuda\" )\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 实例化模型并移动到设备上\n",
    "model = SimpleCNN().to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "# 训练模型\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()   # 梯度清零\n",
    "        output = model(data)   # 前向传播\n",
    "        loss = criterion(output, target) # 使用交叉熵损失函数\n",
    "        loss.backward() # 反向传播\n",
    "        optimizer.step() # 更新参数\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "\n",
    "# 测试模型\n",
    "def test():\n",
    "    # 切换模型为评估模式,这会关闭dropout和batch normalization\n",
    "    model.eval()\n",
    "    # 初始化测试损失\n",
    "    test_loss = 0\n",
    "    # 初始化预测正确的数量\n",
    "    correct = 0\n",
    "    \n",
    "    # 不记录模型梯度信息\n",
    "    with torch.no_grad():\n",
    "        # 遍历测试集\n",
    "        for data, target in test_loader:\n",
    "            # 将数据移动到设备上,若设置有gpu则device代表gpu\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # 前向传播\n",
    "            output = model(data)\n",
    "            # 计算测试损失，criterion（）.item（）返回张量中的元素值,这里表示用交叉熵损失函数计算损失\n",
    "            test_loss += criterion(output, target).item()  # 将一批的损失相加\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # 获得概率最大的索引\n",
    "            \n",
    "            # 计算预测正确的数量，.eq()表示比较两个张量相同位置的元素是否相等，.view_as(pred)表示将target张量的形状变成和pred一样\n",
    "            # correct为预测正确的数量\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    \n",
    "    # test_loss为所有batch的平均损失\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    \n",
    "    # 打印测试结果\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)\\n')\n",
    "\n",
    "# 执行训练和测试,这里只训练了一个epoch\n",
    "for epoch in range(1, 2):\n",
    "    train(epoch)\n",
    "    test()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T19:00:07.913947100Z",
     "start_time": "2024-02-02T18:59:56.312990400Z"
    }
   },
   "id": "615fc8535612c47a",
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. 典型的卷积神经网络\n",
    "---\n",
    "## 2.1 LeNet-5\n",
    "LeNet-5 是一个经典的卷积神经网络（CNN）架构，由 Yann LeCun 等人于 1998 年提出，主要用于手写数字识别和其他图像识别任务。LeNet-5 是深度学习和计算机视觉领域最早期的突破之一，尽管它的结构相对简单，但它奠定了现代深度卷积网络的基础。\n",
    "\n",
    "LeNet-5 的架构主要包括以下层：\n",
    "\n",
    "1. **输入层**：输入图像大小通常为 32x32x1（对于 MNIST 数据集，图像会被调整到这个大小，因为LeNet-5一般输入为灰度图像，所以只有一个通道）。\n",
    "2. **C1 层（第一个卷积层）**：使用 6 个 5x5 的卷积核，不包括边缘填充（padding），输出维度会是 28x28x6（因为 32-5+1=28）。\n",
    "3. **S2 层（第一个池化层）**：对 C1 层的输出进行 2x2,stride=2 的池化操作，输出维度变为 14x14x6。(因为(28-2)/2+1=14).\n",
    "4. **C3 层（第二个卷积层）**：使用 16 个 5x5 的卷积核，对 S2 层的输出进行卷积，输出维度变为 10x10x16（因为 14-5+1=10）。\n",
    "5. **S4 层（第二个池化层）**：对 C3 层的输出进行 2x2 的池化操作，输出维度变为 5x5x16。(因为(10-2)/2+1=5).\n",
    "\n",
    "接下来是 **C5 层**，这一层通常被视为一个全连接层，因为其卷积核的大小与输入维度相同（5x5），使得每个卷积核覆盖了 S4 层输出的整个区域。因此，每个卷积核产生的输出将是一个单独的值，而不是一个特征图。\n",
    "\n",
    "- **C5 层**：使用 120 个 5x5 的卷积核，每个卷积核都完整地覆盖了 S4 层的 5x5x16 的输出，产生一个单一的输出值。因此，C5 层的输出维度将是 1x1x120。因为 5-5+1=1。\n",
    "\n",
    "- **F6 层（第一个全连接层）**：C5 层的 120 个 1x1x120 的输出将被连接到一个包含 84 个神经元的全连接层，输出维度为 1x1x84。\n",
    "- **输出层**：F6 层的 84 个神经元将连接到一个输出层，输出维度为 1x1x10，对应于 10 个类别的概率分布(因为MNIST数据集有10个类别即数字0-9)。\n",
    "\n",
    "LeNet-5 使用的激活函数最初是 Sigmoid 或者双曲正切（Tanh），但在现代实现中，ReLU 变得更为流行，因为它能够加速训练过程并减少梯度消失的问题。此外，LeNet-5 中的下采样层在现代架构中通常被更有效的最大池化层所替代。\n",
    "\n",
    "尽管 LeNet-5 相对于现代深度学习模型而言比较简单，但它对深度学习和卷积神经网络的发展具有重要的历史意义。\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77210394d0c3fcf4"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# LeNet-5示例\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mLeNet5\u001B[39;00m(\u001B[43mnn\u001B[49m\u001B[38;5;241m.\u001B[39mModule):\n\u001B[0;32m      3\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m      4\u001B[0m         \u001B[38;5;28msuper\u001B[39m(LeNet5, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m()\n",
      "\u001B[1;31mNameError\u001B[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "# LeNet-5示例\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        \n",
    "        # 定义第卷积层C1, 输入通道为1，输出通道为6，卷积核大小为5(6个5x5的卷积核)\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
    "        # 定义池化层S2，最大池化，窗口大小为2\n",
    "        self.max_pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # 定义第卷积层C3, 输入通道为6，输出通道为16，卷积核大小为5(16个5x5的卷积核)\n",
    "        self.conv3 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        # 定义池化层S4，最大池化，窗口大小为2\n",
    "        self.max_pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # 定义第卷积层C5，可以视为全连接层, 输入通道为16*5*5，输出通道为120。\n",
    "        self.fc5 = nn.Linear(16*5*5, 120)\n",
    "        # 定义全连接层F6，输入维度为120，输出维度为84\n",
    "        self.fc6 = nn.Linear(120, 84)\n",
    "        # 定义输出层，输入维度为84，输出维度为10\n",
    "        self.fc7 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 通过C1后使用ReLU激活函数\n",
    "        x = F.relu(self.conv1(x))\n",
    "        # 通过S2\n",
    "        x = self.max_pool2(x)\n",
    "        # 通过C3后使用ReLU激活函数\n",
    "        x = F.relu(self.conv3(x))\n",
    "        # 通过S4\n",
    "        x = self.max_pool4(x)\n",
    "        # 展平特征图\n",
    "        # x.view(-1, 16*5*5) 这行代码的作用是将前面卷积层和池化层处理后的多维特征图展平为一维向量。这一步是必要的，因为全连接层（如 self.fc5）期望其输入是一维向量形式，而不是多维特征图。\n",
    "        x = x.view(-1, 16*5*5)\n",
    "        # 通过C5后使用ReLU激活函数\n",
    "        x = F.relu(self.fc5(x))\n",
    "        # 通过F6后使用ReLU激活函数\n",
    "        x = F.relu(self.fc6(x))\n",
    "        # 输出层\n",
    "        x = self.fc7(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T12:17:29.283831500Z",
     "start_time": "2024-02-03T12:17:28.899799400Z"
    }
   },
   "id": "6a6f3fe5d831649",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'transforms' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 5\u001B[0m\n\u001B[0;32m      2\u001B[0m batch_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m64\u001B[39m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# MNIST 数据集的转换器，首先将数据转换为张量，然后标准化\u001B[39;00m\n\u001B[1;32m----> 5\u001B[0m transform \u001B[38;5;241m=\u001B[39m \u001B[43mtransforms\u001B[49m\u001B[38;5;241m.\u001B[39mCompose([\n\u001B[0;32m      6\u001B[0m     transforms\u001B[38;5;241m.\u001B[39mResize((\u001B[38;5;241m32\u001B[39m, \u001B[38;5;241m32\u001B[39m)),  \u001B[38;5;66;03m# LeNet-5 需要 32x32 的输入\u001B[39;00m\n\u001B[0;32m      7\u001B[0m     transforms\u001B[38;5;241m.\u001B[39mToTensor(),\n\u001B[0;32m      8\u001B[0m     transforms\u001B[38;5;241m.\u001B[39mNormalize((\u001B[38;5;241m0.1307\u001B[39m,), (\u001B[38;5;241m0.3081\u001B[39m,))\n\u001B[0;32m      9\u001B[0m ])\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# 下载并加载训练集\u001B[39;00m\n\u001B[0;32m     12\u001B[0m train_dataset \u001B[38;5;241m=\u001B[39m datasets\u001B[38;5;241m.\u001B[39mMNIST(root\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mC:\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mUsers\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124myhb\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mMscProject\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mAI&TA\u001B[39m\u001B[38;5;130;01m\\\\\u001B[39;00m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m'\u001B[39m, train\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, download\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, transform\u001B[38;5;241m=\u001B[39mtransform)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'transforms' is not defined"
     ]
    }
   ],
   "source": [
    "# 加载数据集\n",
    "batch_size = 64\n",
    "\n",
    "# MNIST 数据集的转换器，首先将数据转换为张量，然后标准化\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),  # LeNet-5 需要 32x32 的输入\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# 下载并加载训练集\n",
    "train_dataset = datasets.MNIST(root='C:\\\\Users\\\\yhb\\\\MscProject\\\\AI&TA\\\\data', train=True, download=False, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 下载并加载测试集\n",
    "test_dataset = datasets.MNIST(root='C:\\\\Users\\\\yhb\\\\MscProject\\\\AI&TA\\\\data', train=False, download=False, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T12:17:33.476800100Z",
     "start_time": "2024-02-03T12:17:33.418480800Z"
    }
   },
   "id": "3e7a14d8a81faa22",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 设置训练过程\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T19:59:40.767831300Z",
     "start_time": "2024-02-02T19:59:40.745904800Z"
    }
   },
   "id": "a472c15f304cc404",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 设置测试过程\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    \n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T19:59:43.501757800Z",
     "start_time": "2024-02-02T19:59:43.482818Z"
    }
   },
   "id": "8e6696d3e59ff078",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.298209\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.390365\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.098400\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.115190\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.183708\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.141212\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.131079\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.166541\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.087854\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.120810\n",
      "\n",
      "Test set: Average loss: 0.0699, Accuracy: 9768/10000 (98%)\n"
     ]
    }
   ],
   "source": [
    "# 设置设备并实例化模型，定义损失函数和优化器，开始训练和测试\n",
    "device = torch.device(\"cuda\" )\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LeNet5().to(device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "for epoch in range(1, 2): # 这里只训练了一个epoch\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T20:02:30.131732300Z",
     "start_time": "2024-02-02T20:02:16.682671Z"
    }
   },
   "id": "e0c403cca40005f",
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e7354ee7c7604573"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 AlexNet\n",
    "AlexNet是一个深度卷积神经网络（CNN），由Alex Krizhevsky、Ilya Sutskever和Geoffrey Hinton共同开发。它在2012年的ImageNet大规模视觉识别挑战赛（ILSVRC）中获得了冠军，大幅度提高了图像识别的准确率，从而标志着深度学习在计算机视觉领域的重要突破。\n",
    "\n",
    "### AlexNet结构\n",
    "\n",
    "AlexNet包含5个卷积层，其中某些层后面跟有最大池化层，以及3个全连接层，最后是一个1000路的softmax分类器。使用ReLU（Rectified Linear Unit）作为激活函数，采用了Dropout来减少过拟合。\n",
    "\n",
    "### 每一层的数据变化和计算过程\n",
    "\n",
    "\n",
    "1. **输入层**：输入图像的尺寸为 224x224x3，适用于彩色图像。\n",
    "\n",
    "2. **第1个卷积层（C1层）**：使用96个11x11的卷积核，步长为4，不包括边缘填充（padding），输出的维度变为 \\[ (224-11)/4 + 1 \\] x \\[ (224-11)/4 + 1 \\] x 96 = 55x55x96。\n",
    "\n",
    "3. **第1个池化层（S2层）**：对C1层的输出进行3x3，步长为2的最大池化操作，输出的维度变为 \\[ (55-3)/2 + 1 \\] x \\[ (55-3)/2 + 1 \\] x 96 = 27x27x96。\n",
    "\n",
    "4. **第2个卷积层（C3层）**：使用256个5x5的卷积核，采用same padding（边缘填充），使得卷积操作的输出尺寸不变，因此输出维度为27x27x256。\n",
    "\n",
    "5. **第2个池化层（S4层）**：对C3层的输出进行3x3，步长为2的最大池化操作，输出的维度变为 \\[ (27-3)/2 + 1 \\] x \\[ (27-3)/2 + 1 \\] x 256 = 13x13x256。\n",
    "\n",
    "6. **第3个卷积层（C5层）**：使用384个3x3的卷积核，采用same padding，输出维度为13x13x384。\n",
    "\n",
    "7. **第4个卷积层（C6层）**：使用384个3x3的卷积核，同样采用same padding，输出维度为13x13x384。\n",
    "\n",
    "8. **第5个卷积层（C7层）**：使用256个3x3的卷积核，同样采用same padding，然后直接连接到池化层，输出维度为13x13x256。\n",
    "\n",
    "9. **第3个池化层（S8层）**：对C7层的输出进行3x3，步长为2的最大池化操作，输出维度变为 \\[ (13-3)/2 + 1 \\] x \\[ (13-3)/2 + 1 \\] x 256 = 6x6x256。\n",
    "\n",
    "10. **第1个全连接层（F9层）**：该层将S8层的输出向量化（展平），并连接到4096个神经元上，使用ReLU激活函数。\n",
    "\n",
    "11. **第2个全连接层（F10层）**：这一层也有4096个神经元，使用ReLU激活函数。\n",
    "\n",
    "12. **输出层（F11层）**：最后一层是一个含有1000个输出神经元的全连接层，对应于1000个类别的softmax输出，用于分类。\n",
    "\n",
    "\n",
    "在每个全连接层之后，AlexNet使用了Dropout技术来减少过拟合。全连接层的输入是前一层的输出展平后的结果。\n",
    "\n",
    "### 总结\n",
    "\n",
    "AlexNet的成功证明了深度学习特别是卷积神经网络在图像识别任务中的有效性，它的结构成为了后续许多网络设计的基础。通过交替的卷积层和池化层，网络能够从图像中提取复杂的特征，而全连接层则负责将这些特征转换为分类结果。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b956cf8eb5412218"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\"Same padding\"（相同填充）是卷积神经网络中的一个术语，它指的是在进行卷积操作前，对输入数据的边界进行填充，以保证卷积操作后的输出尺寸与输入尺寸相同（或者更具体地，保证输出的高度和宽度与输入的高度和宽度相同）。这种填充通常用0值（也就是空白）来填充输入数据的边缘。\n",
    "\n",
    "#### 目的\n",
    "\n",
    "Same padding的主要目的是允许深度神经网络的每一层保持输入数据的空间尺寸（即宽度和高度），这样可以避免因为多次卷积操作而导致的尺寸缩减过快，从而丢失边缘信息。这对于构建更深的网络结构特别重要，因为它允许网络通过多层传递更多的信息。\n",
    "\n",
    "#### 如何工作\n",
    "\n",
    "- 假设输入数据的尺寸为 $\\(H \\times W\\)$（高度 $\\(H\\)$，宽度 $\\(W\\)$）。\n",
    "- 卷积核（过滤器）的尺寸为 $\\(F \\times F\\)$。\n",
    "- 为了保持输出尺寸与输入尺寸相同，卷积操作前需要在输入数据的周围填充宽度和高度为 $\\(P\\)$ 的0值，其中 $\\(P\\)$ 可以根据卷积核的大小来计算。\n",
    "\n",
    "对于步长为1的卷积操作，填充量 $\\(P\\)$ 可以通过以下公式来计算：\n",
    "\n",
    "$\\[ P = \\frac{F - 1}{2} \\]$\n",
    "\n",
    "这样，当应用 $\\(F \\times F\\)$ 大小的卷积核进行步长为1的卷积操作时，输出数据的尺寸仍然是 $\\(H \\times W\\)$，即与输入数据的尺寸相同。\n",
    "\n",
    "\n",
    "通过这种方式，same padding使得网络能够在深层次中保留更多的空间信息，有助于改善网络性能和精度。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be86447579c824e7"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.301891\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.272054\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.177905\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.225261\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.050001\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.213359\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.039305\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.098063\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.056787\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.046887\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 9729/10000 (97%)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 定义AlexNet模型\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):  # MNIST有10个类别\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=11, stride=4, padding=5)  # 11x11的卷积核，步长为4，填充为5\n",
    "        self.conv2 = nn.Conv2d(64, 192, kernel_size=5, padding=2)\n",
    "        self.conv3 = nn.Conv2d(192, 384, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(384, 256, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(12544, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.fc3 = nn.Linear(4096, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, 0.5)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, 0.5)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# 加载MNIST数据集\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),  # 将图像大小调整为AlexNet期望的224x224\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST数据集的均值和标准差\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='C:\\\\Users\\\\yhb\\\\MscProject\\\\AI&TA\\\\data', train=True, download=False, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='C:\\\\Users\\\\yhb\\\\MscProject\\\\AI&TA\\\\data', train=False, download=False, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "# 初始化模型、损失函数和优化器\n",
    "device = torch.device(\"cuda\" )\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AlexNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# 训练模型\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "\n",
    "# 测试模型\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)\\n')\n",
    "\n",
    "# 开始训练和测试\n",
    "for epoch in range(1, 2):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T12:19:40.247653700Z",
     "start_time": "2024-02-03T12:18:05.583884500Z"
    }
   },
   "id": "c7d05af9b7e43d36",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3 Inception\n",
    "Inception网络，也被称为GoogLeNet，是一种深度卷积神经网络(CNN)架构。它最初是在2014年的ImageNet大规模视觉识别挑战赛(ILSVRC)中被介绍，并取得了显著的成果。Inception网络的主要特点是它的网络结构非常深，但同时计算效率很高。这是通过采用一种名为“Inception模块”的特殊设计实现的，它允许网络在不同尺度上捕获信息，并在不显著增加计算负担的情况下深化网络。\n",
    "\n",
    "Inception模块的关键思想是并行地应用不同尺寸的卷积滤波器（如1x1, 3x3, 5x5），然后将这些滤波器的输出在深度维度上进行合并。这种结构允许模型在每一层同时捕捉到不同尺度的特征，从而增强了模型的表达能力。此外，1x1卷积在这里也被用来作为降维的手段，以减少参数数量和计算复杂度。\n",
    "\n",
    "<img src=\"./images/img_6.png\">"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a5808cee23788ed0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 定义简化版的Inception模块\n",
    "class InceptionModule(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(InceptionModule, self).__init__()\n",
    "        self.branch1x1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n",
    "\n",
    "        self.branch5x5_1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n",
    "        self.branch5x5_2 = nn.Conv2d(16, 24, kernel_size=5, padding=2)\n",
    "\n",
    "        self.branch3x3dbl_1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n",
    "        self.branch3x3dbl_2 = nn.Conv2d(16, 24, kernel_size=3, padding=1)\n",
    "        self.branch3x3dbl_3 = nn.Conv2d(24, 24, kernel_size=3, padding=1)\n",
    "\n",
    "        self.branch_pool = nn.Conv2d(in_channels, 24, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "\n",
    "        branch5x5 = self.branch5x5_1(x)\n",
    "        branch5x5 = self.branch5x5_2(branch5x5)\n",
    "\n",
    "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
    "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
    "\n",
    "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "\n",
    "        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n",
    "        return torch.cat(outputs, 1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a9535ecb22a3134"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 定义整个网络结构\n",
    "class InceptionNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(InceptionNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.inception1 = InceptionModule(10)\n",
    "        self.inception2 = InceptionModule(88)  # 16+24*3=88\n",
    "        self.fc = nn.Linear(88, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = self.inception1(x)\n",
    "        x = F.relu(F.max_pool2d(x, 2))\n",
    "        x = self.inception2(x)\n",
    "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "deada1c5e0151a3c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "该代码的前向传播逻辑主要分为几个部分，通过`InceptionNet`类的`forward`方法实现。下面是这个过程的详细解释：\n",
    "\n",
    "1. **初始卷积层（`self.conv1`）**:\n",
    "    - 输入的MNIST图像首先通过一个卷积层（`self.conv1`），这个层使用了5x5的卷积核和10个输出通道。这一步的目的是提取图像的初级特征。\n",
    "    - 接着，使用最大池化（`F.max_pool2d`）来减小特征图的空间维度，并通过ReLU激活函数（`F.relu`）增加非线性。\n",
    "\n",
    "2. **第一个Inception模块（`self.inception1`）**:\n",
    "    - 经过初始处理后的特征图被送入第一个Inception模块。在这个模块中，会并行地应用四种不同的操作（1x1卷积、5x5卷积、3x3卷积序列和平均池化后的1x1卷积），每种操作都试图从输入特征图中捕捉不同的信息。\n",
    "    - 这四种操作的输出会在深度维度上被合并（使用`torch.cat`），这样可以保持空间尺寸不变，同时增加通道的深度。\n",
    "\n",
    "3. **第二次最大池化**:\n",
    "    - 经过第一个Inception模块处理后的输出再次通过ReLU激活函数和最大池化来减小特征图的空间维度并增加非线性。\n",
    "\n",
    "4. **第二个Inception模块（`self.inception2`）**:\n",
    "    - 这一步与第一个Inception模块类似，但输入通道的数量根据第一个Inception模块的输出来确定。它同样并行地应用四种不同的操作，并将这些操作的输出在深度上合并。\n",
    "\n",
    "5. **自适应平均池化（`F.adaptive_avg_pool2d`）**:\n",
    "    - 经过第二个Inception模块后，使用自适应平均池化来将特征图的空间维度降低到1x1，这样无论输入图像的尺寸如何，输出的特征图尺寸都是固定的。\n",
    "\n",
    "6. **全连接层（`self.fc`）**:\n",
    "    - 将平坦化（`torch.flatten`）后的特征图送入一个全连接层（`self.fc`），这个层的输出维度为10，对应于MNIST数据集的10个类别。\n",
    "    - 这一步的目的是将提取的特征映射到最终的类别预测上。\n",
    "\n",
    "整个前向传播过程是通过组合这些操作来实现的，目的是从输入的MNIST图像中提取复杂的特征，并将这些特征用于最终的分类任务。通过这种方式，Inception网络能够在不同的尺度上捕捉图像特征，从而提高分类的准确性。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f438d9b06fc0d073"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
