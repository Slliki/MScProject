{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Recurrent Neural Networks  (RNN)\n",
    "## 1. Introduction\n",
    "前馈神经网络中，信息只有单向传播，这一定程度上使得模型容易学习，但同时也削弱了模型的能力。前馈神经网络可以看作一个复杂的函数，每次输入独立，输出只依赖于当前的输入。但实际任务中，网络输出不仅与当前输入相关，还与之前的输出相关。例如，自然语言处理中，一句话的意思不仅仅取决于当前的词，还取决于之前的词。为了解决这个问题，RNN应运而生。RNN是一类有短期记忆能力的神经网络，常用于处理序列数据，使得它们特别适合处理语言处理、时间序列分析、生物信息学等领域的任务。\n",
    "\n",
    "#### 核心概念\n",
    "\n",
    "- **循环结构**：RNN的核心特点是网络中存在循环，这意味着网络的输出可以再次作为输入。这种结构使得RNN能够保留过去信息的记忆，并将其用于当前的计算中。\n",
    "- **隐藏状态**：RNN通过隐藏状态（hidden state）来存储过去信息的摘要。隐藏状态在序列的每个时间步上都会更新，以包含到当前步骤为止的信息。\n",
    "- **参数共享**：在处理序列的每个元素时，RNN在其所有时间步上共享参数。这种参数共享机制使得RNN能够处理变长的输入序列。\n",
    "\n",
    "#### 基本结构\n",
    "\n",
    "一个基本的RNN单元包含一个简单的神经网络层，该层不仅接收当前时间步的输入，还接收上一个时间步的隐藏状态作为输入。输出包括当前时间步的隐藏状态（可以传递到下一个时间步）和实际的输出（可选）。\n",
    "\n",
    "<img src=\"./images/img_7.png\">\n",
    "\n",
    "#### 数学表示\n",
    "\n",
    "假设在时间步$\\( t \\)$的输入是$\\( x_t \\)$，上一个时间步的隐藏状态是$\\( h_{t-1} \\)$，那么RNN单元可以表示为：\n",
    "\n",
    "- 更新隐藏状态：$$\\[ h_t = f(W_{hh}h_{t-1} + W_{xh}x_t + b_h) \\]$$\n",
    "- 生成输出（可选）：$$\\[ y_t = g(W_{hy}h_t + b_y) \\]$$\n",
    "\n",
    "其中，$\\( W_{hh} \\)$、$\\( W_{xh} \\)$和$\\( W_{hy} \\)$是权重矩阵，$\\( b_h \\)$和$\\( b_y \\)$是偏置项，$\\( f \\)$和$\\( g \\)$是激活函数，通常$\\( f \\)$是非线性激活函数如$tanh$或$ReLU$，$\\( g \\)$可以是softmax函数（用于分类任务）。\n",
    "\n",
    "#### 挑战\n",
    "\n",
    "- **梯度消失和梯度爆炸**：由于RNN的循环结构，它在训练过程中很容易遇到梯度消失和梯度爆炸问题，这使得模型难以学习长距离依赖。\n",
    "- **长期依赖问题**：标准RNN在处理长序列时效果不佳，因为随着时间的推移，信息会逐渐丢失。\n",
    "\n",
    "#### 变体\n",
    "\n",
    "为了解决标准RNN的一些问题，研究者们提出了一些变体，如长短期记忆网络（LSTM）和门控循环单元（GRU）。这些变体通过引入门机制来控制信息的流动，从而更有效地捕捉长期依赖关系。\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "822a26049e299035"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. 应用模式\n",
    "循环神经网络（RNN）可以有多种不同的模式，这些模式决定了网络如何处理输入序列和产生输出序列。这些模式包括：\n",
    "\n",
    "#### 1. 序列到序列（One-to-Many）\n",
    "- **场景**：当需要从单个输入生成一系列数据时使用，例如音乐生成或图像描述。\n",
    "- **结构**：一个输入对应多个输出。\n",
    "\n",
    "#### 2. 序列到单个输出（Many-to-One）\n",
    "- **场景**：适用于序列分类任务，如情感分析或者主题识别，整个输入序列对应一个单独的输出。\n",
    "- **结构**：多个输入对应一个输出。\n",
    "\n",
    "<img src=\"./images/img_9.png\">\n",
    "\n",
    "#### 3. 序列到序列（Many-to-Many，同步）\n",
    "- **场景**：当输入和输出都是序列，并且对于每个输入时间步都有一个对应的输出时使用，如机器翻译（输入和输出序列长度相同）。\n",
    "- **结构**：每个输入对应一个输出，序列长度保持不变。\n",
    "<img src=\"./images/img_10.png\">\n",
    "\n",
    "#### 4. 序列到序列（Many-to-Many，异步）\n",
    "- **场景**：输入和输出都是序列，但输出序列的长度可以不同，如语音识别或者机器翻译（输入和输出序列长度不同）。\n",
    "- **结构**：多个输入对应多个输出，序列长度可以变化。\n",
    "<img src=\"./images/img_11.png\">\n",
    "\n",
    "#### 5. 双向RNN（Bidirectional RNN）\n",
    "- **场景**：当需要考虑输入序列的前后文时使用，比如在文本处理中更好地理解上下文。\n",
    "- **结构**：RNN在两个方向上运行，正向和反向，每个时间步的输出是两个方向隐藏状态的函数。\n",
    "\n",
    "#### 6. 深层RNN（Deep RNN）\n",
    "- **场景**：为了增加模型的复杂性和学习能力，可以堆叠多个RNN层。\n",
    "- **结构**：每个时间步的输出除了传递给下一个时间步，也传递给下一个RNN层。\n",
    "\n",
    "这些模式可以根据具体任务的需求进行组合和匹配，以达到更好的性能。例如，双向和深层RNN可以结合起来处理复杂的序列到序列任务。选择哪种模式取决于输入数据的性质和想要的输出类型。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e11a3466e38d0450"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. 参数学习\n",
    "同样可以使用梯度下降等方法（optimizer）来学习参数，但是由于RNN中存在递归调用的函数，其参数的梯度计算方法与前馈神经网络有所不同。RNN中的梯度计算方法主要有以下几种：\n",
    "\n",
    "#### 1. 时间反向传播（Backpropagation Through Time，BPTT）\n",
    "循环神经网络（RNN）的梯度计算通常使用反向传播（Backpropagation Through Time, BPTT）算法。反向传播是一种用于计算神经网络中参数的梯度的常用方法，而BPTT是针对具有时间步骤的循环结构的特殊情况。\n",
    "\n",
    "以下是RNN的梯度计算步骤：\n",
    "\n",
    "1. 前向传播：首先，通过输入数据和前一时间步骤的隐藏状态进行前向传播，计算当前时间步骤的隐藏状态和输出。\n",
    "\n",
    "2. 计算损失：使用前向传播得到的输出计算损失函数（通常是根据任务选择的，如均方误差或交叉熵）来衡量模型的性能。\n",
    "\n",
    "3. 反向传播：从时间步骤T（最后一个时间步骤）开始，计算梯度。\n",
    "\n",
    "4. 通过时间步骤反向传播：然后，通过时间步骤t从T到1依次反向传播梯度，更新权重和偏差。每个时间步骤的梯度计算包括以下步骤：\n",
    "   a. 计算隐藏状态梯度：\n",
    "   b. 计算权重和偏差的梯度：\n",
    "\n",
    "5. 参数更新：使用计算得到的梯度，通过梯度下降或其他优化算法更新权重矩阵和偏差向量。\n",
    "\n",
    "这些步骤组成了BPTT算法，用于计算RNN模型的梯度，以便在训练过程中更新参数以最小化损失函数。这个过程在训练中迭代进行，直到达到收敛或设定的训练迭代次数。\n",
    "\n",
    "#### 2. 实时递归学习（Real-Time Recurrent Learning，RTRL）\n",
    "RTRL是一种精确的梯度计算方法，它通过计算每个时间步上的梯度来更新参数。RTRL的计算复杂度随着时间步的增加而增加，因此在实际应用中很少使用。\n",
    "\n",
    "通常，传统的反向传播算法在实际训练中更为常用，因为它更高效，尤其是对于长序列和大规模数据。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3583a1def413f7d5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Long Range Dependency\n",
    "RNN的一个主要问题是难以捕捉长期依赖关系。当序列长度较长时，RNN的梯度会出现梯度消失或梯度爆炸的问题，导致模型难以学习长期依赖关系。为了解决这个问题，研究者们提出了一些变体，如长短期记忆网络（LSTM）和门控循环单元（GRU）。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "758e889caa75c450"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.1 LSTM\n",
    "长短期记忆网络（Long Short-Term Memory，LSTM）是一种特殊的RNN变体，它通过引入门机制来控制信息的流动，从而更有效地捕捉长期依赖关系。LSTM单元包括一个细胞状态和三个门：输入门、遗忘门和输出门。\n",
    "\n",
    "- **输入门（Input Gate）**：决定是否更新单元状态（cell state）。它使用sigmoid激活函数来处理输入数据和先前时间步骤的隐藏状态，以产生一个在0到1之间的值。\n",
    "- **遗忘门（Forget Gate）**：决定在当前时间步骤是否保留前一时间步骤的单元状态的信息。它也使用sigmoid激活函数来产生一个在0到1之间的值。\n",
    "- **输出门（Output Gate）**：决定在当前时间步骤的隐藏状态中传递哪些信息给下一个时间步骤的隐藏状态，并将单元状态的一部分映射到输出。它使用sigmoid激活函数来产生一个在0到1之间的值，同时使用双曲正切激活函数来产生一个在-1到1之间的值。\n",
    "\n",
    "#### 注意：\n",
    "一般深度学习网络参数时，初始化参数值都较小，但使用LSTM时，参数初始化值要稍微大一些，因为过小的值使得遗忘门的值较小，意味着前一时刻的信息会损失过多，这样的网络难以捕捉到长距离依赖。\n",
    "\n",
    "LSTM的结构原理可以简要概括如下：\n",
    "\n",
    "1. 输入门（Input Gate）：\n",
    "   - 输入门决定是否更新单元状态（cell state）。它使用sigmoid激活函数来处理输入数据和先前时间步骤的隐藏状态，以产生一个在0到1之间的值。\n",
    "   - 输入门的计算公式为：\n",
    "     $\\[i_t = \\sigma(W_{xi}x_t + W_{hi}h_{t-1} + W_{ci}c_{t-1} + b_i)\\]$\n",
    "   其中，$\\(i_t\\)$是输入门的输出，$\\(x_t\\)$是当前时间步骤的输入，$\\(h_{t-1}\\)$是前一时间步骤的隐藏状态，$\\(c_{t-1}\\)$是前一时间步骤的单元状态，$\\(W_{xi}\\)$、$\\(W_{hi}\\)$、$\\(W_{ci}\\)$和$\\(b_i\\)$是与输入门相关的权重和偏差。\n",
    "\n",
    "2. 遗忘门（Forget Gate）：\n",
    "   - 遗忘门决定在当前时间步骤是否保留前一时间步骤的单元状态的信息。它也使用sigmoid激活函数来产生一个在0到1之间的值。\n",
    "   - 遗忘门的计算公式为：\n",
    "     $\\[f_t = \\sigma(W_{xf}x_t + W_{hf}h_{t-1} + W_{cf}c_{t-1} + b_f)\\]$\n",
    "   其中，$\\(f_t\\)$是遗忘门的输出，$\\(x_t\\)$是当前时间步骤的输入，$\\(h_{t-1}\\)$是前一时间步骤的隐藏状态，$\\(c_{t-1}\\)$是前一时间步骤的单元状态，$\\(W_{xf}\\)$、$\\(W_{hf}\\)$、$\\(W_{cf}\\)$和$\\(b_f\\)$是与遗忘门相关的权重和偏差。\n",
    "\n",
    "3. 更新单元状态（Cell State Update）：\n",
    "   - 更新单元状态的过程包括两部分：首先，通过输入门来确定哪些信息将被添加到单元状态中；然后，通过遗忘门来确定哪些信息将被从单元状态中删除。\n",
    "   - 更新单元状态的计算公式为：\n",
    "     $\\[c_t = f_t \\cdot c_{t-1} + i_t \\cdot \\tanh(W_{xc}x_t + W_{hc}h_{t-1} + b_c)\\]$\n",
    "   其中，$\\(c_t\\)$是当前时间步骤的单元状态，$\\(\\tanh\\)$是双曲正切激活函数，$\\(W_{xc}\\)$、$\\(W_{hc}\\)$和$\\(b_c\\)$是与单元状态更新相关的权重和偏差。\n",
    "\n",
    "4. 输出门（Output Gate）：\n",
    "   - 输出门决定在当前时间步骤的隐藏状态中传递哪些信息给下一个时间步骤的隐藏状态，并将单元状态的一部分映射到输出。它使用sigmoid激活函数来产生一个在0到1之间的值，同时使用双曲正切激活函数来产生一个在-1到1之间的值。\n",
    "   - 输出门的计算公式为：\n",
    "     $\\[o_t = \\sigma(W_{xo}x_t + W_{ho}h_{t-1} + W_{co}c_t + b_o)\\]$\n",
    "   其中，$\\(o_t\\)$是输出门的输出，$\\(x_t\\)$是当前时间步骤的输入，$\\(h_{t-1}\\)$是前一时间步骤的隐藏状态，$\\(c_t\\)$是当前时间步骤的单元状态，$\\(W_{xo}\\)$、$\\(W_{ho}\\)$、$\\(W_{co}\\)$和$\\(b_o\\)$是与输出门相关的权重和偏差。\n",
    "\n",
    "5. 隐藏状态（Hidden State）：\n",
    "   - 最后，通过输出门和单元状态来计算当前时间步骤的隐藏状态：\n",
    "     $\\[h_t = o_t \\cdot \\tanh(c_t)\\]$\n",
    "   其中，$\\(h_t\\)$是当前时间步骤的隐藏状态，$\\(\\tanh\\)$是双曲正切激活函数，$\\(o_t\\)$是输出门的输出，$\\(c_t\\)$是当前时间步骤的单元状态。\n",
    "\n",
    "LSTM的结构允许它在处理长序列时保留和传递信息，同时有效地防止梯度消失问题。它在自然语言处理、语音识别、时间序列预测等领域取得了显著的成功。\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc3d76ee445f2d2f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 代码示例"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "685a41aa9a8665cd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# 定义LSTM模型\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM循环单元\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        # 全连接层\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T14:26:24.533469500Z",
     "start_time": "2024-02-03T14:26:24.518460300Z"
    }
   },
   "id": "e456e6590eb94a43",
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 准备示例数据\n",
    "batch_size = 32\n",
    "sequence_length = 5\n",
    "input_size = 10\n",
    "output_size = 1\n",
    "total_samples = 100000\n",
    "\n",
    "# 生成随机输入数据和标签\n",
    "data = torch.randn(total_samples, sequence_length, input_size)\n",
    "labels = torch.randint(0, 2, (total_samples, output_size), dtype=torch.float32)\n",
    "\n",
    "# 创建数据集\n",
    "dataset = TensorDataset(data, labels)\n",
    "\n",
    "# 划分训练集和验证集\n",
    "train_size = int(0.8 * total_samples)  # 80%的样本用于训练\n",
    "test_size = total_samples - train_size  # 剩余部分用于验证\n",
    "\n",
    "train_dataset, valid_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# 创建数据加载器\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T14:29:45.040435600Z",
     "start_time": "2024-02-03T14:29:44.999941500Z"
    }
   },
   "id": "17c35f433c90d98b",
   "execution_count": 56
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = nn.BCELoss()(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
    "\n",
    "# 测试模型\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            outputs = model(data)\n",
    "            test_loss += nn.BCELoss()(outputs, target).item()\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            correct += (predicted == target).sum().item()\n",
    "        \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = correct / len(test_loader.dataset)\n",
    "\n",
    "    print(f\"\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {accuracy * 100:.2f}%\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T14:27:51.376338100Z",
     "start_time": "2024-02-03T14:27:51.357394600Z"
    }
   },
   "id": "a8b7dbbc6b3e8626",
   "execution_count": 54
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/80000 (0%)]\tLoss: 0.688967\n",
      "Train Epoch: 1 [3200/80000 (4%)]\tLoss: 0.693296\n",
      "Train Epoch: 1 [6400/80000 (8%)]\tLoss: 0.694137\n",
      "Train Epoch: 1 [9600/80000 (12%)]\tLoss: 0.693700\n",
      "Train Epoch: 1 [12800/80000 (16%)]\tLoss: 0.693496\n",
      "Train Epoch: 1 [16000/80000 (20%)]\tLoss: 0.694990\n",
      "Train Epoch: 1 [19200/80000 (24%)]\tLoss: 0.695753\n",
      "Train Epoch: 1 [22400/80000 (28%)]\tLoss: 0.693872\n",
      "Train Epoch: 1 [25600/80000 (32%)]\tLoss: 0.692786\n",
      "Train Epoch: 1 [28800/80000 (36%)]\tLoss: 0.692188\n",
      "Train Epoch: 1 [32000/80000 (40%)]\tLoss: 0.693373\n",
      "Train Epoch: 1 [35200/80000 (44%)]\tLoss: 0.693418\n",
      "Train Epoch: 1 [38400/80000 (48%)]\tLoss: 0.692524\n",
      "Train Epoch: 1 [41600/80000 (52%)]\tLoss: 0.689481\n",
      "Train Epoch: 1 [44800/80000 (56%)]\tLoss: 0.690182\n",
      "Train Epoch: 1 [48000/80000 (60%)]\tLoss: 0.691712\n",
      "Train Epoch: 1 [51200/80000 (64%)]\tLoss: 0.694292\n",
      "Train Epoch: 1 [54400/80000 (68%)]\tLoss: 0.696056\n",
      "Train Epoch: 1 [57600/80000 (72%)]\tLoss: 0.693020\n",
      "Train Epoch: 1 [60800/80000 (76%)]\tLoss: 0.692230\n",
      "Train Epoch: 1 [64000/80000 (80%)]\tLoss: 0.693004\n",
      "Train Epoch: 1 [67200/80000 (84%)]\tLoss: 0.691918\n",
      "Train Epoch: 1 [70400/80000 (88%)]\tLoss: 0.688593\n",
      "Train Epoch: 1 [73600/80000 (92%)]\tLoss: 0.695752\n",
      "Train Epoch: 1 [76800/80000 (96%)]\tLoss: 0.691037\n",
      "Test set: Average loss: 0.0217, Accuracy: 50.38%\n",
      "Train Epoch: 2 [0/80000 (0%)]\tLoss: 0.689963\n",
      "Train Epoch: 2 [3200/80000 (4%)]\tLoss: 0.688954\n",
      "Train Epoch: 2 [6400/80000 (8%)]\tLoss: 0.694111\n",
      "Train Epoch: 2 [9600/80000 (12%)]\tLoss: 0.693313\n",
      "Train Epoch: 2 [12800/80000 (16%)]\tLoss: 0.692501\n",
      "Train Epoch: 2 [16000/80000 (20%)]\tLoss: 0.693153\n",
      "Train Epoch: 2 [19200/80000 (24%)]\tLoss: 0.692717\n",
      "Train Epoch: 2 [22400/80000 (28%)]\tLoss: 0.690859\n",
      "Train Epoch: 2 [25600/80000 (32%)]\tLoss: 0.693009\n",
      "Train Epoch: 2 [28800/80000 (36%)]\tLoss: 0.691504\n",
      "Train Epoch: 2 [32000/80000 (40%)]\tLoss: 0.695148\n",
      "Train Epoch: 2 [35200/80000 (44%)]\tLoss: 0.692864\n",
      "Train Epoch: 2 [38400/80000 (48%)]\tLoss: 0.691884\n",
      "Train Epoch: 2 [41600/80000 (52%)]\tLoss: 0.693027\n",
      "Train Epoch: 2 [44800/80000 (56%)]\tLoss: 0.691267\n",
      "Train Epoch: 2 [48000/80000 (60%)]\tLoss: 0.694163\n",
      "Train Epoch: 2 [51200/80000 (64%)]\tLoss: 0.693555\n",
      "Train Epoch: 2 [54400/80000 (68%)]\tLoss: 0.690823\n",
      "Train Epoch: 2 [57600/80000 (72%)]\tLoss: 0.692344\n",
      "Train Epoch: 2 [60800/80000 (76%)]\tLoss: 0.691326\n",
      "Train Epoch: 2 [64000/80000 (80%)]\tLoss: 0.692084\n",
      "Train Epoch: 2 [67200/80000 (84%)]\tLoss: 0.690560\n",
      "Train Epoch: 2 [70400/80000 (88%)]\tLoss: 0.694448\n",
      "Train Epoch: 2 [73600/80000 (92%)]\tLoss: 0.696084\n",
      "Train Epoch: 2 [76800/80000 (96%)]\tLoss: 0.696986\n",
      "Test set: Average loss: 0.0217, Accuracy: 49.55%\n",
      "Train Epoch: 3 [0/80000 (0%)]\tLoss: 0.694023\n",
      "Train Epoch: 3 [3200/80000 (4%)]\tLoss: 0.691847\n",
      "Train Epoch: 3 [6400/80000 (8%)]\tLoss: 0.694938\n",
      "Train Epoch: 3 [9600/80000 (12%)]\tLoss: 0.697330\n",
      "Train Epoch: 3 [12800/80000 (16%)]\tLoss: 0.689606\n",
      "Train Epoch: 3 [16000/80000 (20%)]\tLoss: 0.689504\n",
      "Train Epoch: 3 [19200/80000 (24%)]\tLoss: 0.693674\n",
      "Train Epoch: 3 [22400/80000 (28%)]\tLoss: 0.692796\n",
      "Train Epoch: 3 [25600/80000 (32%)]\tLoss: 0.692490\n",
      "Train Epoch: 3 [28800/80000 (36%)]\tLoss: 0.695383\n",
      "Train Epoch: 3 [32000/80000 (40%)]\tLoss: 0.694134\n",
      "Train Epoch: 3 [35200/80000 (44%)]\tLoss: 0.693821\n",
      "Train Epoch: 3 [38400/80000 (48%)]\tLoss: 0.686154\n",
      "Train Epoch: 3 [41600/80000 (52%)]\tLoss: 0.692052\n",
      "Train Epoch: 3 [44800/80000 (56%)]\tLoss: 0.693787\n",
      "Train Epoch: 3 [48000/80000 (60%)]\tLoss: 0.694919\n",
      "Train Epoch: 3 [51200/80000 (64%)]\tLoss: 0.696463\n",
      "Train Epoch: 3 [54400/80000 (68%)]\tLoss: 0.693162\n",
      "Train Epoch: 3 [57600/80000 (72%)]\tLoss: 0.690941\n",
      "Train Epoch: 3 [60800/80000 (76%)]\tLoss: 0.689787\n",
      "Train Epoch: 3 [64000/80000 (80%)]\tLoss: 0.692471\n",
      "Train Epoch: 3 [67200/80000 (84%)]\tLoss: 0.690529\n",
      "Train Epoch: 3 [70400/80000 (88%)]\tLoss: 0.692091\n",
      "Train Epoch: 3 [73600/80000 (92%)]\tLoss: 0.694256\n",
      "Train Epoch: 3 [76800/80000 (96%)]\tLoss: 0.686002\n",
      "Test set: Average loss: 0.0217, Accuracy: 49.52%\n",
      "Train Epoch: 4 [0/80000 (0%)]\tLoss: 0.692275\n",
      "Train Epoch: 4 [3200/80000 (4%)]\tLoss: 0.692218\n",
      "Train Epoch: 4 [6400/80000 (8%)]\tLoss: 0.697464\n",
      "Train Epoch: 4 [9600/80000 (12%)]\tLoss: 0.702825\n",
      "Train Epoch: 4 [12800/80000 (16%)]\tLoss: 0.690981\n",
      "Train Epoch: 4 [16000/80000 (20%)]\tLoss: 0.695531\n",
      "Train Epoch: 4 [19200/80000 (24%)]\tLoss: 0.678994\n",
      "Train Epoch: 4 [22400/80000 (28%)]\tLoss: 0.688490\n",
      "Train Epoch: 4 [25600/80000 (32%)]\tLoss: 0.688309\n",
      "Train Epoch: 4 [28800/80000 (36%)]\tLoss: 0.691643\n",
      "Train Epoch: 4 [32000/80000 (40%)]\tLoss: 0.683035\n",
      "Train Epoch: 4 [35200/80000 (44%)]\tLoss: 0.702021\n",
      "Train Epoch: 4 [38400/80000 (48%)]\tLoss: 0.695058\n",
      "Train Epoch: 4 [41600/80000 (52%)]\tLoss: 0.688815\n",
      "Train Epoch: 4 [44800/80000 (56%)]\tLoss: 0.694455\n",
      "Train Epoch: 4 [48000/80000 (60%)]\tLoss: 0.698879\n",
      "Train Epoch: 4 [51200/80000 (64%)]\tLoss: 0.698312\n",
      "Train Epoch: 4 [54400/80000 (68%)]\tLoss: 0.690636\n",
      "Train Epoch: 4 [57600/80000 (72%)]\tLoss: 0.697795\n",
      "Train Epoch: 4 [60800/80000 (76%)]\tLoss: 0.688529\n",
      "Train Epoch: 4 [64000/80000 (80%)]\tLoss: 0.684837\n",
      "Train Epoch: 4 [67200/80000 (84%)]\tLoss: 0.689485\n",
      "Train Epoch: 4 [70400/80000 (88%)]\tLoss: 0.687682\n",
      "Train Epoch: 4 [73600/80000 (92%)]\tLoss: 0.699596\n",
      "Train Epoch: 4 [76800/80000 (96%)]\tLoss: 0.694018\n",
      "Test set: Average loss: 0.0217, Accuracy: 49.91%\n",
      "Train Epoch: 5 [0/80000 (0%)]\tLoss: 0.689791\n",
      "Train Epoch: 5 [3200/80000 (4%)]\tLoss: 0.693503\n",
      "Train Epoch: 5 [6400/80000 (8%)]\tLoss: 0.694991\n",
      "Train Epoch: 5 [9600/80000 (12%)]\tLoss: 0.701308\n",
      "Train Epoch: 5 [12800/80000 (16%)]\tLoss: 0.692881\n",
      "Train Epoch: 5 [16000/80000 (20%)]\tLoss: 0.683933\n",
      "Train Epoch: 5 [19200/80000 (24%)]\tLoss: 0.699259\n",
      "Train Epoch: 5 [22400/80000 (28%)]\tLoss: 0.699842\n",
      "Train Epoch: 5 [25600/80000 (32%)]\tLoss: 0.687304\n",
      "Train Epoch: 5 [28800/80000 (36%)]\tLoss: 0.688462\n",
      "Train Epoch: 5 [32000/80000 (40%)]\tLoss: 0.692198\n",
      "Train Epoch: 5 [35200/80000 (44%)]\tLoss: 0.686765\n",
      "Train Epoch: 5 [38400/80000 (48%)]\tLoss: 0.686517\n",
      "Train Epoch: 5 [41600/80000 (52%)]\tLoss: 0.690156\n",
      "Train Epoch: 5 [44800/80000 (56%)]\tLoss: 0.703781\n",
      "Train Epoch: 5 [48000/80000 (60%)]\tLoss: 0.689447\n",
      "Train Epoch: 5 [51200/80000 (64%)]\tLoss: 0.694432\n",
      "Train Epoch: 5 [54400/80000 (68%)]\tLoss: 0.697236\n",
      "Train Epoch: 5 [57600/80000 (72%)]\tLoss: 0.696296\n",
      "Train Epoch: 5 [60800/80000 (76%)]\tLoss: 0.685300\n",
      "Train Epoch: 5 [64000/80000 (80%)]\tLoss: 0.679010\n",
      "Train Epoch: 5 [67200/80000 (84%)]\tLoss: 0.690464\n",
      "Train Epoch: 5 [70400/80000 (88%)]\tLoss: 0.691727\n",
      "Train Epoch: 5 [73600/80000 (92%)]\tLoss: 0.683792\n",
      "Train Epoch: 5 [76800/80000 (96%)]\tLoss: 0.677839\n",
      "Test set: Average loss: 0.0217, Accuracy: 49.70%\n",
      "Train Epoch: 6 [0/80000 (0%)]\tLoss: 0.693338\n",
      "Train Epoch: 6 [3200/80000 (4%)]\tLoss: 0.688828\n",
      "Train Epoch: 6 [6400/80000 (8%)]\tLoss: 0.688991\n",
      "Train Epoch: 6 [9600/80000 (12%)]\tLoss: 0.698959\n",
      "Train Epoch: 6 [12800/80000 (16%)]\tLoss: 0.692015\n",
      "Train Epoch: 6 [16000/80000 (20%)]\tLoss: 0.678210\n",
      "Train Epoch: 6 [19200/80000 (24%)]\tLoss: 0.670527\n",
      "Train Epoch: 6 [22400/80000 (28%)]\tLoss: 0.683677\n",
      "Train Epoch: 6 [25600/80000 (32%)]\tLoss: 0.690576\n",
      "Train Epoch: 6 [28800/80000 (36%)]\tLoss: 0.681406\n",
      "Train Epoch: 6 [32000/80000 (40%)]\tLoss: 0.687817\n",
      "Train Epoch: 6 [35200/80000 (44%)]\tLoss: 0.677643\n",
      "Train Epoch: 6 [38400/80000 (48%)]\tLoss: 0.678248\n",
      "Train Epoch: 6 [41600/80000 (52%)]\tLoss: 0.696351\n",
      "Train Epoch: 6 [44800/80000 (56%)]\tLoss: 0.681216\n",
      "Train Epoch: 6 [48000/80000 (60%)]\tLoss: 0.696868\n",
      "Train Epoch: 6 [51200/80000 (64%)]\tLoss: 0.679318\n",
      "Train Epoch: 6 [54400/80000 (68%)]\tLoss: 0.711710\n",
      "Train Epoch: 6 [57600/80000 (72%)]\tLoss: 0.699815\n",
      "Train Epoch: 6 [60800/80000 (76%)]\tLoss: 0.676503\n",
      "Train Epoch: 6 [64000/80000 (80%)]\tLoss: 0.700230\n",
      "Train Epoch: 6 [67200/80000 (84%)]\tLoss: 0.676272\n",
      "Train Epoch: 6 [70400/80000 (88%)]\tLoss: 0.686714\n",
      "Train Epoch: 6 [73600/80000 (92%)]\tLoss: 0.690182\n",
      "Train Epoch: 6 [76800/80000 (96%)]\tLoss: 0.682952\n",
      "Test set: Average loss: 0.0217, Accuracy: 49.97%\n",
      "Train Epoch: 7 [0/80000 (0%)]\tLoss: 0.685040\n",
      "Train Epoch: 7 [3200/80000 (4%)]\tLoss: 0.693079\n",
      "Train Epoch: 7 [6400/80000 (8%)]\tLoss: 0.689011\n",
      "Train Epoch: 7 [9600/80000 (12%)]\tLoss: 0.678161\n",
      "Train Epoch: 7 [12800/80000 (16%)]\tLoss: 0.680997\n",
      "Train Epoch: 7 [16000/80000 (20%)]\tLoss: 0.693455\n",
      "Train Epoch: 7 [19200/80000 (24%)]\tLoss: 0.709543\n",
      "Train Epoch: 7 [22400/80000 (28%)]\tLoss: 0.700983\n",
      "Train Epoch: 7 [25600/80000 (32%)]\tLoss: 0.692482\n",
      "Train Epoch: 7 [28800/80000 (36%)]\tLoss: 0.689559\n",
      "Train Epoch: 7 [32000/80000 (40%)]\tLoss: 0.701662\n",
      "Train Epoch: 7 [35200/80000 (44%)]\tLoss: 0.691034\n",
      "Train Epoch: 7 [38400/80000 (48%)]\tLoss: 0.705482\n",
      "Train Epoch: 7 [41600/80000 (52%)]\tLoss: 0.701242\n",
      "Train Epoch: 7 [44800/80000 (56%)]\tLoss: 0.702106\n",
      "Train Epoch: 7 [48000/80000 (60%)]\tLoss: 0.712752\n",
      "Train Epoch: 7 [51200/80000 (64%)]\tLoss: 0.693758\n",
      "Train Epoch: 7 [54400/80000 (68%)]\tLoss: 0.696869\n",
      "Train Epoch: 7 [57600/80000 (72%)]\tLoss: 0.684527\n",
      "Train Epoch: 7 [60800/80000 (76%)]\tLoss: 0.682057\n",
      "Train Epoch: 7 [64000/80000 (80%)]\tLoss: 0.695170\n",
      "Train Epoch: 7 [67200/80000 (84%)]\tLoss: 0.682240\n",
      "Train Epoch: 7 [70400/80000 (88%)]\tLoss: 0.683149\n",
      "Train Epoch: 7 [73600/80000 (92%)]\tLoss: 0.686460\n",
      "Train Epoch: 7 [76800/80000 (96%)]\tLoss: 0.701848\n",
      "Test set: Average loss: 0.0217, Accuracy: 49.41%\n",
      "Train Epoch: 8 [0/80000 (0%)]\tLoss: 0.686381\n",
      "Train Epoch: 8 [3200/80000 (4%)]\tLoss: 0.698608\n",
      "Train Epoch: 8 [6400/80000 (8%)]\tLoss: 0.701531\n",
      "Train Epoch: 8 [9600/80000 (12%)]\tLoss: 0.691502\n",
      "Train Epoch: 8 [12800/80000 (16%)]\tLoss: 0.703556\n",
      "Train Epoch: 8 [16000/80000 (20%)]\tLoss: 0.689507\n",
      "Train Epoch: 8 [19200/80000 (24%)]\tLoss: 0.681524\n",
      "Train Epoch: 8 [22400/80000 (28%)]\tLoss: 0.711810\n",
      "Train Epoch: 8 [25600/80000 (32%)]\tLoss: 0.704348\n",
      "Train Epoch: 8 [28800/80000 (36%)]\tLoss: 0.698612\n",
      "Train Epoch: 8 [32000/80000 (40%)]\tLoss: 0.690662\n",
      "Train Epoch: 8 [35200/80000 (44%)]\tLoss: 0.687183\n",
      "Train Epoch: 8 [38400/80000 (48%)]\tLoss: 0.673996\n",
      "Train Epoch: 8 [41600/80000 (52%)]\tLoss: 0.679422\n",
      "Train Epoch: 8 [44800/80000 (56%)]\tLoss: 0.701217\n",
      "Train Epoch: 8 [48000/80000 (60%)]\tLoss: 0.682730\n",
      "Train Epoch: 8 [51200/80000 (64%)]\tLoss: 0.655573\n",
      "Train Epoch: 8 [54400/80000 (68%)]\tLoss: 0.711005\n",
      "Train Epoch: 8 [57600/80000 (72%)]\tLoss: 0.687523\n",
      "Train Epoch: 8 [60800/80000 (76%)]\tLoss: 0.659690\n",
      "Train Epoch: 8 [64000/80000 (80%)]\tLoss: 0.700653\n",
      "Train Epoch: 8 [67200/80000 (84%)]\tLoss: 0.685457\n",
      "Train Epoch: 8 [70400/80000 (88%)]\tLoss: 0.697178\n",
      "Train Epoch: 8 [73600/80000 (92%)]\tLoss: 0.691224\n",
      "Train Epoch: 8 [76800/80000 (96%)]\tLoss: 0.690695\n",
      "Test set: Average loss: 0.0217, Accuracy: 49.50%\n",
      "Train Epoch: 9 [0/80000 (0%)]\tLoss: 0.696305\n",
      "Train Epoch: 9 [3200/80000 (4%)]\tLoss: 0.686732\n",
      "Train Epoch: 9 [6400/80000 (8%)]\tLoss: 0.694789\n",
      "Train Epoch: 9 [9600/80000 (12%)]\tLoss: 0.674547\n",
      "Train Epoch: 9 [12800/80000 (16%)]\tLoss: 0.672609\n",
      "Train Epoch: 9 [16000/80000 (20%)]\tLoss: 0.681962\n",
      "Train Epoch: 9 [19200/80000 (24%)]\tLoss: 0.704290\n",
      "Train Epoch: 9 [22400/80000 (28%)]\tLoss: 0.690113\n",
      "Train Epoch: 9 [25600/80000 (32%)]\tLoss: 0.706265\n",
      "Train Epoch: 9 [28800/80000 (36%)]\tLoss: 0.708163\n",
      "Train Epoch: 9 [32000/80000 (40%)]\tLoss: 0.699555\n",
      "Train Epoch: 9 [35200/80000 (44%)]\tLoss: 0.666953\n",
      "Train Epoch: 9 [38400/80000 (48%)]\tLoss: 0.665496\n",
      "Train Epoch: 9 [41600/80000 (52%)]\tLoss: 0.681802\n",
      "Train Epoch: 9 [44800/80000 (56%)]\tLoss: 0.663983\n",
      "Train Epoch: 9 [48000/80000 (60%)]\tLoss: 0.688818\n",
      "Train Epoch: 9 [51200/80000 (64%)]\tLoss: 0.686303\n",
      "Train Epoch: 9 [54400/80000 (68%)]\tLoss: 0.695616\n",
      "Train Epoch: 9 [57600/80000 (72%)]\tLoss: 0.696342\n",
      "Train Epoch: 9 [60800/80000 (76%)]\tLoss: 0.700019\n",
      "Train Epoch: 9 [64000/80000 (80%)]\tLoss: 0.674221\n",
      "Train Epoch: 9 [67200/80000 (84%)]\tLoss: 0.694539\n",
      "Train Epoch: 9 [70400/80000 (88%)]\tLoss: 0.674893\n",
      "Train Epoch: 9 [73600/80000 (92%)]\tLoss: 0.676090\n",
      "Train Epoch: 9 [76800/80000 (96%)]\tLoss: 0.711108\n",
      "Test set: Average loss: 0.0218, Accuracy: 49.73%\n",
      "Train Epoch: 10 [0/80000 (0%)]\tLoss: 0.699218\n",
      "Train Epoch: 10 [3200/80000 (4%)]\tLoss: 0.687564\n",
      "Train Epoch: 10 [6400/80000 (8%)]\tLoss: 0.691227\n",
      "Train Epoch: 10 [9600/80000 (12%)]\tLoss: 0.645349\n",
      "Train Epoch: 10 [12800/80000 (16%)]\tLoss: 0.706800\n",
      "Train Epoch: 10 [16000/80000 (20%)]\tLoss: 0.659608\n",
      "Train Epoch: 10 [19200/80000 (24%)]\tLoss: 0.696287\n",
      "Train Epoch: 10 [22400/80000 (28%)]\tLoss: 0.706461\n",
      "Train Epoch: 10 [25600/80000 (32%)]\tLoss: 0.695980\n",
      "Train Epoch: 10 [28800/80000 (36%)]\tLoss: 0.705576\n",
      "Train Epoch: 10 [32000/80000 (40%)]\tLoss: 0.671325\n",
      "Train Epoch: 10 [35200/80000 (44%)]\tLoss: 0.688474\n",
      "Train Epoch: 10 [38400/80000 (48%)]\tLoss: 0.695139\n",
      "Train Epoch: 10 [41600/80000 (52%)]\tLoss: 0.708660\n",
      "Train Epoch: 10 [44800/80000 (56%)]\tLoss: 0.686681\n",
      "Train Epoch: 10 [48000/80000 (60%)]\tLoss: 0.732017\n",
      "Train Epoch: 10 [51200/80000 (64%)]\tLoss: 0.655082\n",
      "Train Epoch: 10 [54400/80000 (68%)]\tLoss: 0.683465\n",
      "Train Epoch: 10 [57600/80000 (72%)]\tLoss: 0.655049\n",
      "Train Epoch: 10 [60800/80000 (76%)]\tLoss: 0.662911\n",
      "Train Epoch: 10 [64000/80000 (80%)]\tLoss: 0.673016\n",
      "Train Epoch: 10 [67200/80000 (84%)]\tLoss: 0.681011\n",
      "Train Epoch: 10 [70400/80000 (88%)]\tLoss: 0.686579\n",
      "Train Epoch: 10 [73600/80000 (92%)]\tLoss: 0.696880\n",
      "Train Epoch: 10 [76800/80000 (96%)]\tLoss: 0.680420\n",
      "Test set: Average loss: 0.0218, Accuracy: 49.45%\n"
     ]
    }
   ],
   "source": [
    "# 设置设备并实例化模型，定义损失函数和优化器，开始训练和测试\n",
    "device = torch.device(\"cuda\" )\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LSTMModel(input_size, hidden_size=16, num_layers=2, output_size=output_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "for epoch in range(1, 11):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-03T14:30:39.490368100Z",
     "start_time": "2024-02-03T14:29:48.325558300Z"
    }
   },
   "id": "68df0408a26043b",
   "execution_count": 57
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
