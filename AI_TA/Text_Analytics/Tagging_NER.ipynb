{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Pos Tagging-NER-Dependency Parsing\n",
    "INCLUDE:\n",
    "\n",
    "- [0. Basic Concepts](#1-basic-concepts)\n",
    "    * [1. 隐马尔可夫模型（HMM）](#2-隐马尔可夫模型hmm)\n",
    "    * [2. 条件随机场（CRF）](#3-条件随机场crf)\n",
    "    * [3. POS Tagging：词性标注](#4-pos-tagging)\n",
    "    * [4. Dependency Parsing：依存句法分析](#5-dependency-parsing)\n",
    "    * [5. NER: Named Entity Recognition：命名实体识别](#5-ner-named-entity-recognition)\n",
    "    * [6. Relation Extraction：实体关系提取](#6-relation-extraction)\n",
    "- [1. Data Preprocessing (for POS Tagging--HMM)](#6-data-preprocessing-for-pos-tagging--hmm)\n",
    "    * [1.1 Getting the Data](#7-getting-the-data)\n",
    "    * [1.2 Splitting the Data into Train and Test Sets](#8-splitting-the-data-into-train-and-test-sets)\n",
    "    * [1.3 Encode the Tokens(单词编码)](#9-encode-the-tokens)\n",
    "    * [1.4 Encode the Tags(词性编码)](#10-encode-the-tags)\n",
    "- [2. Hidden Markov Model (HMM) for POS Tagging](#11-hidden-markov-model--hmm--for-pos-tagging)\n",
    "    - Transition Matrix & Start State Probabilities\n",
    "    - Observation Matrix\n",
    "    - Viterbi Algorithm: Predicting the Most Likely Sequence，即预测最可能的词性标记序列POS Tagging\n",
    "    - Evaluation\n",
    "    - NLTK库自带的HMM模型\n",
    "- [3. Conditional Random Fields (CRF) for NER](#12-conditional-random-fields--crf--for-ner)\n",
    "    - 数据准备：使用包含NER标注的数据集\n",
    "    - NLTK库实现CRF模型：`CRFTagger`\n",
    "    - 自定义实体提取器和评估函数：`extract_spans`和`cal_span_level_f1`\n",
    "    - 优化CRFTagger: 自定义CRF类并添加`_get_features`来获取更多特征\n",
    "    - 进一步优化：创建一个CRF类的子类，添加POS作为新特征\n",
    "- [4. Dependency Parsing via spacy](#13-dependency-parsing-via-spacy)\n",
    "    - 依存句法分析\n",
    "    - 使用spacy库进行依存句法分析\n",
    "\n",
    "上述都属于数据预处理内容，一般顺序为：Tokenization -> POS Tagging -> NER -> Dependency Parsing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d42edba3c10679dc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datasets import load_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T20:09:53.300974Z",
     "start_time": "2024-03-18T20:09:52.688966Z"
    }
   },
   "id": "81a3a8e8f88ed814",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0. Basic Concepts\n",
    "#### 判别模型 Discriminative Model\n",
    "判别模型指的是直接对条件概率分布进行建模，而不是对联合概率分布进行建模。CRF是一种判别模型，它直接对输出序列的条件概率分布进行建模，而不是对输入和输出序列的联合概率分布进行建模。CRF的目标是学习一个条件概率分布$\\(P(Y|X)\\)$，其中$\\(X\\)$是输入序列，$\\(Y\\)$是输出序列。\n",
    "\n",
    "#### 生成模型 Generative Model\n",
    "与判别模型相对的是生成模型，它对输入和输出序列的联合概率分布进行建模。HMM是一种生成模型，它对输入和输出序列的联合概率分布进行建模。HMM的目标是学习一个联合概率分布$\\(P(X, Y)\\)$，其中$\\(X\\)$是输入序列，$\\(Y\\)$是输出序列。\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31d04078f22191bc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. 隐马尔可夫模型（HMM）\n",
    "\n",
    "在本实验室中，我们将研究隐马尔可夫模型（HMM），以建立序列数据模型。HMM 基于马尔可夫假设，即现在的状态 $z_n$ 足以预测未来的状态 $y_{n+1}$，因此过去的状态 $y_{0:n-1}$ 可以被遗忘。\n",
    "\n",
    "通常，我们感兴趣的状态是无法直接观察到的--它们是 \"隐藏 \"的。（POS）标记就是我们想要预测的隐藏状态。我们只能观察单词，并利用它们来推断标签。\n",
    "\n",
    "马尔可夫模型基于马尔可夫链的数学理论。在时间序列中，假设有一个状态空间\\(S = \\{s_1, s_2, ..., s_N\\}\\)，其中每个\\(s_i\\)代表一个可能的状态。马尔可夫模型的核心是状态转移概率，即从一个状态转移到另一个状态的概率。\n",
    "\n",
    "状态转移概率表示为：\n",
    "$$\\[P(s_{t+1} = j | s_t = i)\\]$$\n",
    "\n",
    "这表示在时间$\\(t\\)$处于状态$\\(i\\)$的条件下，在时间$\\(t+1\\)$转移到状态$\\(j\\)$的概率。\n",
    "\n",
    "整个模型可以通过一个状态转移矩阵$\\(A\\)$来描述，其中矩阵的元素$\\(a_{ij}\\)$表示从状态$\\(i\\)$转移到状态$\\(j\\)$的概率：\n",
    "$$\\[A = [a_{ij}] = P(s_{t+1} = j | s_t = i)\\]$$\n",
    "\n",
    "\n",
    "#### 数学表达\n",
    "隐马尔可夫模型包括三个主要的参数集：状态转移概率矩阵$\\(A\\)$，观测概率矩阵$\\(B\\)$，和初始状态概率向量$\\(\\pi\\)$。\n",
    "\n",
    "1. **状态转移概率矩阵**$\\(A\\)$:$\\[A = [a_{ij}]\\]$\n",
    "其中，$\\(a_{ij} = P(q_{t+1} = j | q_t = i)\\)$，表示在时间$\\(t\\)$处于状态$\\(i\\)$的条件下，在时间$\\(t+1\\)$转移到状态$\\(j\\)$的概率。\n",
    "\n",
    "2. **观测概率矩阵**$\\(B\\)$:$\\[B = [b_j(k)]\\]$\n",
    "其中，$\\(b_j(k) = P(o_t = v_k | q_t = j)\\)$，表示在时间$\\(t\\)$处于状态$\\(j\\)$的条件下，观测到$\\(v_k\\)$的概率。$\\(o_t\\)$是在时间$\\(t\\)$的观测值，$\\(v_k\\)$是所有可能观测值的集合。\n",
    "\n",
    "3. **初始状态概率向量**$\\(\\pi\\)$: $\\[\\pi = [\\pi_i]\\]$\n",
    "其中，$\\(\\pi_i = P(q_1 = i)\\)$，表示在时间$\\(t=1\\)$时，系统处于状态$\\(i\\)$的概率。\n",
    "\n",
    "因此，一个HMM可以由参数$\\(\\lambda = (A, B, \\pi)\\)$完全定义。\n",
    "\n",
    "#### HMM 和 马尔可夫模型的区别\n",
    "\n",
    "- **马尔可夫模型**关注于状态的转移概率，核心思想是未来的状态只依赖于当前的状态，而与之前的历史状态无关。\n",
    "  \n",
    "- **隐马尔可夫模型（HMM**则更进一步，引入了隐藏状态和观测值之间的关系。在HMM中，我们不能直接观察到状态，而是通过与这些状态相关联的观测值来推断状态的序列。这种模型特别适用于序列数据的处理，例如语音识别、自然语言处理中的词性标注等，其中系统的内部状态不能直接观察到，但可以通过观察到的行为（如单词序列）来推断。\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "50cc2723cf89b25"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. 条件随机场（CRF）\n",
    "CRF是一种判别模型，用于标注或分割序列数据。CRF是一种无向图模型，它可以用于对序列数据进行建模，特别适用于标注或分割任务。CRF的核心思想是对给定的输入序列，预测输出序列的条件概率分布。\n",
    "\n",
    "CRF的输入是一个输入序列$\\(X = (x_1, x_2, ..., x_n)\\)$，输出是一个输出序列$\\(Y = (y_1, y_2, ..., y_n)\\)$。CRF的目标是学习一个条件概率分布$\\(P(Y|X)\\)$，即给定输入序列$\\(X\\)$，预测输出序列$\\(Y\\)$的概率。\n",
    "\n",
    "\n",
    "CRF的关键概念是通过一组特征函数来定义模型，这些特征函数可以捕捉数据中的不同模式和依赖关系，从而帮助模型了解不同的输入观测序列应该如何影响相应的标签序列。\n",
    "\n",
    "在CRF中，特征函数通常是二元或实值的，用于量化特定的观测与标签配置之间的关系。例如，一个特征函数可能检查一个词的前缀和后缀是否与特定的标签相匹配。特征函数的权重决定了这些模式在最终概率计算中的重要性。\n",
    "\n",
    "训练CRF模型涉及最大化条件对数似然，这通常通过迭代优化算法如梯度下降完成。优化的目标是找到特征函数权重的最佳设置，以便模型最准确地预测训练数据中观察到的标签序列。\n",
    "\n",
    "在使用CRF进行预测时，对于一个新的观测序列，我们会计算所有可能标签序列的条件概率，并选择概率最高的序列作为预测结果。这通常通过动态规划算法实现，如前面提到的维特比算法。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cde19adcdff6a3aa"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. POS Tagging \n",
    "POS标注（Part-of-Speech tagging）是自然语言处理（NLP）中的一个基本任务，它涉及到将文本中的每个单词与一个特定的词性（如名词、动词、形容词等）相关联。POS标注的目标是识别出句子中每个单词的词性，这对于理解句子结构和意义非常重要。在很多NLP应用中，比如句法分析、语义分析、机器翻译和信息检索等，POS标注都是一个重要的预处理步骤。\n",
    "\n",
    "POS标注通常利用统计模型或机器学习算法来自动完成，这些模型会根据单词本身、上下文信息以及单词之间的关系来预测每个单词的词性。随着深度学习的发展，基于深度学习的模型（如循环神经网络RNN和长短期记忆网络LSTM）在POS标注任务中也表现出了优异的性能。\n",
    "\n",
    "我们将使用HMM模型来完成POS标注任务。HMM模型是一种生成式模型，它可以用于对观察序列进行建模，即根据观察序列预测隐藏状态序列。在POS标注任务中，观察序列是单词序列，隐藏状态序列是词性标记序列。HMM模型的训练过程涉及到估计转移概率矩阵和观察概率矩阵，这两个矩阵分别表示了词性标记之间的转移概率和单词与词性标记之间的关联概率。训练好的HMM模型可以用于对新的文本进行词性标注。\n",
    "\n",
    "---\n",
    "\n",
    "条件随机场（CRF）和隐马尔可夫模型（HMM）都可以用来进行词性标注（POS Tagging）。\n",
    "\n",
    "**隐马尔可夫模型（HMM）** 是早期在自然语言处理领域，特别是在词性标注任务中非常流行的生成模型。HMM假设每个词的标签只依赖于它前面的一个或几个标签（一阶或高阶马尔可夫链），并且每个词的出现只依赖于它的标签，而忽略了其他词的影响。HMM通常使用维特比算法来预测给定句子中单词的词性标签序列。\n",
    "\n",
    "**条件随机场（CRF）** 是一种判别模型，后来在POS Tagging中变得非常流行，因为它能够考虑更复杂的特征并捕捉数据中的长距离依赖关系。与HMM不同，CRF不对观测序列（即单词序列）进行建模，而是直接建模标签序列条件于观测序列的概率。这种模型特别适合于词性标注，因为在一个句子中，一个词的标签可能不仅仅依赖于它相邻的词的标签，而且还依赖于整个句子的上下文信息。\n",
    "\n",
    "总的来说，HMM和CRF都可以用于词性标注，但CRF由于其能够考虑更广泛的上下文信息和特征依赖，通常会在这项任务上提供更准确的结果。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a42de9309e3bc4f4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Dependency Parsing\n",
    "依存句法分析（Dependency Parsing）是自然语言处理（NLP）中的一个重要任务，它旨在分析文本中单词之间的依存关系。在这种分析中，句子被表示为一个依存树（或图），其中节点代表句子中的单词，而边则代表单词之间的依存关系，通常是语法关系。\n",
    "\n",
    "在依存句法分析中，每个句子都有一个根（通常是动词或句子的主要谓语），其他单词通过各种类型的依存关系与这个根连接。这些依存关系可以是主谓关系、定中关系、动宾关系等，它们帮助揭示句子的内在结构和意义。\n",
    "\n",
    "依存句法分析的目的是帮助机器理解句子中词语的语法功能以及它们之间的相互作用，这对于很多NLP任务来说都是非常重要的基础，如信息提取、问答系统、机器翻译和文本摘要等。依存句法分析的结果可以提供丰富的语法信息，帮助提高这些任务的性能。依存句法分析通常在自然语言处理（NLP）的**文本预处理**阶段执行，特别是在需要深入理解句子结构和语义关系的任务中。执行这个任务可以帮助后续的分析，如情感分析、实体识别、关系抽取等，提供结构化的语言信息。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9869a2bc175a90a7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. NER: Named Entity Recognition\n",
    "命名实体识别（Named Entity Recognition, NER）是自然语言处理（NLP）中的一项重要任务，其目的是从文本中识别出具有特定意义的实体，如人名、地点名、组织名、时间表达式、数值表达式等，并将这些实体分类到预定义的类别中。NER在信息抽取、问答系统、内容摘要、情感分析等多种应用场景中扮演着关键角色。\n",
    "\n",
    "#### NER的实现方法\n",
    "\n",
    "1. **基于规则的方法**：这种方法依赖于手工编写的规则来识别和分类实体。规则可能包括词性标签、实体边界标识符（如大写字母开头）、上下文线索（比如位于“在”和“之间”的词组可能是地点）等。基于规则的方法简单直观，但缺乏灵活性，维护成本高，且难以覆盖所有情况。\n",
    "\n",
    "2. **基于统计的方法**：利用机器学习模型，如隐马尔可夫模型（HMM）、条件随机场（CRF）、支持向量机（SVM）等，根据标注数据自动学习识别命名实体的规则。这些方法能够考虑到词语的上下文信息，对不确定性和多样性有更好的处理能力。\n",
    "\n",
    "3. **基于深度学习的方法**：近年来，深度学习模型，特别是循环神经网络（RNN）、长短期记忆网络（LSTM）、双向长短期记忆网络（BiLSTM）和Transformer架构（如BERT、GPT），已经成为NER任务的主流方法。这些模型能够自动从大量数据中学习复杂的特征表示，捕捉更深层次的语言规律和实体间的依赖关系，从而提高识别的准确性和鲁棒性。\n",
    "\n",
    "#### 应用实例\n",
    "\n",
    "- **信息抽取**：从新闻、报告等文档中自动抽取关键实体，如人物、组织、地点，用于构建知识库或提供快速检索。\n",
    "- **问答系统**：理解用户问题中的关键实体，并基于这些实体从数据库或知识库中检索正确的答案。\n",
    "- **舆情分析**：识别社交媒体或评论中提及的产品、品牌或公司等实体，用于分析公众情感倾向或品牌影响力。\n",
    "- **自动摘要**：识别文本中的关键实体和事件，生成内容摘要或概述。\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e52b8306501bcb64"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Relation Extraction\n",
    "关系抽取（Relation Extraction）是自然语言处理（NLP）中的一个重要任务，旨在从文本中识别实体之间的语义关系。例如，给定一句话\"苹果公司是由乔布斯创立的\"，关系抽取的任务是识别出\"苹果公司\"和\"乔布斯\"之间存在\"创立者\"的关系。\n",
    "\n",
    "关系抽取的方法大致可以分为三类：\n",
    "\n",
    "1. **基于规则的方法**：通过预定义的语言规则来识别实体之间的关系。这种方法简单直观，但缺乏灵活性，难以覆盖语言的多样性。\n",
    "\n",
    "2. **基于特征的机器学习方法**：通过手工设计特征，使用传统的机器学习算法（如支持向量机、随机森林等）进行关系抽取。这种方法比基于规则的方法灵活，但需要大量的人工劳动和领域知识。\n",
    "\n",
    "3. **基于深度学习的方法**：使用诸如卷积神经网络（CNN）、循环神经网络（RNN）、Transformer等深度神经网络自动从数据中学习表示，进行关系抽取。深度学习方法减少了对手工特征的依赖，能够更好地捕捉文本中的复杂模式，是当前关系抽取研究的热点。\n",
    "\n",
    "关系抽取在信息抽取、知识图谱构建、问答系统等领域有着广泛的应用。随着深度学习技术的发展，关系抽取的准确性和应用范围都在不断扩大。\n",
    "\n",
    "### 1. Classifier based approach\n",
    "基于分类器的方法，是一种常用的关系抽取方法。它通常包括两个主要步骤：\n",
    "\n",
    "**列出所有命名实体对**：首先，在句子中识别并列出所有命名实体（比如人名、组织名等）的组合。\n",
    "**特征向量**：为每对实体生成的特征向量，它包含了对于分类器来说有用的信息，比如词性标注、依存关系、实体周围的词汇等。\n",
    "**对每一对实体应用分类器**：对于每一对实体，使用一个分类器来预测它们之间是否存在某种关系。这里的类别标签可以是“无关系”或者是某种具体的关系类型\n",
    "\n",
    "这种基于分类器的方式通常是监督学习模型。在监督学习中，模型会从标记好的训练数据中学习，这些数据包含了输入特征向量以及每个实例的正确标签（在关系抽取的场景中是实体对之间的关系类型）。模型的目的是通过学习这些示例来推广到新的未标记数据，预测实体对之间的关系类型。\n",
    "\n",
    "### 2. Deep Learning based approach\n",
    "如果使用深度学习方法，可以使用深度神经网络来自动学习特征表示。这种方式可直接输入原始数据（包含标记好的关系标签）到神经网络中，神经网络会自动学习特征表示并预测实体对之间的关系。\n",
    "\n",
    "如果你的任务是多分类问题（即每个示例只能归于一个类别），你可能会在这个模型的输出上应用`softmax`函数来获取每个类别的概率。如果是多标签分类问题（即每个示例可能同时属于多个类别），则可能会使用`sigmoid`函数来独立地计算每个类别的概率。在实际应用中，选择哪种函数取决于你的具体任务需求。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "207eee291b96c32e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Data Preprocessing (for POS Tagging--HMM)\n",
    "### 1.1 Getting the Data\n",
    "\n",
    "首先需要对文本进行数值转换。之前的Bag-of-Words模型是将文本转换为词频向量，而在POS标注任务中，需要将文本转换为词性标记序列。我们会先使用词性标注函数处理所有文本，得到一个list，该list每个元素是一个句子list，句子list中每个元素是一个元组，元组的第一个元素是单词，第二个元素是词性标记。\n",
    "\n",
    "然后分别对单词和词性进行数值转换：\n",
    "- 单词：使用gensim库中的Dictionary类，将单词转换为整数编码。\n",
    "- 词性：使用sklearn库中的LabelEncoder类，将词性标记转换为整数编码。\n",
    "\n",
    "---\n",
    "\n",
    "1. `nltk.download('brown')`：这行代码下载Brown语料库。Brown语料库是第一个大规模的英语电子语料库，包含了不同风格和领域的文本，常用于语言学研究和自然语言处理任务中。\n",
    "\n",
    "2. `nltk.download('universal_tagset')`：这行代码下载通用词性标记集。词性标记集（tagset）是一套预定义的词性标记，用于POS标注。通用词性标记集（Universal Tagset）是一个简化的、跨语言的词性标记集，旨在促进不同语言间的词性标注研究和应用。\n",
    "\n",
    "3. `nltk_data = list(brown.tagged_sents(tagset='universal'))`：这行代码加载了Brown语料库，并将其句子以及这些句子中单词的词性标注（使用通用词性标记集进行标注）转换为一个Python列表。`brown.tagged_sents(tagset='universal')`这个函数调用会返回一个包含已标注句子的迭代器，每个句子都是一个单词及其词性标记的列表。通过将其转换为列表，便于后续的处理和分析。\n",
    "\n",
    "核心内容在于使用`list(brown.tagged_sents(tagset='universal'))`将Brown语料库中的句子和词性标注转换为一个Python列表。这个列表的每个元素都是一个句子，每个句子都是一个元组列表，每个元组包含一个单词（token）和一个词性标记（tag）。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "105854a2cf699232"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\yhb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\yhb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('brown')  # download Brown corpus\n",
    "nltk.download('universal_tagset')   # download the POS tags data\n",
    "nltk_data = list(brown.tagged_sents(tagset='universal'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T13:21:17.617367Z",
     "start_time": "2024-03-17T13:21:08.938567Z"
    }
   },
   "id": "5214b1cd91000522",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.2 Splitting the Data into Train and Test Sets\n",
    "\n",
    "- 提取出每个句子中的单词（token）和词性标记（tag），分别存储在两个列表中。\n",
    "- 对训练集和测试集都进行上述处理。\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd1e5d99ad9bef73"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 45872\n",
      "Number of test sentences: 11468\n",
      "Number of training sentences in train_toks: 45872\n",
      "Number of test sentences in test_toks: 11468\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and test sets\n",
    "train_set, test_set = train_test_split(\n",
    "    nltk_data,\n",
    "    train_size=0.80,  # use 80% as the training data\n",
    "    test_size=0.20,\n",
    "    random_state=101\n",
    ")\n",
    "print(f'Number of training sentences: {len(train_set)}')\n",
    "print(f'Number of test sentences: {len(test_set)}')\n",
    "\n",
    "# 下面是train_set[0]的输出结果：\n",
    "# 可以认为train_set每个元素是一个句子，每个句子是一个元组列表，每个元组包含一个单词（token）和一个词性标记（tag）。\n",
    "# [('A', 'DET'),\n",
    "#  ('Newfoundland', 'NOUN'),\n",
    "#  ('sat', 'VERB'),\n",
    "#  ('solemnly', 'ADV'),\n",
    "#  ('beside', 'ADP'),\n",
    "#  ('a', 'DET'),\n",
    "#  ('doghouse', 'NOUN'),\n",
    "#  ('half', 'PRT'),\n",
    "#  ('his', 'DET'),\n",
    "#  ('size', 'NOUN'),\n",
    "#  ('.', '.')]\n",
    "# # \n",
    "# token为单词，即train/tes_set中的每个元组的第一个元素；\n",
    "# tag为词性标记，即train/tes_set中的每个元组的第二个元素。\n",
    "\n",
    "# Separate the labels from the text\n",
    "train_toks = []  # each item in the list is a list of tokens in a document\n",
    "train_tags = []  # each item in the list is a list of corresponding tags\n",
    "for tagged_sentence in train_set:\n",
    "    sentence_toks = []\n",
    "    sentence_tags = []\n",
    "    for token, tag in tagged_sentence:\n",
    "        sentence_toks.append(token)\n",
    "        sentence_tags.append(tag)\n",
    "\n",
    "    train_toks.append(sentence_toks)\n",
    "    train_tags.append(sentence_tags)\n",
    "\n",
    "test_toks = []\n",
    "test_tags = []\n",
    "for tagged_sentence in test_set:\n",
    "    sentence_toks = []\n",
    "    sentence_tags = []\n",
    "    for token, tag in tagged_sentence:\n",
    "        sentence_toks.append(token)\n",
    "        sentence_tags.append(tag)\n",
    "    test_toks.append(sentence_toks)\n",
    "    test_tags.append(sentence_tags)\n",
    "\n",
    "print(f'Number of training sentences in train_toks: {len(train_toks)}')\n",
    "print(f'Number of test sentences in test_toks: {len(test_toks)}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T13:21:23.448322Z",
     "start_time": "2024-03-17T13:21:22.928675Z"
    }
   },
   "id": "4d62d54eb3bb91a8",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.3 Encode the Tokens(单词)\n",
    "使用`gensim`库中的`Dictionary`类，将训练集和测试集中的单词转换为整数编码。`Dictionary`类是一个映射，用于将单词映射到整数编码。`doc2idx`方法可以将文档转换为整数编码的列表。\n",
    "- train_toks_encoded: list; 训练集中的每个句子的每个词用对应的整数编码（idx）表示，这个列表的每个元素都是一个列表，代表了一个句子。\n",
    "\n",
    "```python\n",
    "train_toks_encoded[0]=[1, 2, 8, 10, 4, 3, 5, 6, 7, 9, 0]\n",
    "# 表述train set的第一个句子的每个词的整数编码\n",
    "```\n",
    "\n",
    "预词袋模型类似，这都是将文本转换为数学表示的方法。不同的是，预词袋模型主要关注于词汇的出现频率而忽略了词序，而`Dictionary`和词汇ID映射保留了词序信息。\n",
    "总结来说，词袋模型主要关注于词汇的出现频率而忽略了词序，适合于主题识别等任务；而Gensim的Dictionary和词汇ID映射保留了词序信息，适合于需要文本结构信息的任务\n",
    "\n",
    "\n",
    "1. **建立`Dictionary`对象**：通过`Dictionary(train_toks + test_toks)`这行代码，你创建了一个`Dictionary`对象，它基于提供的文本数据（在这个案例中是训练集和测试集的合并）。这个对象内部构建了一个词汇表，其中包含了所有不重复的词汇以及每个词汇对应的唯一ID。`Dictionary`对象是一种高效的方式来管理词汇和它们的ID映射，这对于文本处理和模型训练都非常有用。\n",
    "\n",
    "2. **使用`doc2idx`方法编码句子**：`doc2idx`方法接受一个词汇列表（即一个句子中的所有词汇）作为输入，并返回一个整数列表。该列表每个元素都是一个列表，其中每个值都是整数，代表了一个单词的映射idx。因此，`train_toks_encoded = [dictionary.doc2idx(sent) for sent in train_toks]`这行代码将训练数据集中的每个句子转换成了由词汇ID组成的列表，同样的，`test_toks_encoded`也经过了相同的处理。\n",
    "\n",
    "通过这种方式，你可以将文本数据转换成数值形式，这是训练机器学习模型的常见和必要步骤。这样处理后的数据，即用词汇ID表示的句子，可以直接用于各种自然语言处理模型，如词嵌入模型、循环神经网络（RNN）等，来进行进一步的分析和学习。这个过程不仅减少了数据的维度（通过去除重复的词汇），也为后续的处理步骤（如词嵌入的学习）提供了便利。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b669003e2acab0ae"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: ['many', 'of', 'their', 'gifted', 'members', 'were', 'prominent', 'in', 'the', 'Vatican', 'as', 'physicians', ',', 'musicians', ',', 'bankers', '.']\n",
      "Encoded sentence: [41, 28, 46, 40, 42, 47, 45, 23, 31, 37, 38, 44, 11, 43, 11, 39, 0]\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# 建立一个Dictionary对象，用于将单词映射到整数编码\n",
    "dictionary = Dictionary(train_toks + test_toks)\n",
    "\n",
    "# 使用dictionary对象的doc2idx方法将训练集和测试集中的单词转换为整数编码\n",
    "train_toks_encoded = [dictionary.doc2idx(sent) for sent in train_toks]\n",
    "test_toks_encoded = [dictionary.doc2idx(sent) for sent in test_toks]\n",
    "\n",
    "# 查看一个句子的编码结果\n",
    "print(f'Original sentence: {train_toks[3]}')\n",
    "print(f'Encoded sentence: {train_toks_encoded[3]}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T13:21:34.588987Z",
     "start_time": "2024-03-17T13:21:33.075723Z"
    }
   },
   "id": "54b6e98e085f4ece",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.4 Encode the Tags(词性)\n",
    "使用sklearn库中的`LabelEncoder`类，将训练集和测试集中的词性标记转换为整数编码。\n",
    "- train_tags_encoded: list; 训练集中的每个句子的每个词性标记用对应的整数编码（idx）表示，这个列表的每个元素都是一个array，代表了一个句子,array的每个值是句子中每个词性标记的整数编码。\n",
    "```python\n",
    "train_tags_encoded[0]=[ 5  6 10  3  2  5  6  9  5  6  0]\n",
    "# 表述train set的第一个句子的每个词的词性标记的整数编码\n",
    "``` "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "493fa2805cbe6f19"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 建立一个LabelEncoder对象，用于将词性标记映射到整数编码\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# 使用label_encoder对象的fit_transform方法将训练集和测试集中的词性标记转换为整数编码\n",
    "label_encoder.fit([tag for sentence in train_tags for tag in sentence]) # 用训练集中的词性标记来训练label_encoder\n",
    "train_tags_encoded = [label_encoder.transform(sentence) for sentence in train_tags] # 将训练集中的词性标记转换为整数编码\n",
    "test_tags_encoded = [label_encoder.transform(sentence) for sentence in test_tags] # 将测试集中的词性标记转换为整数编码"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T13:24:10.117723Z",
     "start_time": "2024-03-17T13:24:07.643925Z"
    }
   },
   "id": "351fd6fa645b8629",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of tags: ['.' 'ADJ' 'ADP' 'ADV' 'CONJ' 'DET' 'NOUN' 'NUM' 'PRON' 'PRT' 'VERB' 'X']\n",
      "Mappings from tags to IDs:\n",
      ".: 0\n",
      "ADJ: 1\n",
      "ADP: 2\n",
      "ADV: 3\n",
      "CONJ: 4\n",
      "DET: 5\n",
      "NOUN: 6\n",
      "NUM: 7\n",
      "PRON: 8\n",
      "PRT: 9\n",
      "VERB: 10\n",
      "X: 11\n"
     ]
    }
   ],
   "source": [
    "print(f\"List of tags: {label_encoder.classes_}\") # 。classes_属性是一个列表，包含了所有不重复的词性标记\n",
    "print(f\"Mappings from tags to IDs:\")\n",
    "for tag in label_encoder.classes_:\n",
    "    print(f\"{tag}: {label_encoder.transform([tag])[0]}\") # 给出上述不重复的词性标记的整数编码，共有12个不同的词性标记[0--11]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T13:24:12.141569Z",
     "start_time": "2024-03-17T13:24:12.136588Z"
    }
   },
   "id": "4e2f1362990c0aa7",
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Hidden Markov Model (HMM) for POS Tagging\n",
    "HMMs的两个主要组成部分是转移模型(transition matrix)和观察模型(observation matrix)。\n",
    "由上述数据集进行词性标记结果显示，该数据集一共由12个不同的词性标记。\n",
    "\n",
    "#### Transition Matrix: 12x12\n",
    "转移模型估计$P(tag_{t+1}|tag_t)$，即给定当前标记的情况下下一个标记的概率。\n",
    "具体实现中，我们计算训练集中每个标记对的频率，即由标记$tag_t$转移到标记$tag_{t+1}$的频率。然后将这些频率归一化(除以每个标记的总数)以得到概率。\n",
    "transition矩阵的每行表示$t-1$时刻的标记，每列表示$t$时刻标记，矩阵中的每个元素表示从第一个标记转移到第二个标记的频率。\n",
    "#### Observation Matrix: 12xV\n",
    "对于离散特征，例如标记，观察模型估计$P(word|tag_t)$，即给定当前标记的情况下观察到一个词的概率。\n",
    "具体实现中，我们计算训练集中每个标记对应的词的频率，即由标记$tag_t$生成词$word$的频率。然后将这些频率归一化(除以每个标记的总数)以得到概率。\n",
    "observations矩阵每行表示一个标记，每列表示一个词，矩阵中的每个元素表示对应标记生成对应词的频率。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "271bdc700ebfe603"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1 Transition Matrix & Start State Probabilities\n",
    "#### Transition Matrix\n",
    "- 计算转移矩阵，即t-1时刻的标记转移到t时刻标记的概率。先填充转移次数。\n",
    "- 计算转移概率矩阵A，每行元素=转移次数矩阵的每行元素除以每行的总和。\n",
    "结果是一个12x12的矩阵。（12是不同的词性标记的数量）\n",
    "#### Start State Probabilities\n",
    "- 计算起始状态概率向量π，即每个标记在序列开始时出现的概率。\n",
    "- 计算每个标记在序列开始时出现的次数，然后除以总次数得到概率。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a854421d33c6fc00"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "转移概率矩阵A:\n",
      " [[1.73216366e-01 4.65903639e-02 1.04461654e-01 7.00705915e-02\n",
      "  1.12247276e-01 1.10204921e-01 1.33931876e-01 2.00123364e-02\n",
      "  7.43746145e-02 2.91549585e-02 1.23761223e-01 1.97381948e-03]\n",
      " [1.00785399e-01 5.64290523e-02 8.77851747e-02 9.63422844e-03\n",
      "  3.78038746e-02 5.87927294e-03 6.53033136e-01 7.25559129e-03\n",
      "  3.75495549e-03 1.95078166e-02 1.76228589e-02 5.08639390e-04]\n",
      " [9.75938952e-03 8.21847882e-02 2.06089236e-02 1.52533721e-02\n",
      "  1.90342703e-03 4.55766951e-01 2.58355612e-01 3.02904457e-02\n",
      "  6.95443023e-02 1.45179571e-02 4.13908860e-02 4.23945112e-04]\n",
      " [1.70394487e-01 1.37658051e-01 1.40735455e-01 9.66260063e-02\n",
      "  1.70149187e-02 7.32555806e-02 3.25134358e-02 1.35361149e-02\n",
      "  4.78112526e-02 2.88785318e-02 2.41464666e-01 1.11500123e-04]\n",
      " [2.02334886e-02 1.11333377e-01 7.23421001e-02 9.19525152e-02\n",
      "  2.29553355e-04 1.51898734e-01 2.43949629e-01 1.89545484e-02\n",
      "  6.74558930e-02 2.51852824e-02 1.95874598e-01 5.90280055e-04]\n",
      " [1.29902207e-02 2.40081235e-01 9.11145061e-03 1.78295354e-02\n",
      "  5.94622780e-04 5.86389542e-03 6.26110344e-01 9.66033317e-03\n",
      "  1.02275118e-02 2.04916158e-03 6.40271514e-02 1.45453880e-03]\n",
      " [2.84371312e-01 1.31101076e-02 2.45214150e-01 2.65118547e-02\n",
      "  5.98681243e-02 1.55571454e-02 1.50449535e-01 8.21147510e-03\n",
      "  1.97722478e-02 1.78127948e-02 1.58756704e-01 3.64549394e-04]\n",
      " [2.74649064e-01 5.94919786e-02 1.30765374e-01 2.08054813e-02\n",
      "  3.96056150e-02 1.27840909e-02 3.79344920e-01 2.22259358e-02\n",
      "  8.77339572e-03 5.26403743e-03 4.60394385e-02 2.50668449e-04]\n",
      " [1.02425122e-01 9.79718554e-03 5.53222892e-02 5.35155355e-02\n",
      "  1.13240196e-02 1.73041199e-02 9.05921572e-03 1.04333664e-03\n",
      "  8.39758760e-03 2.37677176e-02 7.08018424e-01 2.54472352e-05]\n",
      " [7.81184199e-02 1.91611219e-02 9.00783290e-02 3.52059294e-02\n",
      "  1.20441337e-02 8.40562621e-02 3.64693001e-02 5.34826918e-03\n",
      "  6.86431399e-03 1.06123137e-02 6.21957382e-01 8.42247115e-05]\n",
      " [8.05794912e-02 5.74273061e-02 1.70304508e-01 1.03477634e-01\n",
      "  1.46108689e-02 1.62655773e-01 9.77857118e-02 8.96700882e-03\n",
      "  5.46603042e-02 6.52339593e-02 1.84098321e-01 1.99114285e-04]\n",
      " [2.75830258e-01 2.76752768e-03 5.53505535e-02 9.22509225e-03\n",
      "  2.21402214e-02 5.53505535e-03 5.62730627e-02 9.22509225e-04\n",
      "  6.45756458e-03 8.30258303e-03 5.81180812e-02 4.99077491e-01]]\n",
      "起始状态概率π:\n",
      " [0.08887775 0.03468347 0.12319062 0.09077433 0.04861353 0.21187217\n",
      " 0.14117544 0.01757063 0.16051186 0.03686345 0.04540896 0.0004578 ]\n"
     ]
    }
   ],
   "source": [
    "num_tags = len(label_encoder.classes_)  # number of unique tags\n",
    "transitions = np.zeros((num_tags, num_tags))  # 转移矩阵，初始时用转移次数填充\n",
    "start_states = np.zeros(num_tags) # 开始状态概率，首先用每个状态在序列开始时出现的次数填充\n",
    "\n",
    "# 遍历训练集中的每个句子，计算转移矩阵和开始状态概率 （只关注tag）\n",
    "for sentence_tags in train_tags_encoded:\n",
    "    for i, tag in enumerate(sentence_tags):\n",
    "        if i == 0:\n",
    "            # 若当前标记是句子的第一个标记，则更新初始状态矩阵\n",
    "            start_states[tag] += 1\n",
    "            continue\n",
    "        previous_tag = sentence_tags[i-1]  # 获取前一个标签\n",
    "        # 转移矩阵每个元素值表示从previous_tag转移到tag的次数（之后会计算频率，作为概率矩阵）\n",
    "        transitions[previous_tag, tag] += 1  # 更新转移次数\n",
    "        \n",
    "# 计算转移概率矩阵A\n",
    "transition_probabilities = np.zeros_like(transitions) # np.zeros_like()返回一个与给定数组具有相同形状和类型的零数组\n",
    "for i in range(num_tags):\n",
    "    total = np.sum(transitions[i]) # 计算每一行的总和\n",
    "    if total > 0:\n",
    "        # 转移矩阵A 的每行元素=转移次数矩阵的每行元素除以每行的总和\n",
    "        transition_probabilities[i] = transitions[i] / total \n",
    "\n",
    "# 计算起始状态概率向量π\n",
    "start_state_probabilities = start_states / np.sum(start_states)\n",
    "\n",
    "# 打印结果以验证\n",
    "print(\"转移概率矩阵A:\\n\", transition_probabilities)\n",
    "print(\"起始状态概率π:\\n\", start_state_probabilities)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T13:24:16.510009Z",
     "start_time": "2024-03-17T13:24:16.039947Z"
    }
   },
   "id": "be647a3cef378c87",
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 Observation Matrix\n",
    "观测矩阵是一个12xV的矩阵，V是词汇表的大小。每行对应一个标记，每列对应一个单词。矩阵中的每个元素表示在t时刻为tag_i时，观察到word_j的次数。之后用频率归一化得到概率矩阵B。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f614962ece6d37ee"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "观察概率矩阵B:\n",
      " [[0.33475022 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.00092081 0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "V= len(dictionary.values())  # vocabulary\n",
    "observations = np.zeros((num_tags, V))  # We will first fill this with counts of words given tags\n",
    "\n",
    "for i, sentence_toks in enumerate(train_toks_encoded):\n",
    "    # i=25047; sentence_toks=[178, 1687, 37610, 378, 37608, 37609, 47, 37611, 0]\n",
    "    # 这表示train_toks_encoded第25047个句子中的单词对应的编码\n",
    "    \n",
    "    sentence_tags = train_tags_encoded[i] # 用同样索引i取出\n",
    "    for j, tok in enumerate(sentence_toks):\n",
    "        tag = sentence_tags[j]\n",
    "        observations[tag, tok] += 1        \n",
    "# observations是一个12*V的矩阵，其中每行对应于一个标记，每列对应于一个单词。相当于记录在t时刻为tag_i时，观察到word_j的次数。\n",
    "    \n",
    "# 计算观察概率矩阵B\n",
    "observation_probabilities = np.zeros_like(observations)\n",
    "for i in range(num_tags):\n",
    "    total = np.sum(observations[i])\n",
    "    if total > 0:\n",
    "        observation_probabilities[i] = observations[i] / total\n",
    "\n",
    "# 打印结果以验证\n",
    "print(\"观察概率矩阵B:\\n\", observation_probabilities)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T13:24:21.771963Z",
     "start_time": "2024-03-17T13:24:21.398569Z"
    }
   },
   "id": "96991b36215595e4",
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6c0c976ebba21281"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3 Viterbi Algorithm\n",
    "Viterbi算法是一个动态规划算法，用于在HMM中找到最可能的隐藏状态序列。在POS标注任务中，Viterbi算法用于找到给定一个单词序列的最可能的词性标记序列。\n",
    "\n",
    "Input：\n",
    "- observed_seq: 观测序列，即输入的句子\n",
    "- num_tags: 标记的数量\n",
    "- start_probs: 起始状态概率\n",
    "- transition_probs: 转移概率矩阵\n",
    "- observation_probs: 观察概率矩阵\n",
    "\n",
    "Output：\n",
    "- state_seq: 最可能的状态序列，即预测的词性标记序列\n",
    "\n",
    "注意，Viterbi算法返回的结果是最可能的状态序列，即预测的词性标记序列。需要进行转换，将整数编码转换为原始的词性标记。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b09d4a93fd700e6"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def viterbi(observed_seq, num_tags, start_probs, transition_probs, observation_probs):\n",
    "    eps = 1e-7\n",
    "\n",
    "    num_obs = len(observed_seq)\n",
    "\n",
    "    # Initialise the V and backpointers\n",
    "    V = np.zeros((num_obs, num_tags))\n",
    "    backpointer = np.zeros((num_obs, num_tags))\n",
    "\n",
    "    # For the first data point in the sequence:\n",
    "    V[0, :] = start_probs * observation_probs[:, observed_seq[0]]\n",
    "\n",
    "    # Run Viterbi forward for t > 0\n",
    "    for t in range(1, num_obs):\n",
    "\n",
    "        for state in range(num_tags):\n",
    "            # probabilities for all the sequences leading to this state at time t\n",
    "            seq_prob = V[t-1, :] * transition_probs[:, state]\n",
    "\n",
    "            # Choose the most likely sequence\n",
    "            max_seq_prob = np.max(seq_prob)\n",
    "            best_previous_state = np.argmax(seq_prob)\n",
    "\n",
    "            # Calculate the probability of the most likely sequence leading to this state at time t, including the current observation.\n",
    "            # Add eps to help with numerical issues.\n",
    "            V[t, state] = (max_seq_prob + eps) * (observation_probs[state, observed_seq[t]] + eps)\n",
    "\n",
    "            backpointer[t, state] = best_previous_state\n",
    "\n",
    "    t = num_obs - 1\n",
    "\n",
    "    # Initialise the sequence of predicted states\n",
    "    state_seq = np.zeros(num_obs, dtype=int)\n",
    "\n",
    "    # Get the most likely final state:\n",
    "    state_seq[t] = np.argmax(V[t, :])\n",
    "\n",
    "    # Backtrack until the first observation\n",
    "    for t in range(len(observed_seq)-1, 0, -1):\n",
    "        state_seq[t-1] = backpointer[t, state_seq[t]]\n",
    "\n",
    "    return state_seq"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T13:24:24.437256Z",
     "start_time": "2024-03-17T13:24:24.423756Z"
    }
   },
   "id": "9ca2f383488c47ac",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:      ['``', 'My', 'God', ',', \"I'm\", 'shot', \"''\", '!', '!']\n",
      "Gold tag:    ['.', 'DET', 'NOUN', '.', 'PRT', 'VERB', '.', '.', '.']\n",
      "Predictions: ['.' 'DET' 'NOUN' '.' 'PRT' 'NOUN' '.' '.' '.'] \n",
      "\n",
      "Tokens:      ['She', 'thought', 'she', 'was', 'bigger', 'than', 'we', 'are', 'because', 'she', 'came', 'from', 'Torino', \"''\", '.']\n",
      "Gold tag:    ['PRON', 'VERB', 'PRON', 'VERB', 'ADJ', 'ADP', 'PRON', 'VERB', 'ADP', 'PRON', 'VERB', 'ADP', 'NOUN', '.', '.']\n",
      "Predictions: ['PRON' 'VERB' 'PRON' 'VERB' 'ADJ' 'ADP' 'PRON' 'VERB' 'ADP' 'PRON' 'VERB'\n",
      " 'ADP' 'NOUN' '.' '.'] \n",
      "\n",
      "Accuracy: 0.9437\n"
     ]
    }
   ],
   "source": [
    "# 使用Viterbi算法对测试集中的每个句子进行词性标记预测\n",
    "predictions = []\n",
    "for sentence in test_toks_encoded:\n",
    "    predictions.append(viterbi(sentence, num_tags, start_state_probabilities, transition_probabilities, observation_probabilities))\n",
    "    \n",
    "# 将整数编码转换为原始的词性标记\n",
    "predicted_tags = [label_encoder.inverse_transform(pred) for pred in predictions]\n",
    "\n",
    "# 打印一些结果以验证\n",
    "examples = [2, 334]\n",
    "for eg in examples:\n",
    "    print(f'Tokens:      {test_toks[eg]}')\n",
    "    print(f'Gold tag:    {test_tags[eg]}')\n",
    "    print(f'Predictions: {predicted_tags[eg]}','\\n')\n",
    "    \n",
    "# 计算准确率\n",
    "correct = 0\n",
    "total = 0\n",
    "for i in range(len(test_tags)):\n",
    "    correct += np.sum(np.array(test_tags[i]) == np.array(predicted_tags[i]))\n",
    "    total += len(test_tags[i])\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# 或使用sklearn库中的accuracy_score函数计算准确率\n",
    "# all_predictions = [tag for sentence in predictions for tag in sentence]\n",
    "# all_targets = [tag for sentence in test_tags_encoded for tag in sentence]\n",
    "# acc = accuracy_score(all_targets, all_predictions)\n",
    "# print(f'Accuracy = {acc}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T13:24:40.308948Z",
     "start_time": "2024-03-17T13:24:27.201874Z"
    }
   },
   "id": "1f163718c4e1c5b0",
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.5 NLTK---HiddenMarkovModelTrainer\n",
    "NLTK库中的HiddenMarkovModelTrainer类可以用于训练HMM模型。HiddenMarkovModelTrainer类的train_supervised方法可以用于训练HMM模型，train_supervised方法的输入是训练数据，输出是训练好的HMM模型。\n",
    "\n",
    "只需要将格式正确的数据划分为训练集和测试集，然后使用train_supervised方法即可训练HMM模型。训练好的模型可以用于预测新的数据，并且可以使用accuracy方法计算模型的准确率。\n",
    "\n",
    "对于提升模型准确率的问题，由于HiddenMarkovModelTrainer类在train_supervised方法中没有直接提供调整模型复杂度或学习行为的参数，提升模型性能的途径主要依赖于训练数据的质量、量和预处理步骤。以下是一些可能帮助提高准确率的策略："
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "983c95b18804acaa"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('However', 'ADV'), (',', '.'), ('the', 'DET'), ('wei', 'X'), ('books', 'NOUN'), ('were', 'VERB'), ('also', 'ADV'), ('destroyed', 'VERB'), ('in', 'ADP'), ('a', 'DET'), ('series', 'NOUN'), ('of', 'ADP'), ('Orthodox', 'ADJ'), ('Confucian', 'ADJ'), ('purges', 'NOUN'), ('which', 'DET'), ('culminated', 'VERB'), ('in', 'ADP'), ('a', 'DET'), ('final', 'ADJ'), ('proscription', 'NOUN'), ('in', 'ADP'), ('605', 'NUM'), ('.', '.')] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Programming\\anaconda3\\envs\\mytorch\\lib\\site-packages\\nltk\\tag\\hmm.py:336: RuntimeWarning: overflow encountered in cast\n",
      "  O[i, k] = self._output_logprob(si, self._symbols[k])\n",
      "E:\\Programming\\anaconda3\\envs\\mytorch\\lib\\site-packages\\nltk\\tag\\hmm.py:364: RuntimeWarning: overflow encountered in cast\n",
      "  O[i, k] = self._output_logprob(si, self._symbols[k])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('However', 'ADV'), (',', '.'), ('the', 'DET'), ('wei', 'DET'), ('books', 'DET'), ('were', 'DET'), ('also', 'DET'), ('destroyed', 'DET'), ('in', 'DET'), ('a', 'DET'), ('series', 'DET'), ('of', 'DET'), ('Orthodox', 'DET'), ('Confucian', 'DET'), ('purges', 'DET'), ('which', 'DET'), ('culminated', 'DET'), ('in', 'DET'), ('a', 'DET'), ('final', 'DET'), ('proscription', 'DET'), ('in', 'DET'), ('605', 'DET'), ('.', 'DET')]\n",
      "HMM模型的准确率: 0.7492\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag.hmm import HiddenMarkovModelTrainer\n",
    "\n",
    "# 训练数据和测试数据应该是这样的格式：[[(word1, tag1), (word2, tag2), ...], ...]\n",
    "# 其中每个内层列表代表一个句子，句子由单词和对应标签的元组组成\n",
    "\n",
    "# 创建一个HMM模型训练器\n",
    "trainer = HiddenMarkovModelTrainer()\n",
    "\n",
    "# 训练模型, 用train_supervised方法\n",
    "hmm_model = trainer.train_supervised(train_set)\n",
    "\n",
    "# 输出预测结果\n",
    "print(test_set[0],'\\n')\n",
    "print(hmm_model.tag(test_toks[0]))\n",
    "\n",
    "# 使用测试集评估模型\n",
    "accuracy = hmm_model.accuracy(test_set)  # 注意，这里的test_data也应该是同样的格式\n",
    "\n",
    "print(f\"HMM模型的准确率: {accuracy:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T13:25:16.514027Z",
     "start_time": "2024-03-17T13:24:43.501133Z"
    }
   },
   "id": "d692921655c41e06",
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Conditional Random Fields (CRFs) for Named Entity Recognition (NER)\n",
    "这里我们使用CRF模型来完成命名实体识别（NER）任务。\n",
    "首先加载`conll2003`数据集，这是一个常用的NER数据集，包含了英文句子和对应的命名实体标记。然后我们将数据集划分为训练集和测试集，用于训练和评估CRF模型。\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2df574ce417cf01c"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset with 14041 instances loaded\n",
      "Test dataset with 3453 instances loaded\n"
     ]
    }
   ],
   "source": [
    "cache_dir = \"./data_cache\"\n",
    "\n",
    "# The data is already divided into training and test sets.\n",
    "# Load the training set:\n",
    "train_dataset = load_dataset(\n",
    "    \"conll2003\",\n",
    "    split=\"train\",\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "print(f\"Training dataset with {len(train_dataset)} instances loaded\")\n",
    "\n",
    "# Load the test set:\n",
    "test_dataset = load_dataset(\n",
    "    \"conll2003\",\n",
    "    split=\"test\",\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "print(f\"Test dataset with {len(test_dataset)} instances loaded\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T14:19:05.411219Z",
     "start_time": "2024-03-17T14:18:54.667473Z"
    }
   },
   "id": "75a4a396b112775b",
   "execution_count": 55
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1 Data Preprocessing\n",
    "首先查看数据集的格式。可以看到每个元素都是一个字典，对应了文本的一句话，以及这句话中每个单词的词性标记（pos）,分块标记（chunk）和命名实体标记（ner）。\n",
    "这个数据集输出看起来是某个自然语言处理（NLP）任务的一部分，其中包含了句子的分词（tokens）、词性标注（pos_tags）、分块标签（chunk_tags）以及命名实体识别标签（ner_tags）。每个标签对应一种特定的标注，通常这些标注是预先定义好的。下面是对这些标签的基本解释：\n",
    "\n",
    "1. 词性标注（POS Tagging）\n",
    "`pos_tags`列表中的每个数字代表一个词性（Part-of-Speech, POS）的标识符，对应于句子中每个词的词性。词性包括名词、动词、形容词等。不同的数字代表不同的词性。具体到您提供的数据，这些数字（如22, 42, 16等）需要对照特定的词性标注方案来解释，比如Universal POS tags或者Penn Treebank POS Tags。\n",
    "\n",
    "2. 分块标签（Chunking）\n",
    "`chunk_tags`列表表示句子中每个词属于的短语或“块”的类型。分块通常用于标识名词短语（NP）、动词短语（VP）等。这里的数字同样代表不同的短语类型。例如：\n",
    "分块标签通常用于“浅层句法分析”，帮助识别句子中的基本结构，但不进入深层次的句法细节。\n",
    "\n",
    "3. 命名实体识别标签（NER）\n",
    "`ner_tags`列表包含的数字代表句子中每个词是否属于某个命名实体，以及它们的实体类型。命名实体识别（NER）是指识别文本中具有特定意义的实体，如人名、地点、组织等。例如：\n",
    "\n",
    "本部分主要关注NER，我们只需要提取NER，并定义映射关系，将NER标签映射为整数编码。将使用下面的映射：\n",
    "\n",
    "```\n",
    "{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f3720de4f6c2413"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{'id': '0',\n 'tokens': ['EU',\n  'rejects',\n  'German',\n  'call',\n  'to',\n  'boycott',\n  'British',\n  'lamb',\n  '.'],\n 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T14:19:11.094265Z",
     "start_time": "2024-03-17T14:19:11.084842Z"
    }
   },
   "id": "9159bbab867e1ee5",
   "execution_count": 56
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "ner_tag_mapping = {0: 'O', 1:'B-PER', 2:'I-PER', 3:'B-ORG', 4:'I-ORG', 5:'B-LOC', 6:'I-LOC', 7:'B-MISC', 8:'I-MISC'}\n",
    "\n",
    "# 先遍历train_dataset和test_dataset，提取出每个句子：s\n",
    "# 然后对每个句子s，提取出s['ner_tags']\n",
    "# 最后将s['ner_tags']中的每个标签，用ner_tag_mapping将整数编码映射为对应的命名实体标签\n",
    "train_set = [list(zip(s['tokens'], [ner_tag_mapping[tok] for tok in s['ner_tags']])) for s in train_dataset][:-1]\n",
    "test_set = [list(zip(s['tokens'], [ner_tag_mapping[tok] for tok in s['ner_tags']])) for s in test_dataset][:-1]\n",
    "\n",
    "# test_tokens的每个元素是一个句子的单词，test_tags的每个元素是一个句子的命名实体标签\n",
    "test_tokens = [s['tokens'] for s in test_dataset][:-1]\n",
    "test_tags = [[ner_tag_mapping[tok] for tok in s['ner_tags']] for s in test_dataset][:-1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T14:19:26.071114Z",
     "start_time": "2024-03-17T14:19:24.335363Z"
    }
   },
   "id": "2382938994faa250",
   "execution_count": 57
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[('EU', 'B-ORG'),\n ('rejects', 'O'),\n ('German', 'B-MISC'),\n ('call', 'O'),\n ('to', 'O'),\n ('boycott', 'O'),\n ('British', 'B-MISC'),\n ('lamb', 'O'),\n ('.', 'O')]"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set[0]\n",
    "# 此时我们训练集的每个元素只包含了每个句子的单词和对应的命名实体，格式为[(word1, ner1), (word2, ner2), ...]。"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T14:19:27.259040Z",
     "start_time": "2024-03-17T14:19:27.250039Z"
    }
   },
   "id": "eae631349e38cb5f",
   "execution_count": 58
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2 Create CRF Model\n",
    "使用nltk库中的`CRFTagger`类，可以创建和训练CRF模型。CRFTagger类的train方法可以用于训练CRF模型，train方法的输入是训练数据，输出是训练好的CRF模型。\n",
    "\n",
    "假设你已经有了格式化的训练数据 train_set\n",
    "train_set 应该是一个列表，其中包含句子，每个句子又是一系列（单词，标签）对\n",
    "例如: `train_set = [[(\"The\", \"DET\"), (\"cat\", \"NOUN\"), ...], [...]]`\n",
    "\n",
    "#### 建立CRF模型\n",
    "可以直接用下面的方式建立模型，但是进行封装函数的好处是可以在函数内部进行一些预处理，比如数据清洗，数据预处理等 并可以重复使用\n",
    "且若没有在train方法中指定保存路径，模型会保存在内存中，当程序结束时会丢失\n",
    "```python\n",
    "tagger=CRFTagger()\n",
    "tagger.train(train_set)\n",
    "```\n",
    "\n",
    "#### 使用训练好的模型进行预测\n",
    "```python\n",
    "tagger.tag_sents([['This', 'is','about','the','United','States','of','America']])\n",
    "# 输出结果是：[[('This', 'O'), ('is', 'O'), ('about', 'O'), ('the', 'O'), ('United', 'B-LOC'), ('States', 'I-LOC'), ('of', 'O'), ('America', 'B-LOC')]]\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ccd174eb51fcf432"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tag import CRFTagger\n",
    "\n",
    "def train_CRF_NER_tagger(train_set):\n",
    "    # 创建CRF标记器的实例\n",
    "    tagger = CRFTagger()\n",
    "\n",
    "    # 使用训练集训练标记器, 并保存模型\n",
    "    tagger.train(train_set, './nlp_models/model.crf.tagger')\n",
    "    \n",
    "    return tagger  # 返回训练好的模型\n",
    "\n",
    "tagger = train_CRF_NER_tagger(train_set)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T22:54:57.361668Z",
     "start_time": "2024-03-11T22:54:43.489402Z"
    }
   },
   "id": "847fe6b6991e7eeb",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('SOCCER', 'O'), ('-', 'O'), ('JAPAN', 'B-LOC'), ('GET', 'O'), ('LUCKY', 'O'), ('WIN', 'O'), (',', 'O'), ('CHINA', 'B-ORG'), ('IN', 'O'), ('SURPRISE', 'O'), ('DEFEAT', 'O'), ('.', 'O')]\n",
      "[('SOCCER', 'O'), ('-', 'O'), ('JAPAN', 'B-LOC'), ('GET', 'O'), ('LUCKY', 'O'), ('WIN', 'O'), (',', 'O'), ('CHINA', 'B-PER'), ('IN', 'O'), ('SURPRISE', 'O'), ('DEFEAT', 'O'), ('.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "predicted_tags = tagger.tag_sents(test_tokens)\n",
    "print(predicted_tags[0])\n",
    "print(test_set[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T22:58:17.072573Z",
     "start_time": "2024-03-11T22:58:16.910043Z"
    }
   },
   "id": "819e2c092e0e59cb",
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3 Evaluate the Model\n",
    "实体匹配 vs. 标记匹配\n",
    "\n",
    "- 正确匹配的实体：这指的是模型正确识别并分类的整个实体。在NER中，一个实体可能由一个词或多个连续的词组成。因此，只有当实体的所有组成部分都被准确识别并且分类正确时，才认为是一个正确的实体匹配。例如，如果实体“New York”被完整地识别为一个地点名（LOC），这就是一个正确的实体匹配。\n",
    "\n",
    "- 正确标记的标记（Token）：这指的是单个词（或标记）被正确标注的情况。在一般的序列标注任务中，如词性标注，通常评估的是每个词的标注是否准确。但在NER中，即使单个词被正确标注，如果它是一个实体的一部分，而这个实体没有被完整且正确地识别出来，那么这并不足以说明模型的性能好。\n",
    "\n",
    "由于nltk库等并没有提供直接计算NER准确率的函数，我们需要自己编写一个函数来进行评估。注意，我们需要自定义一个提取实体的函数，然后使用这个函数来提取实体，最后计算模型的准确率。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b71eb6301b8782cd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "### 提取实体：`extract_spans(tagged_sents)`\n",
    "\n",
    "这个函数的目的是从标记过的句子集合中提取出所有命名实体的跨度（span）。每个实体跨度由三个部分组成：实体开始的标记（token）索引、实体结束的标记索引，以及实体所在句子的索引。这个函数返回一个字典，其中键是实体类型（例如`\"LOC\"`、`\"PER\"`等），值是该类型实体的跨度列表。\n",
    "\n",
    "函数逻辑如下：\n",
    "\n",
    "- 遍历每个句子及其标记，使用`B-`和`I-`前缀来识别实体的开始和内部标记。\n",
    "- 当遇到`B-`标记时，标记实体的开始，并记录实体类型。\n",
    "- 遇到`I-`标记时，更新实体的结束位置。\n",
    "- 遇到`O`标记或句子结束时，如果之前有开始标记，则将当前记录的实体添加到字典中，并重置开始标记。\n",
    "- 函数返回包含所有实体跨度的字典，每种实体类型一个键。\n",
    "\n",
    "打印出的span结果是一个字典，其中键为实体类型（'PER'、'LOC'、'ORG'等），值为包含实体跨度信息的列表。每个跨度是一个三元组(start, end, sidx)，其中：\n",
    "\n",
    "start：实体开始的标记（token）索引。\n",
    "end：实体结束的标记索引+1（即Python中的范围结束是独占的）。\n",
    "sidx：实体所在句子的索引，从0开始。\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8b024d55d78ea89"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def extract_spans(tagged_sents):\n",
    "    \"\"\"\n",
    "    Extract a list of tagged spans for each named entity type, \n",
    "    where each span is represented by a tuple containing the \n",
    "    start token and end token indexes.\n",
    "    \n",
    "    returns: a dictionary containing a list of spans for each entity type.\n",
    "    \"\"\"\n",
    "    spans = {}\n",
    "        \n",
    "    for sidx, sent in enumerate(tagged_sents):\n",
    "        start = -1\n",
    "        entity_type = None\n",
    "        for i, (tok, lab) in enumerate(sent):\n",
    "            if 'B-' in lab:\n",
    "                start = i\n",
    "                end = i + 1\n",
    "                entity_type = lab[2:]\n",
    "            elif 'I-' in lab:\n",
    "                end = i + 1\n",
    "            elif lab == 'O' and start >= 0:\n",
    "                \n",
    "                if entity_type not in spans:\n",
    "                    spans[entity_type] = []\n",
    "                \n",
    "                spans[entity_type].append((start, end, sidx))\n",
    "                start = -1      \n",
    "        # Sometimes an I-token is the last token in the sentence, so we still have to add the span to the list\n",
    "        if start >= 0:    \n",
    "            if entity_type not in spans:\n",
    "                spans[entity_type] = []\n",
    "                \n",
    "            spans[entity_type].append((start, end, sidx))\n",
    "                \n",
    "    return spans"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "56fcf023857e7c20"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PER': [(0, 1, 0)], 'LOC': [(3, 5, 0)], 'ORG': [(3, 4, 1)]}\n"
     ]
    }
   ],
   "source": [
    "tagged_sents = [\n",
    "    # 第一个句子\n",
    "    [(\"John\", \"B-PER\"), (\"lives\", \"O\"), (\"in\", \"O\"), (\"New\", \"B-LOC\"), (\"York\", \"I-LOC\"), (\".\", \"O\")],\n",
    "    # 第二个句子\n",
    "    [(\"She\", \"O\"), (\"works\", \"O\"), (\"at\", \"O\"), (\"Google\", \"B-ORG\"), (\".\", \"O\")]\n",
    "]\n",
    "# 假设extract_spans函数已经定义\n",
    "\n",
    "spans = extract_spans(tagged_sents)\n",
    "print(spans)\n",
    "\n",
    "# 对于'PER'（人名）实体类型，有一个实体跨度从第0句子（索引为0）的索引0开始到索引1结束。\n",
    "# 对于'LOC'（地点名）实体类型，有一个实体跨度从第0句子的索引3开始到索引5结束。\n",
    "# 对于'ORG'（组织名）实体类型，有一个实体跨度从第1句子（索引为1）的索引3开始到索引4结束。"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T23:29:18.505205Z",
     "start_time": "2024-03-11T23:29:18.489204Z"
    }
   },
   "id": "8104929378803355",
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 评估：`cal_span_level_f1(test_sents, test_sents_with_pred)`\n",
    "\n",
    "这个函数计算和打印出在测试数据上的实体层面F1分数。它首先使用`extract_spans`函数从真实标签和预测标签中提取实体跨度，然后对每个实体类型计算精确度、召回率和F1分数。\n",
    "\n",
    "计算逻辑如下：\n",
    "\n",
    "- 对于每种实体类型，通过比较预测的实体跨度和真实的实体跨度来计算真正例（TP）、假正例（FP）和假负例（FN）。\n",
    "- 使用TP、FP和FN的值，计算每个实体类型的精确度、召回率和F1分数。\n",
    "- 打印每个实体类型的F1分数，并计算所有实体类型F1分数的宏平均（Macro-average）作为总体性能指标。\n",
    "\n",
    "这两个函数结合使用，提供了NER任务中一个详细的性能评估方法，特别是强调了实体层面的评估，而不仅仅是标记层面的正确性。这种评估方法更加符合NER任务的实际应用需求，因为在NER中，完整准确地识别出整个实体比仅正确标记实体中的单个词更为重要。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34ad7144168792d8"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for class LOC = 0.7970501474926254\n",
      "F1 score for class PER = 0.7671940587665484\n",
      "F1 score for class MISC = 0.6956521739130435\n",
      "F1 score for class ORG = 0.6521877994251037\n",
      "Macro-average f1 score = 0.7280210448993303\n"
     ]
    }
   ],
   "source": [
    "def cal_span_level_f1(test_sents, test_sents_with_pred):\n",
    "    # get a list of spans from the test set labels\n",
    "    gold_spans = extract_spans(test_sents)\n",
    "\n",
    "    # get a list of spans predicted by our tagger\n",
    "    pred_spans = extract_spans(test_sents_with_pred)\n",
    "    \n",
    "    # compute the metrics for each class:\n",
    "    f1_per_class = []\n",
    "    \n",
    "    ne_types = gold_spans.keys()  # get the list of named entity types (not the tags)\n",
    "    \n",
    "    for ne_type in ne_types:\n",
    "        # compute the confusion matrix\n",
    "        true_pos = 0\n",
    "        false_pos = 0\n",
    "        \n",
    "        for span in pred_spans[ne_type]:\n",
    "            if span in gold_spans[ne_type]:\n",
    "                true_pos += 1\n",
    "            else:\n",
    "                false_pos += 1\n",
    "                \n",
    "        false_neg = 0\n",
    "        for span in gold_spans[ne_type]:\n",
    "            if span not in pred_spans[ne_type]:\n",
    "                false_neg += 1\n",
    "                \n",
    "        if true_pos + false_pos == 0:\n",
    "            precision = 0\n",
    "        else:\n",
    "            precision = true_pos / float(true_pos + false_pos)\n",
    "            \n",
    "        if true_pos + false_neg == 0:\n",
    "            recall = 0\n",
    "        else:\n",
    "            recall = true_pos / float(true_pos + false_neg)\n",
    "        \n",
    "        if precision + recall == 0:\n",
    "            f1 = 0\n",
    "        else:\n",
    "            f1 = 2 * precision * recall / (precision + recall)\n",
    "            \n",
    "        f1_per_class.append(f1)\n",
    "        print(f'F1 score for class {ne_type} = {f1}')\n",
    "        \n",
    "    print(f'Macro-average f1 score = {np.mean(f1_per_class)}')\n",
    "\n",
    "cal_span_level_f1(test_set, predicted_tags)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T23:25:31.974263Z",
     "start_time": "2024-03-11T23:25:31.826950Z"
    }
   },
   "id": "d57d4efb32a4be50",
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 CRF模型的优化\n",
    "可以通过自定义封装的CRF模型训练函数，对训练数据进行预处理，以及调整CRF模型的参数，来提升模型的性能。\n",
    "\n",
    "首先定义`CustomCRFTagger`类并且在其中重写了_get_features方法后，通过创建一个CustomCRFTagger实例并使用这个实例来训练模型，训练过程会自动使用你定义的_get_features方法来提取特征。\n",
    "\n",
    "然后定义一个CRF模型，并使用`CustomCRFTagger`类的train方法来训练模型。训练好的模型可以用于预测新的数据，并且可以使用accuracy方法计算模型的准确率。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4f12f758097c0f1d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import re, unicodedata\n",
    "import nltk\n",
    "\n",
    "class CustomCRFTagger(nltk.tag.CRFTagger):\n",
    "    _current_tokens = None\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CustomCRFTagger, self).__init__(*args, **kwargs)\n",
    "        self._pattern = re.compile(r'\\d')  # 正则表达式用于检测数字\n",
    "    \n",
    "    def _get_features(self, tokens, idx):\n",
    "        \"\"\"\n",
    "        Extract features for the word at position idx in the sentence.\n",
    "        \"\"\"\n",
    "        token = tokens[idx]\n",
    "\n",
    "        feature_list = []\n",
    "\n",
    "        if not token:\n",
    "            return feature_list\n",
    "\n",
    "        # Capitalization\n",
    "        if token[0].isupper(): # 当前单词的首字母是否大写\n",
    "            feature_list.append(\"CAPITALIZATION\")\n",
    "\n",
    "        # Number\n",
    "        if re.search(self._pattern, token) is not None: # 当前单词是否包含数字\n",
    "            feature_list.append(\"HAS_NUM\")\n",
    "\n",
    "        # Punctuation\n",
    "        # 这段代码检查给定的单词（token）是否完全由标点符号组成。它使用unicodedata.category来获取每个字符的Unicode类别，并检查这些类别是否全部属于标点符号的类别（如连接符（Pc）、破折号（Pd）、开括号（Ps）、闭括号（Pe）、初始引号（Pi）、终结引号（Pf）和其他标点符号（Po））。\n",
    "        # 如果一个单词完全由标点符号组成，该函数会将\"PUNCTUATION\"特征添加到特征列表（feature_list）中。这对于自然语言处理（NLP）任务中识别标点符号的重要性很有帮助，因为标点符号可能对句子的结构和语义有显著影响。\n",
    "        punc_cat = {\"Pc\", \"Pd\", \"Ps\", \"Pe\", \"Pi\", \"Pf\", \"Po\"}\n",
    "        if all(unicodedata.category(x) in punc_cat for x in token):\n",
    "            feature_list.append(\"PUNCTUATION\")\n",
    "\n",
    "        # Suffix up to length 3\n",
    "        # 这段代码将单词的后缀添加到特征列表中。它将单词的后缀添加为特征，以便CRF标记器可以使用这些特征来识别单词。\n",
    "        if len(token) > 1:\n",
    "            feature_list.append(\"SUF_\" + token[-1:])\n",
    "        if len(token) > 2:\n",
    "            feature_list.append(\"SUF_\" + token[-2:])\n",
    "        if len(token) > 3:\n",
    "            feature_list.append(\"SUF_\" + token[-3:])\n",
    "\n",
    "        # Current word\n",
    "        feature_list.append(\"WORD_\" + token)\n",
    "\n",
    "        # Previous word\n",
    "        if idx > 0:\n",
    "            prev_token = tokens[idx - 1]\n",
    "            feature_list.append(\"PREV_WORD_\" + prev_token)\n",
    "        else:\n",
    "            feature_list.append(\"BOS\")  # Beginning of sentence\n",
    "\n",
    "        # Next word\n",
    "        if idx < len(tokens) - 1:\n",
    "            next_token = tokens[idx + 1]\n",
    "            feature_list.append(\"NEXT_WORD_\" + next_token)\n",
    "        else:\n",
    "            feature_list.append(\"EOS\")  # End of sentence\n",
    "\n",
    "        return feature_list"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T23:32:09.582136Z",
     "start_time": "2024-03-11T23:32:09.565135Z"
    }
   },
   "id": "30042cbc1afc764e",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Train a CRF NER tagger\n",
    "def train_CustomCRF_NER_tagger(train_set):\n",
    "    tagger = CustomCRFTagger()\n",
    "    tagger.train(train_set, './nlp_models/model_more_features.crf.tagger')\n",
    "    return tagger  # return the trained model\n",
    "\n",
    "tagger = train_CustomCRF_NER_tagger(train_set)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T23:38:00.784149Z",
     "start_time": "2024-03-11T23:37:44.963442Z"
    }
   },
   "id": "620c49b361e8d3a6",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for class LOC = 0.8218497827436375\n",
      "F1 score for class PER = 0.8260602335586971\n",
      "F1 score for class MISC = 0.7557603686635945\n",
      "F1 score for class ORG = 0.7094880991196609\n",
      "Macro-average f1 score = 0.7782896210213975\n"
     ]
    }
   ],
   "source": [
    "predicted_tags = tagger.tag_sents(test_tokens)\n",
    "cal_span_level_f1(test_set, predicted_tags)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T23:38:03.443015Z",
     "start_time": "2024-03-11T23:38:03.132766Z"
    }
   },
   "id": "1607c6bd64bcd000",
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 添加POS特征\n",
    "可以继承上述定义的CustomCRFTagger类，并在此基础上添加PoS标签作为特征。\n",
    "这也是一般的特征工程方法，即在原有的特征基础上添加新的特征，以提高模型的性能。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f9f55f80926be7a7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 继承之前定义的CustomCRFTagger类，继承原来的特征提取方法，并在此基础上添加PoS标签作为特征\n",
    "class CRFTaggerWithPOS(CustomCRFTagger):\n",
    "    _current_tokens = None\n",
    "    \n",
    "    def _get_features(self, tokens, index):\n",
    "        # Get the basic features from the parent class\n",
    "        basic_features = super()._get_features(tokens, index)\n",
    "        \n",
    "        # Add the PoS tag as an additional feature\n",
    "        pos_tag = nltk.pos_tag([tokens[index]])[0][1]\n",
    "        \n",
    "        # Append the PoS tag as an additional feature\n",
    "        basic_features.append(\"POS_\" + pos_tag)\n",
    "\n",
    "        return basic_features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-11T23:40:59.309852Z",
     "start_time": "2024-03-11T23:40:59.295854Z"
    }
   },
   "id": "19074faa49014079",
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Dependency Parsing\n",
    "NLTK库没有用于依存句法分析的内置模型，这里使用spacy库来完成依存句法分析任务。SpaCy的设计就是为了简化自然语言处理流程，让开发者能够通过一条简单的命令行调用完成复杂的文本分析任务。这种设计极大地提高了开发效率，让开发者可以集中精力在解决实际的问题上，而不是处理底层的NLP细节。\n",
    "\n",
    "SpaCy提供nlp()函数，可以用于对文本进行分析。nlp()函数的输入是文本，输出是一个Doc对象，其中包含了对文本的分析结果。Doc对象是一个包含了分析结果的容器，其中包含了分词、词性标注、命名实体识别、依存句法分析等信息。\n",
    "- `doc=nlp(text)`: 对文本进行分析，返回一个Doc对象。\n",
    "- `doc.sents`: 获取文本中的句子。\n",
    "- `for token in doc`: 遍历文本中的每个单词，每个单词是一个Token对象.\n",
    "    - `token.text`: 获取单词的原始文本。\n",
    "    - `token.pos_`: 获取单词的词性标注。\n",
    "    - `token.dep_`: 获取单词的依存关系。\n",
    "    - `token.head.text`: 获取单词的依存头部（父节点）。\n",
    "- `for ent in doc.ents`: 获取文本中的命名实体。\n",
    "    - `ent.text`: 获取命名实体的文本。\n",
    "    - `ent.label_`: 获取命名实体的ner标签。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "efcce63a958ed1ff"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[38;5;2m✔ Download and installation successful\u001B[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import spacy.cli\n",
    "# 下载spacy的英文模型。如果需要分析其他语言的文本，可以下载其他语言的模型\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "# 加载英文模型\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T13:45:59.813685Z",
     "start_time": "2024-03-17T13:45:55.245533Z"
    }
   },
   "id": "b2e9881c4d1d5ba4",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I POS: PRON Dep: nsubj Head: wondering\n",
      "was POS: AUX Dep: aux Head: wondering\n",
      "wondering POS: VERB Dep: ROOT Head: wondering\n",
      "if POS: SCONJ Dep: mark Head: enlighten\n",
      "anyone POS: PRON Dep: nsubj Head: enlighten\n",
      "out POS: ADV Dep: advmod Head: there\n",
      "there POS: ADV Dep: advmod Head: anyone\n",
      "could POS: AUX Dep: aux Head: enlighten\n",
      "enlighten POS: VERB Dep: advcl Head: saw\n",
      "me POS: PRON Dep: dobj Head: enlighten\n",
      "on POS: ADP Dep: prep Head: enlighten\n",
      "this POS: DET Dep: det Head: car\n",
      "car POS: NOUN Dep: pobj Head: on\n",
      "I POS: PRON Dep: nsubj Head: saw\n",
      "saw POS: VERB Dep: ccomp Head: wondering\n",
      "the POS: DET Dep: det Head: day\n",
      "other POS: ADJ Dep: amod Head: day\n",
      "day POS: NOUN Dep: dobj Head: saw\n",
      ". POS: PUNCT Dep: punct Head: wondering\n",
      "God POS: PROPN Dep: nsubj Head: bless\n",
      "bless POS: VERB Dep: ROOT Head: bless\n",
      "America POS: PROPN Dep: dobj Head: bless\n",
      "AND POS: CCONJ Dep: cc Head: America\n",
      "u.s.a POS: NOUN Dep: conj Head: America\n",
      ". POS: PUNCT Dep: punct Head: bless\n"
     ]
    }
   ],
   "source": [
    "# nlp()函数\n",
    "text='I was wondering if anyone out there could enlighten me on this car I saw the other day. God bless America AND u.s.a.'\n",
    "doc=nlp(text)\n",
    "\n",
    "# Tagging\n",
    "for token in doc:\n",
    "    print(token.text, 'POS:', token.pos_, 'Dep:', token.dep_, 'Head:', token.head.text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T14:04:08.635477Z",
     "start_time": "2024-03-17T14:04:08.627Z"
    }
   },
   "id": "39111ca8979a1148",
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the other day DATE\n",
      "God PERSON\n",
      "America GPE\n"
     ]
    }
   ],
   "source": [
    "# NER\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T14:04:10.848618Z",
     "start_time": "2024-03-17T14:04:10.839609Z"
    }
   },
   "id": "cd96c7920a6165b0",
   "execution_count": 54
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "text_analytics",
   "language": "python",
   "display_name": "text_analytics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
