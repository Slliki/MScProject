{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Deep Learning for NLP \n",
    "\n",
    "INCLUDE:\n",
    "- Text Preprocessing\n",
    "  - Tokenization: 分词\n",
    "  - Encoding Text: 将文本转换为整数序列\n",
    "  - Padding & Truncating: 填充和截断\n",
    "  - Converting to DataLoader: 将数据集转换为DataLoader对象\n",
    "  - Word Embeddings: 词嵌入\n",
    "    - Pre-trained Word Embeddings\n",
    "- Neural Network Models\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d30f5c340e313310"
  },
  {
   "cell_type": "markdown",
   "source": [
    "与逻辑回归一样，神经网络也是辨别模型：它们直接学习分类任务，而无需学习生成文本。但与逻辑回归不同的是，它们可以对输入的非线性函数进行建模。这是通过具有非线性激活函数的隐藏层来实现的。\n",
    "\n",
    "神经网络的一些主要优势可归纳如下：\n",
    "   * 它可以模拟非线性函数，因此可以处理特征和类标签之间更为复杂的关系。\n",
    "   * 它可以进行表征学习：隐藏层可以学习如何从低层数据中提取特征。\n",
    "   * 它可以处理标记序列--我们不必像逻辑回归那样，用单一的特征向量来表示文档。\n",
    "\n",
    "另一方面，它的缺点是\n",
    "   * 训练和测试的成本要高得多。\n",
    "   * 对小数据集可能会产生严重的过拟合。\n",
    "   * 隐藏层学习到的特征可能难以解释，这就很难预测模型的行为，例如它可能在哪种情况下失败。\n",
    "   * 泛化是不可预测的：当应用于训练集分布之外的测试数据时，它们有时会有意想不到的表现。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "87c0dfdda52972dd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 0.1 一般NLP任务数据预处理的最佳实践：\n",
    "\n",
    "1. **建立字典（Vocabulary）**：从文档集合中识别所有唯一的词汇，并为每个词汇分配一个唯一的整数索引。这个字典使得模型能够处理文本数据。\n",
    "\n",
    "2. **文本到整数序列的映射**：使用字典将每个文档中的单词转换为对应的整数索引。这一步将文本数据转换为神经网络能够处理的数值形式。\n",
    "\n",
    "3. **截断和填充**：由于神经网络需要输入固定长度的向量，因此需要将所有文档长度统一。这通常通过截断较长的文档和/或为较短的文档添加填充值（如0）来实现。\n",
    "\n",
    "4. **转换为Tensor**：将处理过的整数序列转换为PyTorch的Tensor格式，这是PyTorch模型的输入格式。\n",
    "\n",
    "5. **DataLoader使用**：为了有效地训练模型，使用PyTorch的`DataLoader`类将数据批量化，并可选地在训练过程中对数据进行打乱和并行处理。\n",
    "\n",
    "一些额外的最佳实践包括：\n",
    "\n",
    "- **预训练词嵌入**：在某些情况下，使用预训练的词嵌入（如GloVe或Word2Vec）作为模型的初始权重可以提高性能，特别是当可用的训练数据量不大时。\n",
    "\n",
    "- **高级文本编码方法**：对于复杂的NLP任务，考虑使用如BERT、GPT或其他Transformer架构的预训练模型，这些模型能够捕获文本中的深层次语义关系，并且可以微调以适应特定的任务。\n",
    "\n",
    "- **注意力机制和序列模型**：对于需要捕捉长距离依赖的任务（如机器翻译或文本生成），使用LSTM、GRU或Transformer模型可以获得更好的结果。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61feea7fcb27099c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 0.2 Embedding Layer\n",
    "在pytorch中，一般直接使用在定义神经网络类的层时，使用`nn.Embedding`来实现词嵌入。嵌入层的权重是随机初始化的，随后在训练过程中学习。这样，模型就可以学习到一个针对特定任务优化的词嵌入。\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5958959abff054b2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 0.3 Pre-trained Word Embeddings\n",
    "除了直接使用nn.Embedding层进行嵌入，还可以使用预训练的词嵌入（如GloVe、Word2Vec等）来初始化模型的词嵌入层。这种方法通常可以提高模型的性能，特别是在训练数据有限的情况下。\n",
    "\n",
    "在PyTorch框架下使用预训练的词嵌入（如GloVe、Word2Vec等）涉及到几个步骤，从加载预训练词向量，创建嵌入层，到将这些预训练向量加载到模型中。以下是一个概括的最佳实践步骤和相应的代码实现：\n",
    "\n",
    "#### 步骤\n",
    "\n",
    "1. **加载预训练词向量**：首先，需要从预训练模型中加载词向量。这通常意味着加载一个大型的文件，其中包含了大量单词及其对应的向量表示。\n",
    "\n",
    "2. **创建词汇表**：基于你的训练数据，创建一个词汇表，确保每个单词都有一个唯一的整数索引。\n",
    "\n",
    "3. **初始化嵌入矩阵**：首先，创建一个零矩阵或随机初始化的矩阵，其形状为`(len(dictionary), embedding_dim)`，其中`embedding_dim`是GloVe向量的维度（对于`glove-twitter-25`，维度是25）。\n",
    "\n",
    "4. **填充嵌入矩阵**：遍历`dictionary`中的每个词汇，如果该词汇在`glove_wv`中有对应的向量，就将这个向量复制到嵌入矩阵中对应的行；如果没有找到，就保留零向量或随机向量。\n",
    "\n",
    "5. **创建嵌入层**：在PyTorch模型中创建一个`nn.Embedding`层，其大小和预训练词向量相匹配。\n",
    "\n",
    "6. **加载嵌入矩阵到嵌入层**：将准备好的嵌入矩阵加载到`nn.Embedding`层中，这通常通过替换嵌入层的权重实现。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "af3ce5974cb56c28"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.models import word2vec\n",
    "from gensim.utils import tokenize\n",
    "import gensim.downloader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# - `%load_ext autoreload` 是一个魔术命令，用于加载或重新加载扩展。在这个例子中，它加载了`autoreload`扩展。这个扩展允许Jupyter自动检测模块代码的更改，并在不需要重启笔记本的情况下，自动重新加载这些模块。\n",
    "# \n",
    "# - `%autoreload 2` 是设置`autoreload`模式的命令，其中`2`表示一种特定的自动重新加载策略。具体来说，`2`表示在执行任何代码之前，自动重新加载所有标记的模块。这意味着只要你在笔记本中运行任何代码片段，它就会自动检查并重新加载自上次导入以来已经更改的所有模块。\n",
    "%load_ext autoreload \n",
    "%autoreload 2 "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T10:04:54.845769Z",
     "start_time": "2024-04-04T10:04:49.732024Z"
    }
   },
   "id": "511176a5269ca08e",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Load a Dataset\n",
    "\n",
    "\n",
    "首先，我们将加载 `tweets hate speech` 数据集。当使用`name=sentiment`时，我们将加载一个包含三个类别的数据集，其中 0表示负面言论，1表示中性言论，2表示正面言论。该数据集常用于情感分析任务。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b623ef44bdc2e1be"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset with 45615 instances loaded\n",
      "Development/validation dataset with 2000 instances loaded\n",
      "Test dataset with 12284 instances loaded\n"
     ]
    }
   ],
   "source": [
    "cache_dir = \"./data_cache\"\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"tweet_eval\",\n",
    "    name=\"sentiment\",\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "\n",
    "train_dataset = dataset[\"train\"]\n",
    "val_dataset = dataset[\"validation\"]\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "print(f\"Training dataset with {len(train_dataset)} instances loaded\")\n",
    "print(f\"Development/validation dataset with {len(val_dataset)} instances loaded\")\n",
    "print(f\"Test dataset with {len(test_dataset)} instances loaded\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T10:05:04.994045Z",
     "start_time": "2024-04-04T10:04:56.778208Z"
    }
   },
   "id": "f741a2615d49f7b2",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['text', 'label'],\n    num_rows: 45615\n})"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T13:57:27.285693Z",
     "start_time": "2024-04-03T13:57:27.229685Z"
    }
   },
   "id": "308a1451e18aee66",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Preprocess the Data\n",
    "\n",
    "\n",
    "\n",
    "对于我们的数据集，我们将执行以下预处理步骤：\n",
    "\n",
    "- tok_text: 将文本分词为单词，会去掉标点符号和空格\n",
    "- encode_text: 将文本转换为token对应的ID（索引+1）。因为要将0用于填充，所以索引从1开始.需要先建立一个Dictionary，将单词映射到索引\n",
    "- pad_text: 当文档长度小于最大长度时，用0填充文档；当文档长度大于最大长度时，截断文档\n",
    "- convert_to_data_loader: 将数据集转换为DataLoader对象，以便使用小批量随机梯度下降进行学习\n",
    "\n",
    "在pytorch中，一般直接使用在定义神经网络类的层时，使用`nn.Embedding`来实现词向量转换。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa2fac350bffd995"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1 Tokenize the Text\n",
    "对于dataset对象，添加一个新的键值对，即tokens，其中包含文本的分词结果。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "51e1ae6e5c80c08e"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['text', 'label', 'tokens'],\n    num_rows: 45615\n})"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.utils import tokenize\n",
    "\n",
    "# tokenize函数：将文本分词为单词，会去掉标点符号和空格\n",
    "def tok_text(sample):\n",
    "    sample[\"tokens\"] = list(tokenize(sample['text']))\n",
    "    return sample\n",
    "\n",
    "tok_train_dataset = train_dataset.map(tok_text)\n",
    "tok_val_dataset = val_dataset.map(tok_text)\n",
    "tok_test_dataset = test_dataset.map(tok_text)\n",
    "\n",
    "tok_train_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T10:05:07.337500Z",
     "start_time": "2024-04-04T10:05:07.255994Z"
    }
   },
   "id": "c1f660df5398bb8b",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 Encode the Text\n",
    "对于dataset对象，添加一个新的键值对，即encoded，将分词的每个词使用建立好的Dictionary映射到整数索引。由于需要使用0来填充，所以索引从1开始。\n",
    "\n",
    "建立Dictionary一般只使用训练集，因为测试集和验证集可能包含未知的单词。通过这种方式，我们可以确保模型不会在训练集之外的单词上进行训练。当在测试集上进行预测时，我们可以使用`<UNK>`标记来表示未知的单词，提升模型的泛化能力。\n",
    "\n",
    "对于未知词（Out-Of-Vocabulary, OOV）以及相关错误处理是NLP项目中的重要部分。在这里，我们将使用`<UNK>`标记来表示未知词。确保\"<UNK>\"键有一个默认值，比如0或任意整数。这确保了如果<UNK>不在字典中，则会添加它，并赋予一个明确的整数值。这个值应该是独一无二的，不与字典中其他词汇的值冲突。常见的做法是将其设置为当前字典大小的下一个整数，即`len(dictionary.token2id)`。\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0baf4bb5a59ce9f"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53730\n"
     ]
    }
   ],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# 构建Dictionary\n",
    "dictionary = Dictionary(tok_train_dataset[\"tokens\"]) # construct word<->id mappings - it does it in alphabetical order\n",
    "print(len(dictionary)) # 53730 个单词"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T10:05:11.354548Z",
     "start_time": "2024-04-04T10:05:09.752674Z"
    }
   },
   "id": "99b8e7bc18330da",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/45615 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7ee14e0e50204b1ba6898c3e9ef9f11b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/12284 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c69a2866912b44bf8c1d1a658aa4fe7d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3a63c2f9f3c24682b8577b06c1aea674"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['text', 'label', 'tokens', 'input_ids'],\n    num_rows: 45615\n})"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 如果\"<UNK>\"不在dictionary.token2id中，则添加它，并赋予一个独立的整数值\n",
    "if \"<UNK>\" not in dictionary.token2id:\n",
    "    dictionary.token2id[\"<UNK>\"] = len(dictionary.token2id) # some_integer_value可以是任意整数，比如最大的id+1\n",
    "\n",
    "def encode_text(sample):\n",
    "    sample['input_ids'] = [dictionary.token2id.get(token, dictionary.token2id.get(\"<UNK>\")) + 1 for token in sample['tokens']]\n",
    "    return sample\n",
    "# 在这个解决方案中，.get()方法尝试从dictionary.token2id获取token对应的id，如果失败，则返回<UNK>对应的id。\n",
    "\n",
    "pre_train_dataset = tok_train_dataset.map(encode_text)\n",
    "pre_test_dataset = tok_test_dataset.map(encode_text)\n",
    "pre_val_dataset = tok_val_dataset.map(encode_text)\n",
    "\n",
    "pre_train_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T10:05:18.020951Z",
     "start_time": "2024-04-04T10:05:12.819557Z"
    }
   },
   "id": "5bdcf1e70f611622",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3 Pad & Truncate \n",
    "由于神经网络要求输入的文档长度是固定的，我们需要对文档进行填充或截断。我们将选择合适的最大长度，并对小于该长度的文档进行0填充，对大于该长度的文档进行截断。\n",
    "\n",
    "填充（Padding）可以发生在文档的开头（称为前置填充或pre-padding）或结尾（称为后置填充或post-padding），这取决于特定的应用场景和模型架构的要求。下面是两种情况的考虑因素：\n",
    "\n",
    "#### 前置填充（Pre-padding）\n",
    "- **序列处理的方向**：如果模型（如某些类型的循环神经网络RNN）从序列的末尾开始处理信息，则在序列开头进行填充可能更有意义。这样，模型首先遇到的是有效数据而不是填充数据，可能有助于效果的提升。\n",
    "- **注意力机制**：在使用注意力机制的模型中，如果注意力可以更集中于序列的实际内容而非填充部分，则前置填充可能更优。\n",
    "  \n",
    "#### 后置填充（Post-padding）\n",
    "- **序列处理的方向**：对于从序列开始处处理信息的模型（如标准的RNN），在序列的结尾添加填充可能更自然，因为这样可以确保模型在处理每个序列时首先遇到的是真实的数据点。\n",
    "- **兼容性**：某些框架和库可能默认使用或优先支持后置填充，因此在没有特定需求的情况下遵循这些默认设置可以简化实现过程。\n",
    "\n",
    "\n",
    "首先查看每个文档的长度分布，以便选择合适的最大长度。可以通过直方图来查看文档长度的分布。\n",
    "下图可以看到，选择30作为最大长度是合适的。\n",
    "\n",
    "本次实验会使用前置填充。在对`pre_train_dataset`等使用map方法添加`pad_text`函数后，可以得到`pad_train_dataset`等数据集。这些数据集保证了每个文档的`input_ids`长度都是30。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c4b5503b9dd950c"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "len_list = [len(sample['input_ids']) for sample in pre_train_dataset]\n",
    "plt.hist(len_list, bins=30)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T19:13:52.413132Z",
     "start_time": "2024-04-03T19:13:50.292098Z"
    }
   },
   "id": "16ab0a450d9059f6",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "29de23f664a6aa6f"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/45615 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a3dab72be8824cd49521d15a7c508d41"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/12284 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3985101f73cf4728b96a1125f823296e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8fe3f7eca04449d499e24844dadf61cb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sequence_length = 30   # 选择30作为最大长度\n",
    "\n",
    "def pad_text(sample):\n",
    "\n",
    "    if len(sample['input_ids']) < sequence_length: # pre-pad\n",
    "        sample['input_ids'] = [0] * (sequence_length - len(sample['input_ids'])) + sample['input_ids']\n",
    "    elif len(sample['input_ids']) > sequence_length: # truncate\n",
    "        sample['input_ids'] = sample['input_ids'][:sequence_length]\n",
    "        \n",
    "    return sample\n",
    "\n",
    "pad_train_dataset = pre_train_dataset.map(pad_text)\n",
    "pad_test_dataset = pre_test_dataset.map(pad_text)\n",
    "pad_val_dataset = pre_val_dataset.map(pad_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T10:05:24.778666Z",
     "start_time": "2024-04-04T10:05:21.568264Z"
    }
   },
   "id": "19be03951943d43f",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.4 Convert to DataLoader\n",
    "将数据集转换为DataLoader对象，以便使用小批量随机梯度下降进行学习。在这里，我们将使用PyTorch的DataLoader类，它允许我们迭代数据集并返回小批量数据。我们将使用DataLoader对象来训练和评估我们的模型。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69b6324078d135a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "# convert from the Huggingface format to a TensorDataset so we can use the mini-batch sampling functionality\n",
    "def convert_to_data_loader(dataset, num_classes):\n",
    "    # convert from list to tensor\n",
    "    input_tensor = torch.from_numpy(np.array(dataset['input_ids'])) # 将input_ids转换为numpy数组，然后转换为tensor\n",
    "    label_tensor = torch.from_numpy(np.array(dataset['label'])).long() # 将label转换为numpy数组，然后转换为tensor\n",
    "    tensor_dataset = TensorDataset(input_tensor, label_tensor)\n",
    "    loader = DataLoader(tensor_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return loader\n",
    "\n",
    "num_classes = len(np.unique(pad_train_dataset[\"label\"]))   # number of possible labels \n",
    "\n",
    "train_loader = convert_to_data_loader(pad_train_dataset, num_classes)\n",
    "val_loader = convert_to_data_loader(pad_val_dataset, num_classes)\n",
    "test_loader = convert_to_data_loader(pad_test_dataset, num_classes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T10:05:27.735282Z",
     "start_time": "2024-04-04T10:05:26.780397Z"
    }
   },
   "id": "67872e8536198345",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.5 Pre-trained Word Embeddings\n",
    "可以直接将映射到整数的tokens输入到神经网络中，并使用embedding层将其转换为词嵌入。在这种情况下，模型将学习如何将整数索引映射到词嵌入，这些词嵌入将在训练过程中进行学习。但为了提高性能，我们可以使用预训练的词嵌入，如GloVe或Word2Vec。这些预训练的词嵌入通常是在大型语料库上训练的，可以提供更好的词嵌入表示。\n",
    "\n",
    "在构建嵌入矩阵时，有两个特殊的标识符 —— 一个是<unk>，用于表示词汇表中不存在的单词；另一个是<pad>，用于在必要时填充序列以使它们长度相等。这意味着你的嵌入矩阵需要考虑这两个特殊标识符，因此它的大小应该是 (vocab_size + 2, embedding_dim)。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8dab29e38436e053"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(53733, 50)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim.downloader\n",
    "\n",
    "# 加载预训练词向量\n",
    "glove_vectors = gensim.downloader.load('glove-twitter-50')\n",
    "\n",
    "# 初始化嵌入矩阵\n",
    "vocab_size = len(dictionary)  # 原始词汇表大小，不包含特殊标识符 [53731]\n",
    "embedding_matrix = np.zeros((vocab_size + 2, glove_vectors.vector_size)) # +2 for the <UNK> token and the <PAD> token\n",
    "\n",
    "# 填充嵌入矩阵\n",
    "for token, idx in dictionary.token2id.items():\n",
    "    if token in glove_vectors: # 如果单词在预训练词向量中，则使用预训练词向量\n",
    "        embedding_matrix[idx] = glove_vectors[token]\n",
    "    else: # 如果单词不在预训练词向量中，则使用随机初始化的向量\n",
    "        embedding_matrix[idx] = np.random.normal(size=(glove_vectors.vector_size)) \n",
    "\n",
    "embedding_matrix.shape # [53733, 25]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T10:06:03.903470Z",
     "start_time": "2024-04-04T10:05:32.985688Z"
    }
   },
   "id": "fa31641cbf9931ed",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Define a Neural Network Model\n",
    "在这里，我们将定义一个简单的神经网络模型，用于对文本进行情感分类。由于这是一个序列分类任务，我们将使用一个LSTM模型，它可以处理序列数据并捕获序列中的长距离依赖关系。我们还将使用一个嵌入层，将整数索引转换为词嵌入。最后，我们将使用一个全连接层将LSTM的输出转换为类别概率。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26029efe9ee7979b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "```python\n",
    "# 假设我们有一个大小为 10000 的词汇表，希望将每个单词映射到一个 300 维的向量空间。\n",
    "nn.Embedding(num_embeddings=10000, embedding_dim=300)\n",
    "\n",
    "# 假设我们已经初始化并填充了一个嵌入矩阵: embedding_matrix\n",
    "nn.Embedding.from_pretrained(torch.FloatTensor(embedding_matrix), freeze=False)\n",
    "```\n",
    "\n",
    "### 参数解释\n",
    "\n",
    "- `vocab_size + 2` 表明你的词汇表大小外加两个特殊标记（如`<unk>`和`<pad>`）。\n",
    "- `embedding_dim=25` 指的是每个词向量的维度。\n",
    "- `padding_idx=0` 意味着索引为0的向量（通常对应于`<pad>`标记）将被用作填充，且其向量在嵌入矩阵中被初始化为0向量。在模型训练过程中，这个向量不会被更新。\n",
    "\n",
    "使用`padding_idx`是处理变长输入序列时的一种常见和有效的方法。通过这种方式，模型能够忽略填充的部分，只关注输入序列中的有效内容。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "22168cc8b65d3eae"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(embedding_matrix.shape[1], hidden_size, num_layers, \n",
    "                            batch_first=True, bidirectional=True) # 双向LSTM,这会使输出维度翻倍\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)  # 维度是两倍的hidden_size\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_matrix), freeze=False)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch_size, seq_length) -> (batch_size, seq_length, embedding_dim)\n",
    "        embedded = self.embedding(x)\n",
    "        # (batch_size, seq_length, embedding_dim) -> output: (batch_size, seq_length, hidden_size*2)\n",
    "        output, _ = self.lstm(embedded)\n",
    "        \n",
    "        # 合并正向和反向LSTM的最后时间步输出，此步骤不改变数据形状，但需要从LSTM输出中提取正向和反向的最后时间步输出。\n",
    "        # 正向最后时间步的输出\n",
    "        forward_output = output[:, -1, :self.hidden_size]\n",
    "        # 反向最后时间步的输出\n",
    "        backward_output = output[:, 0, self.hidden_size:]\n",
    "        # 将两个方向的输出拼接起来 output： (batch_size, 2 * hidden_size)\n",
    "        output = torch.cat((forward_output, backward_output), dim=1)\n",
    "        \n",
    "        # 通过激活层 不改变数据形状\n",
    "        output = self.activation(output)\n",
    "        # 通过全连接层\n",
    "        # (batch_size, 2 * hidden_size) -> (batch_size, num_classes)\n",
    "        output = self.fc(output)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T10:06:06.740540Z",
     "start_time": "2024-04-04T10:06:06.679533Z"
    }
   },
   "id": "fcbd84a55de36efc",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMModel(\n",
      "  (lstm): LSTM(50, 32, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (fc): Linear(in_features=64, out_features=3, bias=True)\n",
      "  (embedding): Embedding(53733, 50)\n",
      "  (activation): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 定义模型参数\n",
    "hidden_size = 32\n",
    "num_layers = 2\n",
    "num_classes = len(np.unique(pad_train_dataset[\"label\"]))  # 3\n",
    "\n",
    "# 初始化模型\n",
    "device = torch.device('cuda')\n",
    "model = LSTMModel(embedding_matrix, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(model)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-04T10:06:15.267849Z",
     "start_time": "2024-04-04T10:06:08.551365Z"
    }
   },
   "id": "eb049ff007ab6f73",
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 关于损失的记录\n",
    "使用样本数计算平均损失：`average_loss = epoch_loss / len(train_loader.dataset)`。这是因为在训练过程中，我们通常使用小批量随机梯度下降，而不是整个数据集。因此，我们需要将损失值除以数据集中的样本数，以获得平均损失。同时，对于验证集和\n",
    "当你选择以每个样本为基础来计算平均损失时，你是在计算整个数据集中每个样本的平均损失值。这是通过将总损失（分子）除以数据集中的样本总数（分母）来实现的。这种方法能够提供每个样本平均上的损失，对于评估模型在处理单个数据点方面的效能非常有用。\n",
    "\n",
    "分子：整个epoch中所有批次损失的累加和。\n",
    "分母：len(loader.dataset)，即数据集中的总样本数。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a913eea63708e25d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 定义训练函数\n",
    "def train(model, train_loader, val_loader, epoch,train_accuracies,val_accuracies):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "        pred = output.argmax(1) # argmax(1)表示取每行中最大值的索引,即dim=1\n",
    "        correct += pred.eq(target).sum().item()\n",
    "        total += target.nelement() # nelement()返回张量中元素的个数\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\n",
    "                f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
    "            \n",
    "    # all_train_losses.append(np.mean(train_losses))\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    train_accuracy = 100. * correct / total\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    print(\"\\nTrain Loss: {:.4f}\".format(avg_train_loss))\n",
    "    print(\"Train Accuracy: {:.2f}%\".format(train_accuracy))\n",
    "    \n",
    "    # 在验证集上评估模型\n",
    "    model.eval()\n",
    "    \n",
    "    total_val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            # val_losses.append(criterion(output, target).item())\n",
    "            loss = criterion(output, target)\n",
    "            total_val_loss += loss.item()\n",
    "            pred = output.argmax(dim=1, keepdim=True) # keepdimm=True表示保持输出张量的维度\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            total += target.nelement()\n",
    "            \n",
    "            \n",
    "    # all_val_losses.append(np.mean(val_losses))\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_accuracy = 100. * correct / total\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    \n",
    "    print(\"Validation Loss: {:.4f}\".format(avg_val_loss)),\n",
    "    print(\"Validation Accuracy: {:.2f}%\\n\".format(val_accuracy))\n",
    "    \n",
    "    return avg_train_loss, avg_val_loss,train_accuracies,val_accuracies\n",
    "\n",
    "# 定义评估函数\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print(f\"\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({test_accuracy:.0f}%)\\n\")\n",
    "    \n",
    "    return test_loss, test_accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T12:10:20.684273Z",
     "start_time": "2024-04-05T12:10:20.584767Z"
    }
   },
   "id": "291b9e5c3de495c2",
   "execution_count": 54
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/45615 (0%)]\tLoss: 0.000004\n",
      "Train Epoch: 1 [6400/45615 (14%)]\tLoss: 0.000002\n",
      "Train Epoch: 1 [12800/45615 (28%)]\tLoss: 0.000004\n",
      "Train Epoch: 1 [19200/45615 (42%)]\tLoss: 0.000123\n",
      "Train Epoch: 1 [25600/45615 (56%)]\tLoss: 0.000003\n",
      "Train Epoch: 1 [32000/45615 (70%)]\tLoss: 0.000006\n",
      "Train Epoch: 1 [38400/45615 (84%)]\tLoss: 0.000005\n",
      "Train Epoch: 1 [44800/45615 (98%)]\tLoss: 0.000002\n",
      "\n",
      "Train Loss: 0.0008\n",
      "Train Accuracy: 99.93%\n",
      "Validation Loss: 3.9903\n",
      "Validation Accuracy: 59.00%\n",
      "\n",
      "Train Epoch: 2 [0/45615 (0%)]\tLoss: 0.000002\n",
      "Train Epoch: 2 [6400/45615 (14%)]\tLoss: 0.000002\n",
      "Train Epoch: 2 [12800/45615 (28%)]\tLoss: 0.012812\n",
      "Train Epoch: 2 [19200/45615 (42%)]\tLoss: 0.000002\n",
      "Train Epoch: 2 [25600/45615 (56%)]\tLoss: 0.000003\n",
      "Train Epoch: 2 [32000/45615 (70%)]\tLoss: 0.000002\n",
      "Train Epoch: 2 [38400/45615 (84%)]\tLoss: 0.000001\n",
      "Train Epoch: 2 [44800/45615 (98%)]\tLoss: 0.000004\n",
      "\n",
      "Train Loss: 0.0008\n",
      "Train Accuracy: 99.94%\n",
      "Validation Loss: 3.9718\n",
      "Validation Accuracy: 59.00%\n",
      "\n",
      "Train Epoch: 3 [0/45615 (0%)]\tLoss: 0.000002\n",
      "Train Epoch: 3 [6400/45615 (14%)]\tLoss: 0.000002\n",
      "Train Epoch: 3 [12800/45615 (28%)]\tLoss: 0.000006\n",
      "Train Epoch: 3 [19200/45615 (42%)]\tLoss: 0.000003\n",
      "Train Epoch: 3 [25600/45615 (56%)]\tLoss: 0.000001\n",
      "Train Epoch: 3 [32000/45615 (70%)]\tLoss: 0.000003\n",
      "Train Epoch: 3 [38400/45615 (84%)]\tLoss: 0.000003\n",
      "Train Epoch: 3 [44800/45615 (98%)]\tLoss: 0.000006\n",
      "\n",
      "Train Loss: 0.0008\n",
      "Train Accuracy: 99.93%\n",
      "Validation Loss: 4.0301\n",
      "Validation Accuracy: 59.10%\n",
      "\n",
      "Train Epoch: 4 [0/45615 (0%)]\tLoss: 0.000002\n",
      "Train Epoch: 4 [6400/45615 (14%)]\tLoss: 0.000003\n",
      "Train Epoch: 4 [12800/45615 (28%)]\tLoss: 0.000004\n",
      "Train Epoch: 4 [19200/45615 (42%)]\tLoss: 0.000003\n",
      "Train Epoch: 4 [25600/45615 (56%)]\tLoss: 0.011390\n",
      "Train Epoch: 4 [32000/45615 (70%)]\tLoss: 0.000004\n",
      "Train Epoch: 4 [38400/45615 (84%)]\tLoss: 0.000001\n",
      "Train Epoch: 4 [44800/45615 (98%)]\tLoss: 0.000005\n",
      "\n",
      "Train Loss: 0.0008\n",
      "Train Accuracy: 99.94%\n",
      "Validation Loss: 4.1492\n",
      "Validation Accuracy: 59.00%\n",
      "\n",
      "Train Epoch: 5 [0/45615 (0%)]\tLoss: 0.000001\n",
      "Train Epoch: 5 [6400/45615 (14%)]\tLoss: 0.000005\n",
      "Train Epoch: 5 [12800/45615 (28%)]\tLoss: 0.000003\n",
      "Train Epoch: 5 [19200/45615 (42%)]\tLoss: 0.000001\n",
      "Train Epoch: 5 [25600/45615 (56%)]\tLoss: 0.000003\n",
      "Train Epoch: 5 [32000/45615 (70%)]\tLoss: 0.000006\n",
      "Train Epoch: 5 [38400/45615 (84%)]\tLoss: 0.000140\n",
      "Train Epoch: 5 [44800/45615 (98%)]\tLoss: 0.000002\n",
      "\n",
      "Train Loss: 0.0008\n",
      "Train Accuracy: 99.92%\n",
      "Validation Loss: 4.1001\n",
      "Validation Accuracy: 58.90%\n",
      "\n",
      "Train Epoch: 6 [0/45615 (0%)]\tLoss: 0.000001\n",
      "Train Epoch: 6 [6400/45615 (14%)]\tLoss: 0.000001\n",
      "Train Epoch: 6 [12800/45615 (28%)]\tLoss: 0.000002\n",
      "Train Epoch: 6 [19200/45615 (42%)]\tLoss: 0.000003\n",
      "Train Epoch: 6 [25600/45615 (56%)]\tLoss: 0.000002\n",
      "Train Epoch: 6 [32000/45615 (70%)]\tLoss: 0.000001\n",
      "Train Epoch: 6 [38400/45615 (84%)]\tLoss: 0.000011\n",
      "Train Epoch: 6 [44800/45615 (98%)]\tLoss: 0.000001\n",
      "\n",
      "Train Loss: 0.0008\n",
      "Train Accuracy: 99.93%\n",
      "Validation Loss: 4.1851\n",
      "Validation Accuracy: 58.70%\n",
      "\n",
      "Train Epoch: 7 [0/45615 (0%)]\tLoss: 0.000001\n",
      "Train Epoch: 7 [6400/45615 (14%)]\tLoss: 0.000000\n",
      "Train Epoch: 7 [12800/45615 (28%)]\tLoss: 0.000002\n",
      "Train Epoch: 7 [19200/45615 (42%)]\tLoss: 0.000011\n",
      "Train Epoch: 7 [25600/45615 (56%)]\tLoss: 0.000001\n",
      "Train Epoch: 7 [32000/45615 (70%)]\tLoss: 0.000004\n",
      "Train Epoch: 7 [38400/45615 (84%)]\tLoss: 0.000000\n",
      "Train Epoch: 7 [44800/45615 (98%)]\tLoss: 0.000000\n",
      "\n",
      "Train Loss: 0.0008\n",
      "Train Accuracy: 99.94%\n",
      "Validation Loss: 4.2521\n",
      "Validation Accuracy: 59.10%\n",
      "\n",
      "Train Epoch: 8 [0/45615 (0%)]\tLoss: 0.000003\n",
      "Train Epoch: 8 [6400/45615 (14%)]\tLoss: 0.000001\n",
      "Train Epoch: 8 [12800/45615 (28%)]\tLoss: 0.000001\n",
      "Train Epoch: 8 [19200/45615 (42%)]\tLoss: 0.000001\n",
      "Train Epoch: 8 [25600/45615 (56%)]\tLoss: 0.000000\n",
      "Train Epoch: 8 [32000/45615 (70%)]\tLoss: 0.000001\n",
      "Train Epoch: 8 [38400/45615 (84%)]\tLoss: 0.000001\n",
      "Train Epoch: 8 [44800/45615 (98%)]\tLoss: 0.000000\n",
      "\n",
      "Train Loss: 0.0008\n",
      "Train Accuracy: 99.94%\n",
      "Validation Loss: 4.2900\n",
      "Validation Accuracy: 59.10%\n",
      "\n",
      "Train Epoch: 9 [0/45615 (0%)]\tLoss: 0.000003\n",
      "Train Epoch: 9 [6400/45615 (14%)]\tLoss: 0.000002\n",
      "Train Epoch: 9 [12800/45615 (28%)]\tLoss: 0.000002\n",
      "Train Epoch: 9 [19200/45615 (42%)]\tLoss: 0.013271\n",
      "Train Epoch: 9 [25600/45615 (56%)]\tLoss: 0.000000\n",
      "Train Epoch: 9 [32000/45615 (70%)]\tLoss: 0.000001\n",
      "Train Epoch: 9 [38400/45615 (84%)]\tLoss: 0.000002\n",
      "Train Epoch: 9 [44800/45615 (98%)]\tLoss: 0.000100\n",
      "\n",
      "Train Loss: 0.0008\n",
      "Train Accuracy: 99.93%\n",
      "Validation Loss: 4.4155\n",
      "Validation Accuracy: 59.10%\n",
      "\n",
      "Train Epoch: 10 [0/45615 (0%)]\tLoss: 0.000004\n",
      "Train Epoch: 10 [6400/45615 (14%)]\tLoss: 0.000000\n",
      "Train Epoch: 10 [12800/45615 (28%)]\tLoss: 0.000001\n",
      "Train Epoch: 10 [19200/45615 (42%)]\tLoss: 0.000001\n",
      "Train Epoch: 10 [25600/45615 (56%)]\tLoss: 0.000001\n",
      "Train Epoch: 10 [32000/45615 (70%)]\tLoss: 0.012876\n",
      "Train Epoch: 10 [38400/45615 (84%)]\tLoss: 0.000001\n",
      "Train Epoch: 10 [44800/45615 (98%)]\tLoss: 0.000001\n",
      "\n",
      "Train Loss: 0.0008\n",
      "Train Accuracy: 99.94%\n",
      "Validation Loss: 4.3974\n",
      "Validation Accuracy: 58.90%\n",
      "\n",
      "Test set: Average loss: 5.7658, Accuracy: 6510/12284 (53%)\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "num_epochs = 10\n",
    "\n",
    "train_losses_epoch, val_losses_epoch ,train_accuracies,val_accuracies= [], [],[],[]\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    avg_train_loss, avg_val_loss,train_accuracies,val_accuracies = train(model, train_loader, val_loader, epoch,train_accuracies,val_accuracies)\n",
    "    train_losses_epoch.append(avg_train_loss)\n",
    "    val_losses_epoch.append(avg_val_loss)\n",
    "    \n",
    "# 评估模型\n",
    "test_loss, test_accuracy = evaluate(model, test_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T12:10:51.327165Z",
     "start_time": "2024-04-05T12:10:22.953550Z"
    }
   },
   "id": "bf929fa016d5010a",
   "execution_count": 55
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 绘制训练和验证损失曲线\n",
    "plt.plot(train_losses_epoch, label='Training loss')\n",
    "plt.plot(val_losses_epoch, label='Validation loss')\n",
    "plt.legend(frameon=False)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T12:10:54.570573Z",
     "start_time": "2024-04-05T12:10:54.431550Z"
    }
   },
   "id": "5bd4106655df5e47",
   "execution_count": 56
  },
  {
   "cell_type": "markdown",
   "source": [
    "当你使用`len(loader)`和`len(loader.dataset)`时，你实际上在获取两个不同的长度值：\n",
    "\n",
    "- `len(loader)`给出的是批次（batches）的总数，即整个数据集被分割成多少个批次进行迭代。\n",
    "- `len(loader.dataset)`给出的是数据集中样本的总数。\n",
    "\n",
    "在计算平均损失时，我们通常先计算每个批次的损失，然后将这些损失加起来，最后除以批次的总数（`len(loader)`），从而得到每个epoch的平均损失。这样做的原因有几个：\n",
    "\n",
    "1. **一致性**：不同批次可能包含不同数量的样本，尤其是最后一个批次，可能因为数据集大小不是批大小的整数倍而包含更少的样本。通过将总损失除以批次的数量而不是样本总数，可以避免因批次大小不一致而导致的计算偏差。\n",
    "\n",
    "2. **简便性**：在大多数情况下，我们关心的是模型在整个数据集上的平均表现，而不是单个样本的性能。计算每个批次的平均损失，然后再取这些平均值的平均，提供了一个直观的性能指标，便于理解和比较。\n",
    "\n",
    "3. **效率**：这种方法简化了损失计算，因为大多数深度学习框架都会自动计算批次内的平均或总损失，我们只需要对这些值进行进一步的平均化处理即可。\n",
    "\n",
    "综上所述，通过除以`len(loader)`（批次的总数），我们得到的是每个epoch的平均批次损失，这是一个标准的做法，用于监控和优化模型训练过程。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "534ebdb9f66b8e2c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
