{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing for Deep Learning NLP   \n",
    "\n",
    "## 1. Introduction\n",
    "在深度学习应用于自然语言处理（NLP）的背景下，数据预处理是一个至关重要的步骤，它通常包括以下几个环节：\n",
    "\n",
    "1. **文本清洗（Text cleaning）**：\n",
    "   - **去除噪声**：例如去除HTML标签、特殊字符等。\n",
    "   - **规范化文本**：比如将所有文本转换为小写，以减少大小写造成的变体。\n",
    "   - **分词（Tokenization）**：这是NLP中的一个基本步骤，涉及将连续的文本字符串分解为单独的单词或符号。\n",
    "   - **去除停用词（Stop word removal）**：诸如“the”、“is”、“in”等单词通常对于理解文本的含义不是很有用，可以被移除。\n",
    "   - **词干提取（Stemming）/词形还原（Lemmatization）**：比如将“running”还原为“run”，有助于模型理解不同词形的共同含义。\n",
    "\n",
    "2. **构建词汇表（Vocabulary construction）**：\n",
    "   - **确定模型的词汇**：基于训练数据创建一个单词集合。\n",
    "\n",
    "3. **词嵌入（Word embeddings）**：\n",
    "   - **单词向量化**：使用预训练的词向量（如GloVe、Word2Vec）或在训练过程中学习词向量。\n",
    "\n",
    "4. **序列填充（Padding）/截断（Truncation）**：\n",
    "   - **统一序列长度**：由于神经网络需要固定长度的输入，过长的序列可能需要截断，而过短的序列则需要通过填充额外的零或特定的占位符来扩展。\n",
    "\n",
    "5. **构建标签（Label construction）**：\n",
    "   - **准备监督学习标签**：对于有监督学习任务，比如分类或序列标注，需要准备相应的标签。\n",
    "\n",
    "6. **数据增强（Data augmentation）**：\n",
    "   - **扩展训练数据**：使用各种技术（如同义词替换、随机删除、句子重组）来创建更多的训练样本，有助于提高模型的鲁棒性和泛化能力。\n",
    "\n",
    "7. **分批处理（Batching）**：\n",
    "   - **准备用于训练的数据批次**：组织数据以便以批次的形式进行有效训练。\n",
    "\n",
    "根据特定的任务和模型架构，预处理的具体步骤和实现可能会有所不同。例如，对于Transformer模型，通常会添加特殊的起始（[CLS]）和分隔（[SEP]）标记。而对于句子对任务，则可能需要确保两个句子合并后的长度不超过模型的最大序列长度。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a958d0001da1fbae"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import spacy\n",
    "from datasets import load_dataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.vocab import GloVe\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.vocab import vocab\n",
    "from torch.nn import Embedding\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T20:18:08.486269Z",
     "start_time": "2024-03-31T20:18:05.826500Z"
    }
   },
   "id": "e46f0bb5d640b501",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Text Cleaning\n",
    "spaCy是一个流行的自然语言处理库，提供了许多文本处理功能，包括分词、词形还原、命名实体识别等。在本节中，我们将使用spaCy来清理文本数据。\n",
    "\n",
    "spaCy提供多种不同类型和大小的预训练模型，适用于多种语言。这些模型包括：\n",
    "\n",
    "1. **核心模型（Core Models）**：\n",
    "   - 包含了词性标注、句法分析、命名实体识别等功能。\n",
    "   - 对于英语，提供了不同大小的模型，如`en_core_web_sm`（小型）、`en_core_web_md`（中型）、`en_core_web_lg`（大型），还有`en_core_web_trf`（基于transformer的模型）。\n",
    "\n",
    "2. **多语种和语言特定模型**：\n",
    "   - spaCy提供了对多种语言的支持，如西班牙语、法语、德语、俄语等。\n",
    "   - 这些模型有不同的大小版本，一般也遵循`xx_core_web_sm`、`xx_core_web_md`、`xx_core_web_lg`的命名规范（`xx`是语言代码）。\n",
    "\n",
    "3. **Transformer模型**：\n",
    "   - 如`en_core_web_trf`，这类模型基于预训练的Transformer模型，如BERT或RoBERTa，它们通常提供更优的性能，尤其在理解上下文和复杂语义方面。\n",
    "\n",
    "除了核心模型，spaCy社区还贡献了额外的模型和管道，这些可以通过spaCy官方网站和spaCy Universe来发现。这些额外的资源可能包括特定于某个领域的模型或用于特定任务的工具，比如情感分析、文本分类等。\n",
    "\n",
    "模型的选择依赖于具体的任务和所需语言。一般而言，小型模型下载快、占用空间少，适合快速原型设计或当环境资源有限时使用；大型模型和基于Transformer的模型提供更高的精度和更好的性能，但下载和加载时间更长，需要更多的资源。\n",
    "\n",
    "可以使用spaCy的命令行工具或在Python代码中使用`spacy.cli.download`函数来下载这些模型。使用时，需要确保选择的模型与你使用的spaCy版本兼容。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a593cdea660907e3"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple is looking at buying U.K. startup for $1 billion. The foxes walk into the box. The dog was barking.\n",
      "\n",
      "Sentences:\n",
      "Apple is looking at buying U.K. startup for $1 billion.\n",
      "The foxes walk into the box.\n",
      "The dog was barking.\n",
      "\n",
      "Named Entities:\n",
      "Apple ORG\n",
      "U.K. GPE\n",
      "$1 billion MONEY\n",
      "\n",
      "Tokens and their POS tags:\n",
      "Apple PROPN\n",
      "is AUX\n",
      "looking VERB\n",
      "at ADP\n",
      "buying VERB\n",
      "U.K. PROPN\n",
      "startup NOUN\n",
      "for ADP\n",
      "$ SYM\n",
      "1 NUM\n",
      "billion NUM\n",
      ". PUNCT\n",
      "The DET\n",
      "foxes NOUN\n",
      "walk VERB\n",
      "into ADP\n",
      "the DET\n",
      "box NOUN\n",
      ". PUNCT\n",
      "The DET\n",
      "dog NOUN\n",
      "was AUX\n",
      "barking VERB\n",
      ". PUNCT\n",
      "\n",
      "Noun chunks:\n",
      "Apple\n",
      "U.K.\n",
      "The foxes\n",
      "the box\n",
      "The dog\n",
      "\n",
      "Lemmatized words:\n",
      "Apple Apple\n",
      "is be\n",
      "looking look\n",
      "at at\n",
      "buying buy\n",
      "U.K. U.K.\n",
      "startup startup\n",
      "for for\n",
      "$ $\n",
      "1 1\n",
      "billion billion\n",
      ". .\n",
      "The the\n",
      "foxes fox\n",
      "walk walk\n",
      "into into\n",
      "the the\n",
      "box box\n",
      ". .\n",
      "The the\n",
      "dog dog\n",
      "was be\n",
      "barking bark\n",
      ". .\n"
     ]
    }
   ],
   "source": [
    "# 加载英语模型\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 处理文本\n",
    "text = \"Apple is looking at buying U.K. startup for $1 billion. The foxes walk into the box. The dog was barking.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# 打印文本\n",
    "print(doc.text)\n",
    "\n",
    "# 遍历文档中的句子\n",
    "print('\\nSentences:')\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)\n",
    "    \n",
    "# 遍历文档中的命名实体\n",
    "print('\\nNamed Entities:')\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "\n",
    "# 打印每个token的文本和词性标注\n",
    "print('\\nTokens and their POS tags:')\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)\n",
    "\n",
    "# 名词短语\n",
    "print('\\nNoun chunks:')\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text)\n",
    "    \n",
    "# 词形还原\n",
    "print('\\nLemmatized words:')\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T20:18:11.655185Z",
     "start_time": "2024-03-31T20:18:11.210768Z"
    }
   },
   "id": "3448b6a93efa0a3f",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 加载本地的imdb数据集，arrow格式\n",
    "data_files = {\n",
    "    \"train\": './data_cache/imdb/plain_text/0.0.0/e6281661ce1c48d982bc483cf8a173c1bbeb5d31/imdb-train.arrow',\n",
    "    \"test\": \"./data_cache/imdb/plain_text/0.0.0/e6281661ce1c48d982bc483cf8a173c1bbeb5d31/imdb-test.arrow\",\n",
    "}\n",
    "dataset = load_dataset('arrow', data_files=data_files)\n",
    "\n",
    "# split the dataset,only use 1000 samples for demo\n",
    "train_dataset = dataset['train'].select(range(1000)) # select 函数用于选择数据集的子集，这里选择前1000个样本\n",
    "test_dataset = dataset['test'].select(range(500))\n",
    "\n",
    "# 获取文本和标签\n",
    "train_text = [sample[\"text\"] for sample in train_dataset]\n",
    "train_label = [sample[\"label\"] for sample in train_dataset]\n",
    "test_text = [sample[\"text\"] for sample in test_dataset]\n",
    "test_label =  [sample[\"label\"] for sample in test_dataset]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T20:18:14.024826Z",
     "start_time": "2024-03-31T20:18:13.246241Z"
    }
   },
   "id": "af24fc61ad41928f",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1 spaCy文本预处理\n",
    "一般来说，spaCy的文本预处理包括以下几个步骤：\n",
    "- 加载spaCy模型\n",
    "- 定义文本预处理函数：\n",
    "    - 使用`nlp.pipe`方法高效地处理大量文本,禁用不需要的组件以提高效率。\n",
    "    - 去除停用词、标点和空格，并进行词形还原或其他处理。\n",
    "    - 将处理后的文本重新组合成字符串。\n",
    "- 对train_text和test_text进行预处理。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d85130b1c7842f3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 加载Spacy模型\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 定义文本预处理函数\n",
    "# 该函数使用Spacy进行文本预处理，包括分词、词形还原和去除停用词\n",
    "# 使用Spacy的管道（pipe）机制，可以高效地处理大量文本，优于直接使用nlp(text)方法；可选择禁用不需要的组件以提高效率\n",
    "def preprocess_text(texts):\n",
    "    cleaned_texts = []\n",
    "    for doc in nlp.pipe(texts, disable=[\"parser\", \"ner\"]):\n",
    "        # 去除停用词、标点和空格，并进行词形还原,转换为小写\n",
    "        tokens = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
    "        cleaned_text = \" \".join(tokens)\n",
    "        cleaned_texts.append(cleaned_text)\n",
    "    return cleaned_texts\n",
    "\n",
    "# 对训练集和测试集进行预处理\n",
    "train_text_clean = preprocess_text(train_text)\n",
    "test_text_clean = preprocess_text(test_text)\n",
    "\n",
    "# 预处理后的文本可以用于后续的NLP任务，比如特征提取、模型训练等"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T20:18:32.767313Z",
     "start_time": "2024-03-31T20:18:16.521008Z"
    }
   },
   "id": "69ec1f4c8d2979b5",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rent curious yellow video store controversy surround release 1967 hear seize u.s. custom try enter country fan film consider controversial myself.<br /><br />the plot center young swedish drama student name lena want learn life particular want focus attention make sort documentary average swede think certain political issue vietnam war race issue united states ask politician ordinary denizen stockholm opinion politic sex drama teacher classmate marry men.<br /><br />what kill curious yellow 40 year ago consider pornographic sex nudity scene far shoot like cheaply porno countryman mind find shocking reality sex nudity major staple swedish cinema ingmar bergman arguably answer good old boy john ford sex scene films.<br /><br />i commend filmmaker fact sex show film show artistic purpose shock people money show pornographic theater america curious yellow good film want study meat potato pun intend swedish cinema film plot\n"
     ]
    }
   ],
   "source": [
    "# 打印预处理后的文本\n",
    "print(train_text_clean[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T19:39:28.280059Z",
     "start_time": "2024-03-31T19:39:28.264050Z"
    }
   },
   "id": "67669db9eee8868",
   "execution_count": 72
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.Vecorization\n",
    "文本向量化是将文本数据转换为数值形式的过程，以便计算机能够理解和处理。在自然语言处理中，文本向量化通常包括以下几种方法：\n",
    "1. 稀疏表达：TDM（Term-Document Matrix）\n",
    "    - Bag of Words（词袋模型）：将文本表示为词汇表中单词的出现次数。\n",
    "    - TF-IDF（Term Frequency-Inverse Document Frequency）：将文本表示为单词的TF-IDF值，以衡量单词在文档中的重要性。\n",
    "2. 密集表达：Word Embeddings\n",
    "    - Word2Vec：将单词映射到低维空间的向量表示，以捕获单词之间的语义关系。\n",
    "    - GloVe（Global Vectors for Word Representation）：基于全局词频统计的词向量。\n",
    "    - FastText：Facebook提出的一种基于字符级n-gram的词向量方法。\n",
    "    - BERT（Bidirectional Encoder Representations from Transformers）：基于Transformer的预训练模型，提供了上下文相关的词向量。\n",
    "\n",
    "对于深度学习模型，通常使用词嵌入（Word Embeddings）来表示文本数据。在PyTorch中，可以使用`torch.nn.Embedding`层来加载预训练的词向量，或在训练过程中学习词向量。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "294f6865eddee626"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Word2Vec词向量\n",
    "Word2Vec是一种常用的词向量表示方法，它通过训练神经网络模型来学习单词的分布式表示。Word2Vec模型通常有两种架构：\n",
    "- **Skip-gram**：通过给定中心词预测上下文词。\n",
    "- **CBOW（Continuous Bag of Words）**：通过给定上下文词预测中心词。\n",
    "\n",
    "参数说明：\n",
    "- `tokenized_texts`：分词后的文本，作为训练数据输入。\n",
    "- `sg=1`：使用Skip-Gram模型。如果设置为`0`，则使用CBOW模型。\n",
    "- `min_count=1`：词语出现的最小次数。此参数确保了只有至少出现一次的词语才会被纳入训练。\n",
    "- `window=3`：当前词与预测词在一个句子中的最大距离。\n",
    "- `vector_size=100`：特征向量的维度大小。\n",
    "\n",
    "Word2Vec模型的训练过程通常使用负采样（Negative Sampling）或层次Softmax（Hierarchical Softmax）来提高训练效率。Word2Vec模型的输出是每个单词的词向量，可以用于后续的文本分类、聚类、相似度计算等任务。\n",
    "\n",
    "输出的数据格式为：[num_words, embedding_dim]，其中`num_words`是词汇表中单词的数量，`embedding_dim`是词向量的维度。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc7b788f1ae27a49"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-5.3623e-04,  2.3643e-04,  5.1033e-03,  ..., -7.0416e-03,\n",
      "          9.0146e-04,  6.3925e-03],\n",
      "        [-8.6197e-03,  3.6657e-03,  5.1899e-03,  ..., -2.3915e-03,\n",
      "         -9.5101e-03,  4.5059e-03],\n",
      "        [ 9.4564e-05,  3.0773e-03, -6.8126e-03,  ...,  5.1259e-04,\n",
      "          8.2131e-03, -7.0190e-03],\n",
      "        ...,\n",
      "        [ 5.0281e-03, -3.6851e-03,  3.4227e-03,  ...,  3.0911e-04,\n",
      "          7.6837e-03,  1.7421e-03],\n",
      "        [-9.0065e-03,  5.3365e-03,  3.7596e-03,  ...,  7.5535e-03,\n",
      "          3.9018e-03,  7.7782e-03],\n",
      "        [ 4.2228e-03,  1.7857e-04,  4.7847e-03,  ...,  2.8274e-03,\n",
      "         -1.6949e-03,  8.0678e-04]])\n"
     ]
    }
   ],
   "source": [
    "# word2vec词向量\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# 定义Word2Vec模型，使用tensor储存词向量\n",
    "def text_to_word2vec(texts):\n",
    "    # vector_size表示词向量的维度，window表示上下文窗口大小，min_count表示最小词频，workers表示线程数\n",
    "    w2v_model = Word2Vec(sentences=texts, vector_size=100, window=5, min_count=1, workers=-1, sg=1)\n",
    "    word_vectors = w2v_model.wv\n",
    "    return word_vectors\n",
    "\n",
    "train_word2vec = text_to_word2vec(train_text_clean)\n",
    "test_word2vec = text_to_word2vec(test_text_clean)\n",
    "\n",
    "# 使用tensor储存词向量\n",
    "train_word2vec_tensor = torch.tensor([train_word2vec[token] for token in train_word2vec.key_to_index]) # [85, 100]: 85个词，每个词的词向量维度为100\n",
    "test_word2vec_tensor = torch.tensor([test_word2vec[token] for token in test_word2vec.key_to_index]) # [77,100]: 77个词，每个词的词向量维度为100\n",
    "\n",
    "# 打印词嵌入\n",
    "print(train_word2vec_tensor)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T20:33:21.997215Z",
     "start_time": "2024-03-31T20:33:21.915694Z"
    }
   },
   "id": "54eb8c5dd317e3f2",
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### GloVe词向量\n",
    "GloVe是一种基于全局词频统计的词向量方法，它通过对词共现矩阵进行奇异值分解（SVD）来学习词向量。GloVe词向量通常在大型语料库上进行预训练，可以直接使用预训练的GloVe词向量，也可以在训练过程中学习词向量。\n",
    "\n",
    "输出的数据格式为：[num_samples, embedding_dim]，其中`num_samples`是样本数量,即原数据的文档数量，`embedding_dim`是词向量的维度。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "793a5e18b684adab"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1028,  0.1800,  0.2473,  ..., -0.1776,  0.2904,  0.1504],\n",
      "        [ 0.0249,  0.2207,  0.2789,  ..., -0.3625,  0.2532,  0.0806],\n",
      "        [-0.1845,  0.2870,  0.3115,  ..., -0.2843,  0.2605,  0.2389],\n",
      "        ...,\n",
      "        [ 0.0226,  0.1057,  0.4098,  ..., -0.1401,  0.3588,  0.1761],\n",
      "        [ 0.0239,  0.1033,  0.2309,  ..., -0.1844,  0.2863,  0.0968],\n",
      "        [-0.0267,  0.1591,  0.2689,  ..., -0.1677,  0.2733,  0.0952]])\n"
     ]
    }
   ],
   "source": [
    "# GloVe词向量，6B表示使用6亿词的预训练词向量，dim表示词向量的维度\n",
    "glove = GloVe(name='6B', dim=100)\n",
    "\n",
    "# 定义词嵌入函数，将文本转换为词向量\n",
    "def text_to_embedding(texts, glove):\n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        tokens = text.split() # 分词\n",
    "        embedding = [glove[token] for token in tokens if token in glove.stoi] # 获取词向量\n",
    "        \n",
    "        if len(embedding) > 0:\n",
    "            embedding = torch.stack(embedding).mean(0)  # 取平均得到整个句子的嵌入表示\n",
    "        else:\n",
    "            # 如果句子中的所有词都不在词向量中，则使用全零向量表示\n",
    "            embedding = torch.zeros(glove.vectors.shape[1])\n",
    "        embeddings.append(embedding)\n",
    "    return torch.stack(embeddings)\n",
    "\n",
    "train_embeddings = text_to_embedding(train_text_clean, glove) # [1000, 100]: 1000个样本，每个样本的词向量维度为100\n",
    "test_embeddings = text_to_embedding(test_text_clean, glove) # [500, 100]: 500个样本，每个样本的词向量维度为100\n",
    "\n",
    "# 打印词嵌入\n",
    "print(train_embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T20:30:53.489062Z",
     "start_time": "2024-03-31T20:30:52.884912Z"
    }
   },
   "id": "d3e28a3f20e23823",
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3f7734b7e3537dc9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## nn.Embedding\n",
    "在PyTorch中，可以使用`torch.nn.Embedding`层来加载预训练的词向量，或在训练过程中学习词向量。`torch.nn.Embedding`层的输入是一个整数张量，表示单词的索引，输出是对应的词向量。\n",
    "\n",
    "可以将文本数据直接转换为整数序列，并将这些整数序列作为模型的输入，而无需手动进行词嵌入。\n",
    "\n",
    "1. 分词和预处理：对原始训练集进行分词和预处理，包括去除标点符号、停用词等，以及将文本转换为小写形式。\n",
    "\n",
    "2. 构建词汇表：根据预处理后的训练集构建词汇表，将每个单词映射到一个唯一的整数。\n",
    "\n",
    "3. 将文本转换为整数序列：将预处理后的文本数据转换为整数序列，其中每个单词都用其在词汇表中的整数表示。可以使用Python的字典或者torchtext库等工具来实现这一步骤。\n",
    "\n",
    "4. 准备数据集：将转换后的整数序列组织成批次，并根据需要填充或截断成相同长度，以便输入到模型中。\n",
    "\n",
    "5. 训练模型：将整数序列的批次作为模型的输入，直接进行训练。\n",
    "\n",
    "通过这种方式，您可以直接将原始文本数据转换为整数序列，并将其传递给模型进行训练和测试，无需手动进行词嵌入。这种方法简化了数据准备过程，并使模型的使用更加方便。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5bb15710fc31fc4"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 222, 1061, 1845,  176,  723, 5177, 1200,  280, 7396,  184, 4060, 2003,\n",
      "        2815,   33,  951,  604,   88,    3,  262, 1846, 7397,    1,   24,   17,\n",
      "        2179,   93, 2816,  394,  370,  285, 1847,   30,  481,   46,  605,   30,\n",
      "         620,  496,   35,  164,  952,  584, 5178,   12,  903, 1012,  518, 2180,\n",
      "         332,  861,  518, 3339, 3340,  260, 2817, 2181, 7398, 4061,  621, 1695,\n",
      "         142,  394,  585, 2818, 1013, 7399,    1,  827,   43, 1061, 1845,  724,\n",
      "          41,  413,  262, 4062,  142,  467,   11,   85,  152,    4, 3341, 2467,\n",
      "        7400,  158,   28, 1696,  468,  142,  467,  456, 5179, 2816,  265, 7401,\n",
      "        3342, 4063, 1014,    6,   53,  165,  348, 2468,  142,   11, 5180,    1,\n",
      "          69, 5181,  414,   76,  142,   79,    3,   79, 2004,  568, 1201,   22,\n",
      "         103,   79, 4062,  371,  674, 1061, 1845,    6,    3,   30, 1697, 2819,\n",
      "        4064, 2469, 1113, 2816,  265,    3,   17,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0])\n",
      "tensor([   31,  1006,  1007,  1166,    71,  1006,  1007,     2,   187,   363,\n",
      "         4953,    33,     4,     6,   187,  1006,  1007,   543,    59,  1109,\n",
      "           86,   303,   282,  2092,   106,  2154,   225,  1330,  1586,   910,\n",
      "         1301,  1142,     7,  2375,  1006,  1007,   870,   140,    12,   543,\n",
      "            6,  1006,  1007,   187,  1028,  4838,   223,     4,   473,     7,\n",
      "          447,  1006,  1007,   463,   313,    59,  1109,   586,   519,   518,\n",
      "         6611,   713,   157,     7,   174, 13740,   208,  2186,    46,   138,\n",
      "         1726,   697,   210,   801,     8,   461,   340,    20,  1185,  1530,\n",
      "         5941,   340,    22,   729,     8,  5941,  4076,    78,   317,   282,\n",
      "          440,   936,     8,   368,   192,   186,  4472,   979,   464,    43,\n",
      "          133,     7,   192,    27,  5579,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch import LongTensor\n",
    "\n",
    "# 定义填充函数\n",
    "def pad_sequences(sequences, padding_value=0):\n",
    "    return pad_sequence([LongTensor(seq) for seq in sequences], batch_first=True, padding_value=padding_value)\n",
    "\n",
    "# 修改你的函数，添加填充步骤\n",
    "def text_to_int_sequence(texts, vocab):\n",
    "    int_sequences = []\n",
    "    for text in texts:\n",
    "        int_sequence = [vocab[word] for word in text.split() if word in vocab]\n",
    "        int_sequences.append(torch.LongTensor(int_sequence))  # 转换为长整型\n",
    "    return pad_sequences(int_sequences)\n",
    "\n",
    "# 重新转换文本为整数序列，并进行填充\n",
    "train_int_sequences = text_to_int_sequence(train_text_clean, vocab)\n",
    "test_int_sequences = text_to_int_sequence(test_text_clean, vocab)\n",
    "\n",
    "# 打印转换和填充后的整数序列示例\n",
    "print(train_int_sequences[0])\n",
    "print(test_int_sequences[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T20:58:10.861627Z",
     "start_time": "2024-03-31T20:58:10.811121Z"
    }
   },
   "id": "13a8b739d89c06b1",
   "execution_count": 47
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "bb5b7cde1f5228f1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
