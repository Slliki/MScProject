{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Preprocessing for Deep Learning NLP   \n",
    "\n",
    "## 1. Introduction\n",
    "在深度学习应用于自然语言处理（NLP）的背景下，数据预处理是一个至关重要的步骤，它通常包括以下几个环节：\n",
    "\n",
    "1. **文本清洗（Text cleaning）**：\n",
    "   - **去除噪声**：例如去除HTML标签、特殊字符等。\n",
    "   - **规范化文本**：比如将所有文本转换为小写，以减少大小写造成的变体。\n",
    "   - **分词（Tokenization）**：这是NLP中的一个基本步骤，涉及将连续的文本字符串分解为单独的单词或符号。\n",
    "   - **去除停用词（Stop word removal）**：诸如“the”、“is”、“in”等单词通常对于理解文本的含义不是很有用，可以被移除。\n",
    "   - **词干提取（Stemming）/词形还原（Lemmatization）**：比如将“running”还原为“run”，有助于模型理解不同词形的共同含义。\n",
    "\n",
    "2. **构建词汇表（Vocabulary construction）**：\n",
    "   - **确定模型的词汇**：基于训练数据创建一个单词集合。这一步将word映射到整数index，以便模型能够处理。\n",
    "\n",
    "3. **词嵌入（Word embeddings）**：\n",
    "   - **单词向量化**：使用预训练的词向量（如GloVe、Word2Vec）或在训练过程中学习词向量。这一步将已经创建好的词汇表中的单词映射到密集的向量表示。\n",
    "   - 不同的词嵌入模型会学习到不同的语义信息，如词义、词性、语法关系等，有助于提高模型的性能。\n",
    "4. **序列填充（Padding）/截断（Truncation）**：\n",
    "   - **统一序列长度**：由于神经网络需要固定长度的输入，过长的序列可能需要截断，而过短的序列则需要通过填充额外的零或特定的占位符来扩展。\n",
    "\n",
    "5. **构建标签（Label construction）**：\n",
    "   - **准备监督学习标签**：对于有监督学习任务，比如分类或序列标注，需要准备相应的标签。\n",
    "\n",
    "6. **数据增强（Data augmentation）**：\n",
    "   - **扩展训练数据**：使用各种技术（如同义词替换、随机删除、句子重组）来创建更多的训练样本，有助于提高模型的鲁棒性和泛化能力。\n",
    "\n",
    "7. **分批处理（Batching）**：\n",
    "   - **准备用于训练的数据批次**：组织数据以便以批次的形式进行有效训练。\n",
    "\n",
    "根据特定的任务和模型架构，预处理的具体步骤和实现可能会有所不同。例如，对于Transformer模型，通常会添加特殊的起始（[CLS]）和分隔（[SEP]）标记。而对于句子对任务，则可能需要确保两个句子合并后的长度不超过模型的最大序列长度。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a958d0001da1fbae"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import spacy\n",
    "from datasets import load_dataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.vocab import GloVe\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.vocab import vocab\n",
    "from torch.nn import Embedding\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-03T13:28:32.687809Z",
     "start_time": "2024-04-03T13:28:30.549453Z"
    }
   },
   "id": "e46f0bb5d640b501",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Text Cleaning\n",
    "spaCy是一个流行的自然语言处理库，提供了许多文本处理功能，包括分词、词形还原、命名实体识别等。在本节中，我们将使用spaCy来清理文本数据。\n",
    "\n",
    "spaCy提供多种不同类型和大小的预训练模型，适用于多种语言。这些模型包括：\n",
    "\n",
    "1. **核心模型（Core Models）**：\n",
    "   - 包含了词性标注、句法分析、命名实体识别等功能。\n",
    "   - 对于英语，提供了不同大小的模型，如`en_core_web_sm`（小型）、`en_core_web_md`（中型）、`en_core_web_lg`（大型），还有`en_core_web_trf`（基于transformer的模型）。\n",
    "\n",
    "2. **多语种和语言特定模型**：\n",
    "   - spaCy提供了对多种语言的支持，如西班牙语、法语、德语、俄语等。\n",
    "   - 这些模型有不同的大小版本，一般也遵循`xx_core_web_sm`、`xx_core_web_md`、`xx_core_web_lg`的命名规范（`xx`是语言代码）。\n",
    "\n",
    "3. **Transformer模型**：\n",
    "   - 如`en_core_web_trf`，这类模型基于预训练的Transformer模型，如BERT或RoBERTa，它们通常提供更优的性能，尤其在理解上下文和复杂语义方面。\n",
    "\n",
    "除了核心模型，spaCy社区还贡献了额外的模型和管道，这些可以通过spaCy官方网站和spaCy Universe来发现。这些额外的资源可能包括特定于某个领域的模型或用于特定任务的工具，比如情感分析、文本分类等。\n",
    "\n",
    "模型的选择依赖于具体的任务和所需语言。一般而言，小型模型下载快、占用空间少，适合快速原型设计或当环境资源有限时使用；大型模型和基于Transformer的模型提供更高的精度和更好的性能，但下载和加载时间更长，需要更多的资源。\n",
    "\n",
    "可以使用spaCy的命令行工具或在Python代码中使用`spacy.cli.download`函数来下载这些模型。使用时，需要确保选择的模型与你使用的spaCy版本兼容。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a593cdea660907e3"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple is looking at buying U.K. startup for $1 billion. The foxes walk into the box. The dog was barking.\n",
      "\n",
      "Sentences:\n",
      "Apple is looking at buying U.K. startup for $1 billion.\n",
      "The foxes walk into the box.\n",
      "The dog was barking.\n",
      "\n",
      "Named Entities:\n",
      "Apple ORG\n",
      "U.K. GPE\n",
      "$1 billion MONEY\n",
      "\n",
      "Tokens and their POS tags:\n",
      "Apple PROPN\n",
      "is AUX\n",
      "looking VERB\n",
      "at ADP\n",
      "buying VERB\n",
      "U.K. PROPN\n",
      "startup NOUN\n",
      "for ADP\n",
      "$ SYM\n",
      "1 NUM\n",
      "billion NUM\n",
      ". PUNCT\n",
      "The DET\n",
      "foxes NOUN\n",
      "walk VERB\n",
      "into ADP\n",
      "the DET\n",
      "box NOUN\n",
      ". PUNCT\n",
      "The DET\n",
      "dog NOUN\n",
      "was AUX\n",
      "barking VERB\n",
      ". PUNCT\n",
      "\n",
      "Noun chunks:\n",
      "Apple\n",
      "U.K.\n",
      "The foxes\n",
      "the box\n",
      "The dog\n",
      "\n",
      "Lemmatized words:\n",
      "Apple Apple\n",
      "is be\n",
      "looking look\n",
      "at at\n",
      "buying buy\n",
      "U.K. U.K.\n",
      "startup startup\n",
      "for for\n",
      "$ $\n",
      "1 1\n",
      "billion billion\n",
      ". .\n",
      "The the\n",
      "foxes fox\n",
      "walk walk\n",
      "into into\n",
      "the the\n",
      "box box\n",
      ". .\n",
      "The the\n",
      "dog dog\n",
      "was be\n",
      "barking bark\n",
      ". .\n"
     ]
    }
   ],
   "source": [
    "# 加载英语模型\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 处理文本\n",
    "text = \"Apple is looking at buying U.K. startup for $1 billion. The foxes walk into the box. The dog was barking.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# 打印文本\n",
    "print(doc.text)\n",
    "\n",
    "# 遍历文档中的句子\n",
    "print('\\nSentences:')\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)\n",
    "    \n",
    "# 遍历文档中的命名实体\n",
    "print('\\nNamed Entities:')\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "\n",
    "# 打印每个token的文本和词性标注\n",
    "print('\\nTokens and their POS tags:')\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)\n",
    "\n",
    "# 名词短语\n",
    "print('\\nNoun chunks:')\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text)\n",
    "    \n",
    "# 词形还原\n",
    "print('\\nLemmatized words:')\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T20:18:11.655185Z",
     "start_time": "2024-03-31T20:18:11.210768Z"
    }
   },
   "id": "3448b6a93efa0a3f",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 加载本地的imdb数据集，arrow格式\n",
    "data_files = {\n",
    "    \"train\": './data_cache/imdb/plain_text/0.0.0/e6281661ce1c48d982bc483cf8a173c1bbeb5d31/imdb-train.arrow',\n",
    "    \"test\": \"./data_cache/imdb/plain_text/0.0.0/e6281661ce1c48d982bc483cf8a173c1bbeb5d31/imdb-test.arrow\",\n",
    "}\n",
    "dataset = load_dataset('arrow', data_files=data_files)\n",
    "\n",
    "# split the dataset,only use 1000 samples for demo\n",
    "train_dataset = dataset['train'].select(range(1000)) # select 函数用于选择数据集的子集，这里选择前1000个样本\n",
    "test_dataset = dataset['test'].select(range(500))\n",
    "\n",
    "# 获取文本和标签\n",
    "train_text = [sample[\"text\"] for sample in train_dataset]\n",
    "train_label = [sample[\"label\"] for sample in train_dataset]\n",
    "test_text = [sample[\"text\"] for sample in test_dataset]\n",
    "test_label =  [sample[\"label\"] for sample in test_dataset]\n",
    "\n",
    "# 将标签转换为Tensor\n",
    "train_labels = torch.tensor(train_label, dtype=torch.long)\n",
    "test_labels = torch.tensor(test_label, dtype=torch.long)\n",
    "\n",
    "# 对于text数据，还需要预处理操作后再转换为Tensor"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T13:59:45.091725Z",
     "start_time": "2024-04-02T13:59:44.199620Z"
    }
   },
   "id": "af24fc61ad41928f",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1 spaCy文本预处理\n",
    "一般来说，spaCy的文本预处理包括以下几个步骤：\n",
    "- 加载spaCy模型\n",
    "- 定义文本预处理函数：\n",
    "    - 使用`nlp.pipe`方法高效地处理大量文本,禁用不需要的组件以提高效率。\n",
    "    - 去除停用词、标点和空格，并进行词形还原或其他处理。\n",
    "    - 将处理后的文本重新组合成字符串。\n",
    "- 对train_text和test_text进行预处理。\n",
    "\n",
    "### nlp.pipe\n",
    "nlp.pipe有着多种用法，其中最常见的用法是处理大量文本数据。在处理大量文本数据时，使用nlp.pipe方法可以提高处理效率，因为它会自动进行批处理，并在后台并行处理文本数据。此外，可以通过设置`disable`参数来禁用不需要的组件，以进一步提高处理速度。\n",
    "\n",
    "向管道中添加组件\n",
    "```\n",
    "if not nlp.has_pipe(\"sentencizer\"):\n",
    "    nlp.add_pipe(\"sentencizer\")\n",
    "```\n",
    "\n",
    "这里使用自定义预处理函数，对于每个文档，进行分词等步骤，返回结果train_text_clean和test_text_clean。格式为：[num_samples]，每个样本是一个列表，包含对应文档中的单词。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9d85130b1c7842f3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 加载Spacy模型\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# 检查nlp对象中是否已经有'sentencizer'组件，如果没有，则添加\n",
    "if not nlp.has_pipe(\"sentencizer\"):\n",
    "    nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "# 定义文本预处理函数\n",
    "# 该函数使用Spacy进行文本预处理，包括分词、词形还原和去除停用词\n",
    "# 使用Spacy的管道（pipe）机制，可以高效地处理大量文本，优于直接使用nlp(text)方法；可选择禁用不需要的组件以提高效率\n",
    "def preprocess_text(texts):\n",
    "    tokenized_texts = []  # 存储处理后的文档列表\n",
    "\n",
    "    for doc in nlp.pipe(texts, disable=[\"parser\", \"ner\"]):\n",
    "        # 去除停用词、标点和空格，并进行词形还原\n",
    "        doc_tokens = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "        tokenized_texts.append(doc_tokens)  # 将处理后的文档添加到结果中\n",
    "    return tokenized_texts\n",
    "\n",
    "train_text_clean = preprocess_text(train_text)\n",
    "test_text_clean = preprocess_text(test_text)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T14:00:05.821922Z",
     "start_time": "2024-04-02T13:59:47.977510Z"
    }
   },
   "id": "e2f90318b7ef9303",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rent', 'curious', 'YELLOW', 'video', 'store', 'controversy', 'surround', 'release', '1967', 'hear', 'seize', 'U.S.', 'custom', 'try', 'enter', 'country', 'fan', 'film', 'consider', 'controversial', 'myself.<br', '/><br', '/>The', 'plot', 'center', 'young', 'swedish', 'drama', 'student', 'name', 'Lena', 'want', 'learn', 'life', 'particular', 'want', 'focus', 'attention', 'make', 'sort', 'documentary', 'average', 'swede', 'think', 'certain', 'political', 'issue', 'Vietnam', 'War', 'race', 'issue', 'United', 'States', 'ask', 'politician', 'ordinary', 'denizen', 'Stockholm', 'opinion', 'politic', 'sex', 'drama', 'teacher', 'classmate', 'marry', 'men.<br', '/><br', '/>What', 'kill', 'CURIOUS', 'YELLOW', '40', 'year', 'ago', 'consider', 'pornographic', 'sex', 'nudity', 'scene', 'far', 'shoot', 'like', 'cheaply', 'porno', 'countryman', 'mind', 'find', 'shocking', 'reality', 'sex', 'nudity', 'major', 'staple', 'swedish', 'cinema', 'Ingmar', 'Bergman', 'arguably', 'answer', 'good', 'old', 'boy', 'John', 'Ford', 'sex', 'scene', 'films.<br', '/><br', '/>i', 'commend', 'filmmaker', 'fact', 'sex', 'show', 'film', 'show', 'artistic', 'purpose', 'shock', 'people', 'money', 'show', 'pornographic', 'theater', 'America', 'CURIOUS', 'YELLOW', 'good', 'film', 'want', 'study', 'meat', 'potato', 'pun', 'intend', 'swedish', 'cinema', 'film', 'plot']\n"
     ]
    },
    {
     "data": {
      "text/plain": "1000"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 打印预处理后的文本\n",
    "print(train_text_clean[0])\n",
    "len(train_text_clean) # 11196 表示训练集1000个样本，一共划分为11196个句子"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T13:30:58.626078Z",
     "start_time": "2024-04-02T13:30:58.606545Z"
    }
   },
   "id": "67669db9eee8868",
   "execution_count": 86
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.Vecorization\n",
    "文本向量化是将文本数据转换为数值形式的过程，以便计算机能够理解和处理。在自然语言处理中，文本向量化通常包括以下几种方法：\n",
    "1. 稀疏表达：TDM（Term-Document Matrix）\n",
    "    - Bag of Words（词袋模型）：将文本表示为词汇表中单词的出现次数。\n",
    "    - TF-IDF（Term Frequency-Inverse Document Frequency）：将文本表示为单词的TF-IDF值，以衡量单词在文档中的重要性。\n",
    "2. 密集表达：Word Embeddings\n",
    "    - Word2Vec：将单词映射到低维空间的向量表示，以捕获单词之间的语义关系。\n",
    "    - GloVe（Global Vectors for Word Representation）：基于全局词频统计的词向量。\n",
    "    - FastText：Facebook提出的一种基于字符级n-gram的词向量方法。\n",
    "    - BERT（Bidirectional Encoder Representations from Transformers）：基于Transformer的预训练模型，提供了上下文相关的词向量。\n",
    "\n",
    "对于深度学习模型，通常使用词嵌入（Word Embeddings）来表示文本数据。在PyTorch中，可以使用`torch.nn.Embedding`层来加载预训练的词向量，或在训练过程中学习词向量。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "294f6865eddee626"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1 Word2Vec词向量\n",
    "Word2Vec是一种常用的词向量表示方法，它通过训练神经网络模型来学习单词的分布式表示。Word2Vec模型通常有两种架构：\n",
    "- **Skip-gram**：通过给定中心词预测上下文词。\n",
    "- **CBOW（Continuous Bag of Words）**：通过给定上下文词预测中心词。\n",
    "\n",
    "参数说明：\n",
    "- `tokenized_texts`：分词后的文本，作为训练数据输入。\n",
    "- `sg=1`：使用Skip-Gram模型。如果设置为`0`，则使用CBOW模型。\n",
    "- `min_count=1`：词语出现的最小次数。此参数确保了只有至少出现一次的词语才会被纳入训练。\n",
    "- `window=3`：当前词与预测词在一个句子中的最大距离。\n",
    "- `vector_size=100`：特征向量的维度大小。\n",
    "\n",
    "Word2Vec模型的训练过程通常使用负采样（Negative Sampling）或层次Softmax（Hierarchical Softmax）来提高训练效率。Word2Vec模型的输出是每个单词的词向量，可以用于后续的文本分类、聚类、相似度计算等任务。\n",
    "\n",
    "输出的数据格式为：[num_words, embedding_dim]，其中`num_words`是词汇表中单词的数量，`embedding_dim`是词向量的维度。\n",
    "\n",
    "需要注意的是，Word2Vec是在单词级别上建立词向量的，这意味着它不会考虑到句子或文档的上下文信息。因此，如果你的任务是在文档级别进行情感分析（例如，给整个IMDB评论打标签），那么你可能需要采取一些策略来将单词级别的词向量转化为文档级别的表示。  一种常见的策略是对一个文档中所有单词的词向量取平均，得到一个代表整个文档的向量。然而，这种方法忽略了词序信息，可能会影响模型的性能。\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc7b788f1ae27a49"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# 使用全部文本数据训练Word2Vec模型\n",
    "all_text_clean = train_text_clean + test_text_clean\n",
    "\n",
    "# 训练Word2Vec模型，设置每个单词的向量维度为100\n",
    "word2vec_model = Word2Vec(sentences=all_text_clean, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# 获取词向量\n",
    "word_vectors = word2vec_model.wv"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T14:00:06.581118Z",
     "start_time": "2024-04-02T14:00:05.823915Z"
    }
   },
   "id": "393fa5c850b50fcd",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.2697045e-02  2.2419440e-02  6.9050072e-03 -3.5571195e-03\n",
      " -4.1548400e-03 -2.3723034e-02  1.2074275e-02  5.2477136e-02\n",
      " -3.1283759e-02 -1.8286215e-02 -9.5697008e-03 -3.8519885e-02\n",
      " -4.6982719e-03  2.4499275e-02  4.6533960e-04 -2.0590622e-02\n",
      "  1.8333714e-02 -1.9812107e-02  1.7628982e-03 -3.9659768e-02\n",
      "  2.1466682e-02  7.1030236e-03  1.0088837e-02 -1.4633660e-02\n",
      " -3.9604022e-03  3.1353601e-03 -1.6019225e-02  7.7307614e-04\n",
      " -3.6090154e-02  1.1287913e-02  3.4830797e-02  5.3286948e-03\n",
      "  6.9953809e-03 -1.6698997e-02 -5.9348703e-03  1.5556945e-02\n",
      " -3.9932742e-03 -4.5184679e-03 -4.0730834e-03 -4.6856493e-02\n",
      "  7.2372574e-03 -1.7406678e-02 -1.2130932e-02 -1.3593365e-02\n",
      "  6.4295521e-03 -1.5891004e-02 -2.8445681e-03 -7.9976469e-03\n",
      "  9.6949833e-03  1.2018349e-02 -1.7873716e-03 -2.9803008e-02\n",
      "  6.6019587e-05 -7.7488702e-03  9.5908111e-04  5.2998197e-04\n",
      "  1.1600737e-02 -3.6689639e-03 -1.8262729e-02 -4.6802030e-04\n",
      "  1.9701929e-03 -1.1213666e-02 -9.2534320e-03 -9.6501652e-03\n",
      " -1.8676648e-02  2.9099742e-02  8.7705944e-03  1.1787235e-02\n",
      " -1.9767921e-02  4.7358166e-02 -1.8953206e-02  1.7754696e-02\n",
      "  3.0039018e-02 -6.6984110e-03  2.3952061e-02  1.9767219e-02\n",
      "  1.1157547e-02  6.5743122e-03 -2.2687089e-02  1.9493225e-03\n",
      " -9.8799188e-03 -7.7644419e-03 -2.4145683e-02  2.3488145e-02\n",
      "  1.6146367e-02 -2.2846919e-03 -3.5624541e-03  6.4867819e-03\n",
      "  3.1366132e-02  1.8899674e-02  3.2087877e-02  9.1085685e-03\n",
      "  5.5233580e-03 -1.2077814e-02  3.0313119e-02  3.0866515e-02\n",
      "  2.3902280e-02 -9.4159609e-03  1.1540258e-02 -7.7323774e-03]\n"
     ]
    }
   ],
   "source": [
    "print(word_vectors['apple']) # 获取单词'apple'的词向量"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T13:31:05.852072Z",
     "start_time": "2024-04-02T13:31:05.843072Z"
    }
   },
   "id": "dd06af88428c5e3",
   "execution_count": 88
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2 Padding and Truncation\n",
    "建立好word2vec模型后，我们需要将文本数据转换为词向量表示，并进行填充或截断，以便输入到模型中。在PyTorch中，可以使用`torch.nn.utils.rnn.pad_sequence`函数来对序列进行填充。\n",
    "\n",
    "下面代码会遍历每个文本，将文本中的每个词转换为词向量，然后将词向量序列填充或截断到指定的长度。\n",
    "如果词不在训练好的词向量模型中，那么在当前的实现中，这个词的词向量将会是一个全零向量，因为我们使用`torch.zeros`初始化了词向量。\n",
    "\n",
    "如果你希望对不在词向量模型中的词进行特殊处理，你可以选择以下几种策略：\n",
    "\n",
    "1. 使用一个特定的向量来表示所有未知的词，比如随机向量。这可以通过在初始化向量时使用`torch.randn`代替`torch.zeros`来实现。\n",
    "\n",
    "2. 使用一个特定的向量来表示所有未知的词，比如全一向量。这可以通过在初始化向量时使用`torch.ones`代替`torch.zeros`来实现。\n",
    "\n",
    "3. 使用预训练词向量模型中的特殊向量，如\"<UNK>\"，来表示所有未知的词。这需要预训练词向量模型中包含这样的特殊向量。\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e261adc9d434e2e9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def document_vector(word2vec_model, doc):\n",
    "    # 初始化一个全零向量\n",
    "    doc_vector = np.zeros(word2vec_model.vector_size)\n",
    "    num_words = 0\n",
    "    for word in doc:\n",
    "        # 如果单词在词汇表中，则加上其词向量\n",
    "        if word in word2vec_model.wv:\n",
    "            doc_vector += word2vec_model.wv[word]\n",
    "            num_words += 1\n",
    "    if num_words != 0:\n",
    "        # 除以文档中词汇的总数，得到平均词向量\n",
    "        doc_vector /= num_words\n",
    "    return doc_vector\n",
    "\n",
    "# 使用函数计算文档的平均词向量\n",
    "train_doc_vectors = [document_vector(word2vec_model, doc) for doc in train_text_clean]\n",
    "test_doc_vectors = [document_vector(word2vec_model, doc) for doc in test_text_clean]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T13:31:08.927988Z",
     "start_time": "2024-04-02T13:31:08.650724Z"
    }
   },
   "id": "f3cd428023ce360f",
   "execution_count": 89
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "1000"
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_doc_vectors)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T13:31:10.580677Z",
     "start_time": "2024-04-02T13:31:10.572701Z"
    }
   },
   "id": "ff7f90551fc70df4",
   "execution_count": 90
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# 定义文本-序列转换函数，该函数使用训练好的word2vec模型将文本转换为词向量序列\n",
    "# 并使用pad_sequence函数对序列进行填充或截断\n",
    "def text_to_sequence(texts, word_vectors, max_len):\n",
    "    sequences = []\n",
    "    for text in texts:\n",
    "        seq = torch.zeros((len(text), word_vectors.vector_size))\n",
    "        for i, word in enumerate(text):\n",
    "            if i >= max_len: # 超过最大长度则跳出循环\n",
    "                break\n",
    "            if word in word_vectors: # 如果词在词向量中，则使用对应的词向量，否则使用全零向量\n",
    "                seq[i] = torch.tensor(word_vectors[word])\n",
    "                \n",
    "        seq = seq[:max_len]  # 截断序列\n",
    "        sequences.append(seq)\n",
    "    return pad_sequence(sequences, batch_first=True, padding_value=0) # padding_value=0表示填充值为0;\n",
    "\n",
    "max_len = 50  # 假定的最大序列长度\n",
    "train_sequences = text_to_sequence(train_text_clean, word_vectors, max_len)\n",
    "test_sequences = text_to_sequence(test_text_clean, word_vectors, max_len)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T14:00:43.966457Z",
     "start_time": "2024-04-02T14:00:43.077200Z"
    }
   },
   "id": "807df18b16e805ae",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1000, 50, 100])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sequences.shape # torch.Size([1000, 50, 100])：1000个样本，每个样本的词向量序列长度为50，每个词向量的维度为100"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T14:00:48.952479Z",
     "start_time": "2024-04-02T14:00:48.938469Z"
    }
   },
   "id": "6b54c59033e00307",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3 DataLoader\n",
    "自定义一个`Dataset`类，用于加载文本数据和标签数据，并定义一个`DataLoader`类，用于生成数据批次。`Dataset`类需要实现`__len__`和`__getitem__`方法，`DataLoader`类则可以指定批次大小、是否打乱数据等参数。\n",
    "\n",
    "这一步最终得到train_dataloader和test_dataloader，可以直接用于模型的训练和测试。\n",
    "\n",
    "train_dataloader.dataset会返回一个Dataset对象，每个元素是一个文本序列和对应的标签。train_dataloader.dataset.labels可以获取标签数据，train_dataloader.dataset.sequences可以获取文本数据。\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f2562991df19db4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, sequences, labels):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.labels[idx]\n",
    "\n",
    "# 假设你已经有了对应的标签数据 train_labels 和 test_labels\n",
    "train_dataset = TextDataset(train_sequences, train_labels)\n",
    "test_dataset = TextDataset(test_sequences, test_labels)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T14:02:26.110133Z",
     "start_time": "2024-04-02T14:02:26.100133Z"
    }
   },
   "id": "e460819cdcc1ae29",
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4 Doc2Vec\n",
    "Doc2Vec是Word2Vec的扩展，它不仅学习单词的向量表示，还学习文档的向量表示。Doc2Vec模型通常有两种架构：\n",
    "- **PV-DM**：通过给定上下文词和文档向量预测中心词。\n",
    "- **PV-DBOW**：通过给定文档向量预测中心词。\n",
    "\n",
    "Doc2Vec模型的训练过程与Word2Vec类似，但需要额外的文档标签。在训练过程中，文档标签可以作为额外的输入，以帮助模型学习文档的向量表示。\n",
    "\n",
    "对于文档情感分析，如果使用word2vec模型，可以将文档中所有单词的词向量取平均，得到一个代表整个文档的向量。然而，这种方法忽略了词序信息，可能会影响模型的性能。\n",
    "因此，使用Doc2Vec模型可以更好地捕获文档的语义信息，提高模型的性能。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5660df32cce046c8"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "# 将文本数据转换为TaggedDocument对象\n",
    "all_text_clean_tagged = [TaggedDocument(doc, [i]) for i, doc in enumerate(all_text_clean)]\n",
    "\n",
    "# 训练Doc2Vec模型\n",
    "doc2vec_model = Doc2Vec(vector_size=100, window=5, min_count=1, workers=4, epochs=20)\n",
    "doc2vec_model.build_vocab(all_text_clean_tagged) # 构建词汇表\n",
    "doc2vec_model.train(all_text_clean_tagged, total_examples=doc2vec_model.corpus_count, epochs=doc2vec_model.epochs) # 训练模型\n",
    "\n",
    "# 获取文档向量,使用infer_vector方法，该方法可以对新文档进行向量化，输入为文档的分词列表，输出为文档的向量表示\n",
    "# 获取训练集和测试集的文档向量\n",
    "train_doc_vectors = [doc2vec_model.infer_vector(doc) for doc in train_text_clean]\n",
    "test_doc_vectors = [doc2vec_model.infer_vector(doc) for doc in test_text_clean]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T14:15:00.360493Z",
     "start_time": "2024-04-02T14:14:55.693075Z"
    }
   },
   "id": "3e15cd30f79bba1e",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(1000, 500)"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_doc_vectors), len(test_doc_vectors)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T14:15:06.829854Z",
     "start_time": "2024-04-02T14:15:06.815337Z"
    }
   },
   "id": "5ce65c92152e642c",
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### GloVe词向量\n",
    "GloVe是一种基于全局词频统计的词向量方法，它通过对词共现矩阵进行奇异值分解（SVD）来学习词向量。GloVe词向量通常在大型语料库上进行预训练，可以直接使用预训练的GloVe词向量，也可以在训练过程中学习词向量。\n",
    "\n",
    "输出的数据格式为：[num_samples, embedding_dim]，其中`num_samples`是样本数量,即原数据的文档数量，`embedding_dim`是词向量的维度。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "793a5e18b684adab"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[35], line 21\u001B[0m\n\u001B[0;32m     18\u001B[0m         embeddings\u001B[38;5;241m.\u001B[39mappend(embedding)\n\u001B[0;32m     19\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mstack(embeddings)\n\u001B[1;32m---> 21\u001B[0m train_embeddings \u001B[38;5;241m=\u001B[39m \u001B[43mtext_to_embedding\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_text_clean\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mglove\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;66;03m# [1000, 100]: 1000个样本，每个样本的词向量维度为100\u001B[39;00m\n\u001B[0;32m     22\u001B[0m test_embeddings \u001B[38;5;241m=\u001B[39m text_to_embedding(test_text_clean, glove) \u001B[38;5;66;03m# [500, 100]: 500个样本，每个样本的词向量维度为100\u001B[39;00m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;66;03m# 打印词嵌入\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[35], line 10\u001B[0m, in \u001B[0;36mtext_to_embedding\u001B[1;34m(texts, glove)\u001B[0m\n\u001B[0;32m      8\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m text \u001B[38;5;129;01min\u001B[39;00m texts:\n\u001B[1;32m---> 10\u001B[0m     tokens \u001B[38;5;241m=\u001B[39m \u001B[43mtext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m() \u001B[38;5;66;03m# 分词\u001B[39;00m\n\u001B[0;32m     11\u001B[0m     embedding \u001B[38;5;241m=\u001B[39m [glove[token] \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m tokens \u001B[38;5;28;01mif\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m glove\u001B[38;5;241m.\u001B[39mstoi] \u001B[38;5;66;03m# 获取词向量\u001B[39;00m\n\u001B[0;32m     13\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(embedding) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "from torchtext.vocab import GloVe\n",
    "\n",
    "# GloVe词向量，6B表示使用6亿词的预训练词向量，dim表示词向量的维度\n",
    "glove = GloVe(name='6B', dim=100)\n",
    "\n",
    "# 定义词嵌入函数，将文本转换为词向量\n",
    "def text_to_embedding(texts, glove):\n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        tokens = text.split() # 分词\n",
    "        embedding = [glove[token] for token in tokens if token in glove.stoi] # 获取词向量\n",
    "        \n",
    "        if len(embedding) > 0:\n",
    "            embedding = torch.stack(embedding).mean(0)  # 取平均得到整个句子的嵌入表示\n",
    "        else:\n",
    "            # 如果句子中的所有词都不在词向量中，则使用全零向量表示\n",
    "            embedding = torch.zeros(glove.vectors.shape[1])\n",
    "        embeddings.append(embedding)\n",
    "    return torch.stack(embeddings)\n",
    "\n",
    "train_embeddings = text_to_embedding(train_text_clean, glove) # [1000, 100]: 1000个样本，每个样本的词向量维度为100\n",
    "test_embeddings = text_to_embedding(test_text_clean, glove) # [500, 100]: 500个样本，每个样本的词向量维度为100\n",
    "\n",
    "# 打印词嵌入\n",
    "print(train_embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-02T12:39:31.597828Z",
     "start_time": "2024-04-02T12:39:31.201697Z"
    }
   },
   "id": "d3e28a3f20e23823",
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3f7734b7e3537dc9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## nn.Embedding\n",
    "在PyTorch中，可以使用`torch.nn.Embedding`层来加载预训练的词向量，或在训练过程中学习词向量。`torch.nn.Embedding`层的输入是一个整数张量，表示单词的索引，输出是对应的词向量。\n",
    "\n",
    "可以将文本数据直接转换为整数序列，并将这些整数序列作为模型的输入，而无需手动进行词嵌入。\n",
    "\n",
    "1. 分词和预处理：对原始训练集进行分词和预处理，包括去除标点符号、停用词等，以及将文本转换为小写形式。\n",
    "\n",
    "2. 构建词汇表：根据预处理后的训练集构建词汇表，将每个单词映射到一个唯一的整数。\n",
    "\n",
    "3. 将文本转换为整数序列：将预处理后的文本数据转换为整数序列，其中每个单词都用其在词汇表中的整数表示。可以使用Python的字典或者torchtext库等工具来实现这一步骤。\n",
    "\n",
    "4. 准备数据集：将转换后的整数序列组织成批次，并根据需要填充或截断成相同长度，以便输入到模型中。\n",
    "\n",
    "5. 训练模型：将整数序列的批次作为模型的输入，直接进行训练。\n",
    "\n",
    "通过这种方式，您可以直接将原始文本数据转换为整数序列，并将其传递给模型进行训练和测试，无需手动进行词嵌入。这种方法简化了数据准备过程，并使模型的使用更加方便。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5bb15710fc31fc4"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 222, 1061, 1845,  176,  723, 5177, 1200,  280, 7396,  184, 4060, 2003,\n",
      "        2815,   33,  951,  604,   88,    3,  262, 1846, 7397,    1,   24,   17,\n",
      "        2179,   93, 2816,  394,  370,  285, 1847,   30,  481,   46,  605,   30,\n",
      "         620,  496,   35,  164,  952,  584, 5178,   12,  903, 1012,  518, 2180,\n",
      "         332,  861,  518, 3339, 3340,  260, 2817, 2181, 7398, 4061,  621, 1695,\n",
      "         142,  394,  585, 2818, 1013, 7399,    1,  827,   43, 1061, 1845,  724,\n",
      "          41,  413,  262, 4062,  142,  467,   11,   85,  152,    4, 3341, 2467,\n",
      "        7400,  158,   28, 1696,  468,  142,  467,  456, 5179, 2816,  265, 7401,\n",
      "        3342, 4063, 1014,    6,   53,  165,  348, 2468,  142,   11, 5180,    1,\n",
      "          69, 5181,  414,   76,  142,   79,    3,   79, 2004,  568, 1201,   22,\n",
      "         103,   79, 4062,  371,  674, 1061, 1845,    6,    3,   30, 1697, 2819,\n",
      "        4064, 2469, 1113, 2816,  265,    3,   17,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0])\n",
      "tensor([   31,  1006,  1007,  1166,    71,  1006,  1007,     2,   187,   363,\n",
      "         4953,    33,     4,     6,   187,  1006,  1007,   543,    59,  1109,\n",
      "           86,   303,   282,  2092,   106,  2154,   225,  1330,  1586,   910,\n",
      "         1301,  1142,     7,  2375,  1006,  1007,   870,   140,    12,   543,\n",
      "            6,  1006,  1007,   187,  1028,  4838,   223,     4,   473,     7,\n",
      "          447,  1006,  1007,   463,   313,    59,  1109,   586,   519,   518,\n",
      "         6611,   713,   157,     7,   174, 13740,   208,  2186,    46,   138,\n",
      "         1726,   697,   210,   801,     8,   461,   340,    20,  1185,  1530,\n",
      "         5941,   340,    22,   729,     8,  5941,  4076,    78,   317,   282,\n",
      "          440,   936,     8,   368,   192,   186,  4472,   979,   464,    43,\n",
      "          133,     7,   192,    27,  5579,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch import LongTensor\n",
    "\n",
    "# 定义填充函数\n",
    "def pad_sequences(sequences, padding_value=0):\n",
    "    return pad_sequence([LongTensor(seq) for seq in sequences], batch_first=True, padding_value=padding_value)\n",
    "\n",
    "# 修改你的函数，添加填充步骤\n",
    "def text_to_int_sequence(texts, vocab):\n",
    "    int_sequences = []\n",
    "    for text in texts:\n",
    "        int_sequence = [vocab[word] for word in text.split() if word in vocab]\n",
    "        int_sequences.append(torch.LongTensor(int_sequence))  # 转换为长整型\n",
    "    return pad_sequences(int_sequences)\n",
    "\n",
    "# 重新转换文本为整数序列，并进行填充\n",
    "train_int_sequences = text_to_int_sequence(train_text_clean, vocab)\n",
    "test_int_sequences = text_to_int_sequence(test_text_clean, vocab)\n",
    "\n",
    "# 打印转换和填充后的整数序列示例\n",
    "print(train_int_sequences[0])\n",
    "print(test_int_sequences[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T20:58:10.861627Z",
     "start_time": "2024-03-31T20:58:10.811121Z"
    }
   },
   "id": "13a8b739d89c06b1",
   "execution_count": 47
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "bb5b7cde1f5228f1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
