# Worksheet for Reinfocement Learning
## Q1:
<img src="C:\Users\yhb\MscProject\AI&TA\AI\images\img_26.png">

这张图片包含了两个关于强化学习中的价值迭代算法的问题。下面是问题的翻译和解答：

**问题1：**
考虑图1中的网格世界。

(a) 我们是否需要等到算法收敛后才知道每个状态的效用值，还是已经知道某些状态的效用值？
(b) 假设我们将每个状态的效用初始化为0，然后执行一次价值迭代算法。每个状态的效用值是什么？

**答案：**

(a) 在网格世界中，+1 和 -1 的状态是终结状态，这些状态的效用值在一开始就已经知道，不需要等到算法收敛。这是因为这些状态的效用值是由它们本身的即时奖励决定的，不会随着价值迭代过程而改变。

(b) 如果我们初始化每个状态的效用为0，并且做一次迭代，那么每个非终结状态的效用值将是它所有可能动作的预期回报的最大值。对于每个非终结状态，这个值是它所有可能的下一个状态的效用值的加权和，加上任何即时奖励（在这个问题中即时奖励是-0.04）。对于终结状态，它们的效用值在一次迭代之后仍然是它们的即时奖励值。

我们可以通过应用贝尔曼方程来计算每个状态的效用。对于这个3x3的网格世界，我们将根据转移概率（例如，从某个状态出发，向上移动有0.8的概率成功，向左或向右有0.1的概率滑向那个方向），考虑即时奖励和预期的未来回报来计算每个状态的效用。

## Q2:
<img src="C:\Users\yhb\MscProject\AI&TA\AI\images\img_27.png">

这张图片包含了一个关于强化学习中的策略评估问题。下面是问题的翻译和解答：

**问题2：**
考虑表1中显示的3x3网格世界。转移模型如下：80%的时间，代理（agent）会按它选择的方向移动；其余时间，它会以10%的概率向预定方向的正右方或正左方移动（即，和前一个问题中的情形相同）。

左上角的r是一个奖励值。对于r的不同值，请说明政策结果。你不需要使用价值迭代或策略迭代，你可以仅用常识来解决。使用折扣回报，其中γ = 0.99。

(a) r = -3

**答案：**
在这个情境下，我们可以不使用算法来解释结果，而是依靠直觉和对强化学习基本原理的理解。

对于 r = -3 的情况，即每个非终结状态的即时奖励为-3。这意味着每次代理留在非终结状态中都会得到一个相对较大的负奖励。因此，代理的最优策略应该是尽快达到正奖励的状态，也就是右上角的+10状态，以此最小化接收负奖励的次数。即使代理有20%的概率不按计划移动，它应该选择那些使其尽快到达+10的路径。由于我们使用了折扣因子γ = 0.99，代理会更倾向于立即获得奖励而非未来的奖励。

(b) r = 3

如果 r = 3，这意味着每个非终结状态的即时奖励是+3。在这种情况下，由于代理在每一步都获得正奖励，它没有迅速到达终点的紧迫性。然而，考虑到折扣因子γ = 0.99，长远来看，最优策略仍然会倾向于尽快到达奖励为+10的状态，因为尽管每一步都有正奖励，+10的奖励在被折扣前是最大的单步奖励，而在折扣后，它的价值将超过多次累积的+3奖励。

因此，即使每一步的即时奖励是+3，代理的行为应该仍然是寻找一条路径，以最大化它的总折扣回报，这通常涉及尽可能快地到达+10的奖励状态，同时减少转移到-1的惩罚状态的可能性。这意味着代理会倾向于向右上角移动，尽管在到达那里的过程中，代理会积累正奖励。

要注意的是，由于γ接近1，未来的奖励在总回报中的权重只略低于即时奖励。这可能会稍微降低对快速达到+10状态的紧迫性，因为即使在达到目标之前，代理也在积累正回报。

## Q3:
这张图片包含了关于强化学习中的桥梁穿越问题的三个问题。下面是问题的翻译和解答：

**问题3：**
<img src="C:\Users\yhb\MscProject\AI&TA\AI\images\img_28.png">

图2展示了一个狭窄的桥梁环境，由一个网格世界表示。一个机器人开始于左手边，在中间行的位置（标有奖励值1）。目标是中间行的右手边，标有奖励值10的格子。标有-100奖励的格子是终止节点，代表机器人掉下桥。机器人可以向上、下、左、右移动一个格子。当被告知向特定方向移动时，它以0.8的概率向预定方向移动，或以0.1的概率向预定方向的90度方向移动，或以0.1的概率向预定方向的-90度方向移动。

(a) 使用一个0.9的折扣值，计算一个和两个移动后，每个非终结格子的效用值。

(b) 这个问题导致了图2b中显示的最优策略，该策略没有穿越桥。降低折扣值对策略有什么影响？

(c) 增加目标的效用值对策略有什么影响？选择一个新的目标状态的效用值，以便最优策略是穿越桥从左到右，并在3次迭代后展示每个非终结格子的效用值。

**答案：**

(a) 要计算一个和两个移动后的效用值，我们需要应用贝尔曼方程，并考虑所有可能的动作及其转移概率。以0.9的折扣因子，效用值将是即时奖励加上折扣后的预期未来奖励。对于一个移动后的效用值，我们需要考虑从起始位置移动一步后可能到达的位置和相应的奖励。对于两个移动后，我们需要考虑两步移动可能到达的位置及其奖励。

(b) 降低折扣值意味着未来的奖励对当前决策的影响减少，即机器人会更加偏向于即时奖励。这可能会导致策略变化，使得机器人选择更短期内奖励较高的动作，这可能包括冒险穿越桥。

(c) 增加目标的效用值会使得长期的大奖励更具吸引力，这可能会导致机器人选择更长期的策略，即冒险穿越桥以获得更大的最终奖励。为了实现这一变化，我们需要显著增加目标的效用值，以使得即使在考虑到掉落的风险后，穿越桥的预期回报仍然是正的。

根据计算结果：

- 在进行一次移动后，除了已知的终止状态（奖励为1和10的状态）外，其他非终止状态的效用值（通过折扣后的值）如下：


  | -90.0  | -90.0  | -90.0  | -90.0  | -90.0  |
  |--------|--------|--------|--------|--------|
  | **0.9**   | **0.72**  | 0.0    | **7.2**   | **9.0**   |
  | -90.0  | -90.0  | -90.0  | -90.0  | -90.0  |

- 在进行两次移动后，非终止状态的效用值（通过折扣后的值）更新为：


  | -90.0   | -90.0   | -90.0   | -90.0   | -90.0   |
  |---------|---------|---------|---------|---------|
  | **0.9**    | **-15.552** | **-11.016** | **-9.72**  | **9.0**    |
  | -90.0   | -90.0   | -90.0   | -90.0   | -90.0   |

在这些矩阵中，负值较大（-90.0）的格子代表机器人掉下桥的终止状态，其效用值是负的，因为这是一个非常不好的结果。正值（0.9，0.72，等等）的格子代表机器人在不掉下桥的情况下移动的可能路径。在每次迭代后，这些路径的效用值都会更新，以反映到达目标的期望总回报。

注意：在实际计算中，非终止状态的效用是根据贝尔曼方程计算的，这需要考虑所有可能的动作及其概率。而且，终止状态的效用值不会随迭代改变，因为一旦到达终止状态，游戏/任务就结束了。