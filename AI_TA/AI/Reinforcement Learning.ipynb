{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Reinforcement Learning\n",
    "强化学习（Reinforcement Learning, RL）是机器学习的一个子领域，它涉及智能体（Agent）如何在环境（Environment）中采取行动以最大化某种累积奖励。强化学习不同于监督学习（Supervised Learning）和无监督学习（Unsupervised Learning），它不依赖于数据集的标签，而是通过与环境的交互来学习。\n",
    "\n",
    "强化学习的核心组成包括：\n",
    "\n",
    "1. **智能体（Agent）**：执行行动的实体。\n",
    "2. **环境（Environment）**：智能体所处并与之交互的世界。\n",
    "3. **状态（State）**：环境的当前情况。\n",
    "4. **动作（Action）**：智能体可以执行的操作。\n",
    "5. **奖励（Reward）**：智能体执行动作后从环境中获得的反馈，指导学习过程。\n",
    "6. **策略（Policy）**：智能体决定何时采取何种动作的规则。\n",
    "7. **价值函数（Value Function）**：预估从某状态开始，遵循特定策略可获得的累积奖励。\n",
    "8. **模型（Model）**：可选，智能体对环境如何响应动作的理解。\n",
    "\n",
    "强化学习的过程通常涉及智能体不断地尝试各种动作并观察结果，包括立即奖励和新的状态。智能体的目标是学习一个策略，它能在长期内获得最大的累积奖励。这通常涉及到探索（Exploration）与利用（Exploitation）之间的平衡：探索是指尝试新的动作以发现更好的策略，而利用是指根据已有的知识选择最佳动作。\n",
    "\n",
    "强化学习的关键挑战之一是奖励可能会延迟发生。这意味着好的或坏的行为的结果可能不会立即显现，智能体需要学会通过长期的结果来判断其行为的价值。\n",
    "\n",
    "强化学习的应用非常广泛，包括自动驾驶汽车、机器人控制、游戏、推荐系统等领域。有许多成功的强化学习案例，例如DeepMind的AlphaGo，它在学习了围棋的策略后，击败了世界冠军级的选手。强化学习算法包括Q学习（Q-Learning）、Sarsa、深度Q网络（Deep Q-Network, DQN）、策略梯度方法、信任区域策略优化（Trust Region Policy Optimization, TRPO）和近端策略优化（Proximal Policy Optimization, PPO）等。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "411095dd8885b130"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**动作的非确定性效应（Actions have non-deterministic effects）**：这意味着当智能体在环境中采取动作时，其结果可能不是确定的。同一个动作在相同状态下的多次执行可能会导致不同的后续状态。这种不确定性要求智能体必须能够处理变化并适应可能的不同结果。\n",
    "\n",
    " **奖励/惩罚的不频繁性（Rewards / punishments are infrequent）**：在强化学习任务中，奖励（或惩罚）可能并不经常出现。例如，在一个游戏中，可能只有在游戏结束时才会给出奖励，这就需要智能体能够从长期的行动序列中学习，并理解哪些行动最终会导致奖励。\n",
    "\n",
    "**长序列动作的奖励通常在末尾（often at the end of long sequences of actions）**：这是上一点的延伸，指的是奖励往往在一系列动作完成后才会出现。这就引入了信贷分配问题（credit assignment problem），即如何将远期的奖励正确地分配给导致该结果的一系列行动。\n",
    "\n",
    "**学习者必须决定采取什么行动（Learner must decide what actions to take）**：智能体需要决策在任何给定的状态下应该执行哪个动作。这包括在探索（尝试新行动以获得信息）和利用（使用已知信息来最大化即时奖励）之间做出平衡。\n",
    "\n",
    "**世界的庞大和复杂性（World is large and complex）**：在真实世界的应用中，智能体可能面临着庞大和复杂的状态空间，这使得学习一个有效的策略变得更加困难。在这样的环境中，可能需要高级的函数近似方法（如深度学习）来帮助智能体理解并操作环境。\n",
    "\n",
    "**智能体与环境的交互是连续的（Interaction between agent and environment is continuous）**：在许多强化学习任务中，智能体与环境的交互是连续的，这意味着智能体需要能够在不断变化的环境中持续地学习和适应。\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "20b2fa8e824ddc7f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Explore-Exploit Dilemma\n",
    "这个图是关于强化学习的概念和策略的一个简要介绍，具体包括探索与利用、基于模型的强化学习和无模型的强化学习。\n",
    "\n",
    "### 探索与利用（Exploration vs Exploitation）\n",
    "\n",
    "- **利用（Exploit）**：使用你通过学习得到的结果来最大化当前的预期效用。这意味着根据已经学习到的模型，选择那些似乎最优的行动来获得最大的奖励。\n",
    "\n",
    "- **探索（Explore）**：选择一个行动来帮助你改进你的模型。这通常意味着尝试那些不确定结果的行动，以获取更多关于环境的信息。\n",
    "\n",
    "探索的方法包括：\n",
    "  - 随机选择一个行动。\n",
    "  - 选择一个之前没有选择过的行动。\n",
    "  - 选择一个将带你进入未探索状态的行动。\n",
    "\n",
    "利用的方法是：跟随策略。\n",
    "\n",
    "关于何时探索，通常需要在探索新可能性和利用已知信息之间找到一个平衡点。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd47de0d661ab394"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Model-based vs Model-free\n",
    "### 基于模型的强化学习（Model-based reinforcement learning）\n",
    "\n",
    "- **学习MDP（Markov Decision Process）**：这意味着学习一个模型，描述状态转换和奖励。\n",
    "\n",
    "- **解决MDP来确定最优策略**：使用像策略迭代或值迭代这样的算法来找到最优策略。\n",
    "\n",
    "- **将预期奖励和实际奖励之间的差异作为错误信号**：这个差异可以用来调整模型或策略，以改进预测奖励的准确性。\n",
    "\n",
    "### 无模型的强化学习（Model-free reinforcement learning）\n",
    "\n",
    "- **直接学习效用（或价值）函数**：这种方法不试图学习环境的模型，而是直接学习从状态到行动价值的映射。这样的方法包括Q学习（Q-Learning）和时序差分学习（Temporal Difference Learning, TD Learning）。\n",
    "\n",
    "在Q学习中，智能体学习一个动作价值函数 $\\( Q(s, a) \\)$，它提供了在状态 $\\( s \\)$ 执行动作 $\\( a \\)$ 的预期效用。在TD学习中，智能体在每一步更新其价值估计，基于当前步的奖励以及下一状态的估计价值。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba34b4ae4e242459"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### EXAMPLE: 机器人过桥\n",
    "<img src=\"./images/img_28.png\">\n",
    "\n",
    "图2展示了一个狭窄的桥梁环境，由一个网格世界表示。一个机器人开始于左手边，在中间行的位置（标有奖励值1）。目标是中间行的右手边，标有奖励值10的格子。标有-100奖励的格子是终止节点，代表机器人掉下桥。机器人可以向上、下、左、右移动一个格子。当被告知向特定方向移动时，它以0.8的概率向预定方向移动，或以0.1的概率向预定方向的90度方向移动，或以0.1的概率向预定方向的-90度方向移动。\n",
    "\n",
    "(a) 使用一个0.9的折扣值，计算一个和两个移动后，每个非终结格子的效用值。\n",
    "\n",
    "(b) 这个问题导致了图2b中显示的最优策略，该策略没有穿越桥。降低折扣值对策略有什么影响？\n",
    "\n",
    "(c) 增加目标的效用值对策略有什么影响？选择一个新的目标状态的效用值，以便最优策略是穿越桥从左到右，并在3次迭代后展示每个非终结格子的效用值。\n",
    "\n",
    "**答案：**\n",
    "\n",
    "(a) 要计算一个和两个移动后的效用值，我们需要应用贝尔曼方程，并考虑所有可能的动作及其转移概率。以0.9的折扣因子，效用值将是即时奖励加上折扣后的预期未来奖励。对于一个移动后的效用值，我们需要考虑从起始位置移动一步后可能到达的位置和相应的奖励。对于两个移动后，我们需要考虑两步移动可能到达的位置及其奖励。\n",
    "\n",
    "(b) 降低折扣值意味着未来的奖励对当前决策的影响减少，即机器人会更加偏向于即时奖励。这可能会导致策略变化，使得机器人选择更短期内奖励较高的动作，这可能包括冒险穿越桥。\n",
    "\n",
    "(c) 增加目标的效用值会使得长期的大奖励更具吸引力，这可能会导致机器人选择更长期的策略，即冒险穿越桥以获得更大的最终奖励。为了实现这一变化，我们需要显著增加目标的效用值，以使得即使在考虑到掉落的风险后，穿越桥的预期回报仍然是正的。\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de8a45dd2a8db9cf"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(array([[-90.  , -90.  , -90.  , -90.  , -90.  ],\n        [  0.9 ,   0.72,   0.  ,   7.2 ,   9.  ],\n        [-90.  , -90.  , -90.  , -90.  , -90.  ]]),\n array([[-90.   , -90.   , -90.   , -90.   , -90.   ],\n        [  0.9  , -15.552, -11.016,  -9.72 ,   9.   ],\n        [-90.   , -90.   , -90.   , -90.   , -90.   ]]))"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 定义3x5的网格世界状态空间，-100表示掉下桥，0表示可以行走的格子，10是目标格子\n",
    "grid = np.array([\n",
    "    [-100, -100, -100, -100, -100],\n",
    "    [1,     0,     0,     0,   10],\n",
    "    [-100, -100, -100, -100, -100]\n",
    "])\n",
    "\n",
    "# 定义折扣因子\n",
    "gamma = 0.9\n",
    "\n",
    "# 初始化效用矩阵，终止状态的效用已知，其余设为0\n",
    "utilities = np.zeros(grid.shape)\n",
    "utilities[1, 0] = 1\n",
    "utilities[1, -1] = 10\n",
    "\n",
    "# 动作转移概率\n",
    "action_prob = {\n",
    "    'intended': 0.8,\n",
    "    'left': 0.1,\n",
    "    'right': 0.1\n",
    "}\n",
    "\n",
    "# 四个可能的动作及其对应的状态转移\n",
    "actions = {\n",
    "    'up': (-1, 0),\n",
    "    'down': (1, 0),\n",
    "    'left': (0, -1),\n",
    "    'right': (0, 1)\n",
    "}\n",
    "\n",
    "# 计算一个状态的效用\n",
    "def calculate_utility(state, utilities):\n",
    "    if grid[state] != 0:\n",
    "        # 如果是终止状态，直接返回其奖励值; grid[state]是状态state的奖励值\n",
    "        return grid[state]\n",
    "    else:\n",
    "        # 否则计算每个动作的期望效用\n",
    "        expected_utilities = []\n",
    "        for action in actions.keys():\n",
    "            # 对于每个动作，计算向预定方向及其左右方向移动的结果\n",
    "            total = 0\n",
    "            for move, prob in action_prob.items():\n",
    "                # 当move为'intended'时，表示按照预定方向移动\n",
    "                if move == 'intended':\n",
    "                    next_state = tuple(np.add(state, actions[action]))\n",
    "                # 当move为'left'时，表示按照预定方向的左边方向移动。比如，向上移动时，左边方向是左移；向下移动时，左边方向是右移\n",
    "                elif move == 'left':\n",
    "                    next_state = tuple(np.add(state, actions[{'up': 'left', 'down': 'right', 'left': 'down', 'right': 'up'}[action]]))\n",
    "                else:  # right\n",
    "                    next_state = tuple(np.add(state, actions[{'up': 'right', 'down': 'left', 'left': 'up', 'right': 'down'}[action]]))\n",
    "                \n",
    "                # 确保下一个状态不会超出网格边界\n",
    "                next_state = (max(0, min(next_state[0], grid.shape[0] - 1)),\n",
    "                              max(0, min(next_state[1], grid.shape[1] - 1)))\n",
    "                \n",
    "                # 累加期望效用\n",
    "                total += prob * utilities[next_state]\n",
    "            expected_utilities.append(total)\n",
    "        \n",
    "        # 返回动作的最大期望效用\n",
    "        return max(expected_utilities)\n",
    "\n",
    "# 进行一次迭代计算效用\n",
    "def one_iteration(utilities):\n",
    "    new_utilities = np.copy(utilities)\n",
    "    for i in range(grid.shape[0]):\n",
    "        for j in range(grid.shape[1]):\n",
    "            new_utilities[i, j] = calculate_utility((i, j), utilities)\n",
    "    return new_utilities\n",
    "\n",
    "# 两次迭代计算效用值\n",
    "utilities_one_move = one_iteration(utilities) * gamma\n",
    "utilities_two_moves = one_iteration(utilities_one_move) * gamma\n",
    "\n",
    "utilities_one_move, utilities_two_moves"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T17:56:43.257808200Z",
     "start_time": "2024-02-06T17:56:43.093364100Z"
    }
   },
   "id": "15eaf4b8ce9299c3",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Q-Learning\n",
    "Q学习（Q-Learning）是一种无模型（model-free）的强化学习算法，它旨在学习一个策略，告诉智能体在给定状态下应采取哪个动作以最大化总回报。它使用了一种称为Q函数的值函数，用于估计在某状态下采取某动作能获得的预期效用。\n",
    "\n",
    "### Q学习的算法原理包括：\n",
    "\n",
    "1. **初始化Q值函数**：算法首先初始化一个Q表，表中的每个条目 $\\( Q(s, a) \\)$ 对应一个状态-动作对的值，初始化时通常设为0或一个小的随机值。\n",
    "\n",
    "2. **策略（Policy）**：虽然Q学习的最终目标是学习最优策略，但在学习过程中通常使用ϵ-贪婪策略（ϵ-greedy policy）来平衡探索和利用。这意味着大部分时间（1-ϵ 的概率）智能体会选择当前Q值最高的动作（利用），而有一小部分时间（ϵ 的概率）会随机选择一个动作（探索）。\n",
    "\n",
    "3. **经历和学习**：智能体在环境中根据其策略经历一系列状态-动作转换。对于每次转换，智能体会观察到新的状态 $\\( s' \\)$ 和奖励 $\\( r \\)$。\n",
    "\n",
    "4. **更新Q值**：根据观察到的奖励和最大的Q值估计，使用以下更新规则更新Q表：\n",
    "$$\\[ Q(s, a) \\leftarrow Q(s, a) + \\alpha [r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)] \\]$$\n",
    "   - $\\( Q(s, a) \\) $是当前状态-动作对的Q值。\n",
    "   - $\\( \\alpha \\)$ 是学习率，它决定了新获得的信息会覆盖旧信息的程度。\n",
    "   - $\\( r \\)$ 是智能体收到的立即奖励。\n",
    "   - $\\( \\gamma \\)$ 是折扣因子，用于权衡未来奖励的重要性。\n",
    "   - $\\( \\max_{a'} Q(s', a') \\)$ 是下一个状态下所有可能动作的最大Q值，表示未来的预期奖励。\n",
    "\n",
    "5. **重复过程**：智能体不断重复这个过程（选择动作，观察新状态和奖励，更新Q值），直到环境达到终止状态，如游戏结束或达到了一个特定的目标状态。此过程通常在多个回合（episodes）中进行，以确保智能体有机会经历不同的状态-动作路径。\n",
    "\n",
    "6. **收敛至最优策略**：理论上，随着时间的推移，Q值会逐渐收敛到最优策略的Q值，这时智能体将能够在每个状态下选择最优动作。\n",
    "\n",
    "Q学习的一个关键优点是它能够处理没有模型的情况，并可以适用于拥有离散动作空间的任何环境。然而，它在处理大状态空间或连续动作空间时可能会变得非常复杂或计算上不可行。在这些情况下，通常会使用函数近似器（如神经网络）来近似Q函数，这就是深度Q网络（Deep Q-Network, DQN）的基础。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ec65a5196f34d190"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained Q-table:\n",
      "[[0.9375     0.5625    ]\n",
      " [0.5625     1.5625    ]\n",
      " [0.9375     0.93749855]\n",
      " [1.5625     0.5625    ]\n",
      " [0.5625     0.9375    ]]\n"
     ]
    }
   ],
   "source": [
    "# Q-Learning Example\n",
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.6  # Discount factor\n",
    "epsilon = 0.1  # Exploration rate\n",
    "num_episodes = 1000  # Number of episodes for training\n",
    "max_steps_per_episode = 100  # Limit the number of steps per episode\n",
    "\n",
    "# Assume a simple environment with 5 states and 2 actions\n",
    "num_states = 5 # 0, 1, 2, 3, 4 共5个状态\n",
    "num_actions = 2 # 0: move right, 1: move left，表示只有0和1两种动作\n",
    "\n",
    "# Initialize Q-table with zeros\n",
    "Q_table = np.zeros((num_states, num_actions))\n",
    "\n",
    "# 设定环境的状态转移和奖励，输入当前状态和动作，返回下一个状态和奖励\n",
    "def step(state, action):\n",
    "    if action == 0: # 若动作为0\n",
    "        next_state = (state + 1) % num_states # 当前状态加1\n",
    "        reward = 1 if next_state == num_states - 1 else 0 # 如果下一个状态是最后一个状态，奖励为1，否则为0\n",
    "    else: # 若动作为1\n",
    "        next_state = (state - 1) % num_states\n",
    "        reward = 1 if next_state == 0 else 0\n",
    "    return next_state, reward\n",
    "\n",
    "# Q-learning algorithm\n",
    "for episode in range(num_episodes):\n",
    "    state = np.random.randint(num_states)  # Start at a random state\n",
    "    for step_num in range(max_steps_per_episode):\n",
    "        \n",
    "        # 若随机数小于探索率，随机选择一个动作进行探索\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            action = np.random.randint(num_actions)\n",
    "            \n",
    "        # 否则，选择Q值最大的动作进行利用\n",
    "        else: \n",
    "            action = np.argmax(Q_table[state, :])\n",
    "\n",
    "        # 进行一步状态转移，获得下一个状态和奖励\n",
    "        next_state, reward = step(state, action)\n",
    "\n",
    "        # 更新Q表，使用Q学习的更新规则：Q(s, a) = Q(s, a) + alpha * (reward + gamma * max(Q(s', a')) - Q(s, a))\n",
    "        Q_table[state, action] = Q_table[state, action] + alpha * (reward + gamma * np.max(Q_table[next_state, :]) - Q_table[state, action])\n",
    "\n",
    "        # 更新当前状态\n",
    "        state = next_state\n",
    "\n",
    "# 训练后的Q表\n",
    "print(\"Trained Q-table:\")\n",
    "print(Q_table)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-06T01:28:04.878418200Z",
     "start_time": "2024-02-06T01:28:04.177165500Z"
    }
   },
   "id": "6745a78cc3f7df4f",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "这个Q表可以用来选择每个状态下的最优动作。例如，在状态0，选择动作0（因为0.9375 > 0.5625）将给出最高的预期效用。同样，在状态1，选择动作1（因为1.5625 > 0.5625）是最优的。这个Q表是通过多次的试验和学习得到的，每个状态-动作对的Q值都是根据奖励和折扣未来奖励的期望值进行更新的。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10f15c6fb4dea0e1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
