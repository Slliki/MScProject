{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Feedforward Neural Network -- FNN\n",
    "---\n",
    "<img src=\".\\images\\img_1.png\">\n",
    "\n",
    "前馈神经网络（Feedforward Neural Network，或称为多层感知机 MLP）是一种基本的人工神经网络模型，其工作原理如下：\n",
    "\n",
    "1. **输入层**：前馈神经网络的第一层是输入层，该层接收来自外部的输入数据。每个输入特征对应输入层中的一个节点（神经元），输入特征值作为节点的激活值。\n",
    "\n",
    "2. **隐藏层**：在输入层之后，前馈神经网络通常包含一到多个隐藏层。每个隐藏层由多个神经元组成，每个神经元都与前一层的所有神经元连接。每个连接都有一个权重，用于调整信号的强度。每个隐藏层的神经元将前一层的输出加权求和后，通过激活函数进行非线性变换，得到输出值。\n",
    "\n",
    "3. **激活函数**：激活函数是隐藏层中每个神经元的非线性变换函数。它引入了非线性性质，使得神经网络可以学习复杂的关系。常见的激活函数包括ReLU（Rectified Linear Unit）、Sigmoid和Tanh等。\n",
    "\n",
    "4. **输出层**：前馈神经网络的最后一层是输出层。输出层的神经元的数量通常取决于任务类型，例如，对于二分类任务，可以有一个输出神经元，表示正类别的概率；对于多分类任务，输出神经元的数量等于类别的数量。\n",
    "\n",
    "5. **前向传播**：神经网络的前向传播是指从输入层开始，通过每一层的神经元计算，将信号从输入传递到输出的过程。具体步骤如下：\n",
    "   - 输入特征在输入层被传递。\n",
    "   - 信号按权重加权并通过激活函数进行非线性变换，然后传递到下一层。\n",
    "   - 这个过程在每个隐藏层和输出层都会重复。\n",
    "   - 最终，输出层的值表示模型对输入数据的预测或分类。\n",
    "\n",
    "6. **损失函数**：在前馈神经网络中，需要定义一个损失函数来度量模型的输出与实际目标之间的差异。损失函数的选择通常取决于任务类型，如均方误差损失用于回归任务，交叉熵损失用于分类任务。\n",
    "\n",
    "7. **反向传播和优化**：模型的训练是通过反向传播算法来实现的。反向传播算法计算损失函数相对于模型参数的梯度，然后使用梯度下降或其他优化算法来更新参数，以最小化损失函数。这个过程反复进行，直到模型收敛或达到停止训练的条件。\n",
    "\n",
    "8. **预测**：一旦模型训练完成，它可以用于对新的未见数据进行预测。输入数据通过前向传播，模型输出预测结果。\n",
    "\n",
    "前馈神经网络是一种非常基础的神经网络模型，用于解决各种任务，包括分类、回归、模式识别等。其工作原理基于输入数据的传递和权重的学习，通过不断调整权重来适应给定任务。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b726acf17f89a8e0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 示例说明\n",
    "当然，我可以通过一个简单的例子来说明前馈神经网络的计算过程。假设我们有一个前馈神经网络用于二分类任务，网络结构如下：\n",
    "\n",
    "- 输入层：2个特征（神经元）\n",
    "- 隐藏层：2个神经元，使用ReLU激活函数\n",
    "- 输出层：1个神经元，使用Sigmoid激活函数\n",
    "\n",
    "我们收到一个输入样本，特征值为 [0.5, 0.7]。\n",
    "\n",
    "1. **前向传播**：\n",
    "   - 输入层接收输入特征 [0.5, 0.7]。\n",
    "   - 隐藏层的第一个神经元计算：  \n",
    "     输入： $\\(0.5 \\times w_{11} + 0.7 \\times w_{21} + b_1\\)$\n",
    "     使用ReLU激活函数： $\\(a_1 = \\max(0, 0.5 \\times w_{11} + 0.7 \\times w_{21} + b_1)\\)$\n",
    "   - 隐藏层的第二个神经元计算：  \n",
    "     输入： $\\(0.5 \\times w_{12} + 0.7 \\times w_{22} + b_2\\)$\n",
    "     使用ReLU激活函数： $\\(a_2 = \\max(0, 0.5 \\times w_{12} + 0.7 \\times w_{22} + b_2)\\)$\n",
    "   - 输出层计算：  \n",
    "     输入： $\\(a_1 \\times w_{31} + a_2 \\times w_{32} + b_3\\)$  \n",
    "     使用Sigmoid激活函数得到输出值： $\\(y = \\frac{1}{{1 + e^{-(a_1 \\times w_{31} + a_2 \\times w_{32} + b_3)}}}\\)$\n",
    "\n",
    "当计算损失和执行反向传播时，需要进行以下具体步骤：\n",
    "\n",
    "1. **计算损失**：\n",
    "   - 使用神经网络前向传播的结果 $\\(y\\)$ 与实际标签 $\\(\\hat{y}\\)$ 计算损失。常见的损失函数包括均方误差损失（MSE）和交叉熵损失（Cross-Entropy Loss）等，具体选择取决于任务类型。\n",
    "   - 例如，如果使用均方误差损失，损失计算如下： \n",
    "   $$\\[L(y, \\hat{y}) = (y - \\hat{y})^2\\]$$\n",
    "\n",
    "2. **反向传播**：\n",
    "   - 计算损失相对于输出层的梯度 $\\(\\frac{\\partial L}{\\partial y}\\)$。对于均方误差损失，这个梯度可以直接计算为 $\\(2(y - \\hat{y})\\)$。\n",
    "   - 使用链式法则，将梯度传播回隐藏层和输入层。具体步骤如下：\n",
    "     - 计算输出层相对于其输入的梯度： $\\(\\frac{\\partial y}{\\partial z}\\)$，其中 $\\(z\\)$ 是输出层的输入（未经过激活函数）。\n",
    "     - 计算输出层的参数梯度： \n",
    "       - $\\[\\frac{\\partial L}{\\partial w_{31}} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial z} \\cdot a_1\\]$\n",
    "       - $\\[\\frac{\\partial L}{\\partial w_{32}} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial z} \\cdot a_2\\]$\n",
    "       - $\\[\\frac{\\partial L}{\\partial b_3} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial z}\\]$\n",
    "     - 计算隐藏层相对于其输入的梯度： $\\(\\frac{\\partial a_1}{\\partial z_1}\\)$ 和 $\\(\\frac{\\partial a_2}{\\partial z_2}\\)$，其中 $\\(z_1\\)$ 和 $\\(z_2\\)$ 是隐藏层的输入。\n",
    "     - 计算隐藏层的参数梯度： \n",
    "       - $\\[\\frac{\\partial L}{\\partial w_{11}} = \\frac{\\partial L}{\\partial a_1} \\cdot \\frac{\\partial a_1}{\\partial z_1} \\cdot x_1\\]$\n",
    "       - $\\[\\frac{\\partial L}{\\partial w_{12}} = \\frac{\\partial L}{\\partial a_2} \\cdot \\frac{\\partial a_2}{\\partial z_2} \\cdot x_1\\]$\n",
    "       - $\\[\\frac{\\partial L}{\\partial w_{21}} = \\frac{\\partial L}{\\partial a_1} \\cdot \\frac{\\partial a_1}{\\partial z_1} \\cdot x_2\\]$\n",
    "       - $\\[\\frac{\\partial L}{\\partial w_{22}} = \\frac{\\partial L}{\\partial a_2} \\cdot \\frac{\\partial a_2}{\\partial z_2} \\cdot x_2\\]$\n",
    "       - $\\[\\frac{\\partial L}{\\partial b_1} = \\frac{\\partial L}{\\partial a_1} \\cdot \\frac{\\partial a_1}{\\partial z_1}\\]$\n",
    "       - $\\[\\frac{\\partial L}{\\partial b_2} = \\frac{\\partial L}{\\partial a_2} \\cdot \\frac{\\partial a_2}{\\partial z_2}\\]$\n",
    "\n",
    "3. **参数更新**：\n",
    "   - 使用梯度下降或其他优化算法，根据参数的梯度更新权重和偏置。通常的更新规则为：\n",
    "     - $\\(w_{ij} = w_{ij} - \\alpha \\cdot \\frac{\\partial L}{\\partial w_{ij}}\\)$，其中 $\\(\\alpha\\)$ 是学习率。\n",
    "     - $\\(b_j = b_j - \\alpha \\cdot \\frac{\\partial L}{\\partial b_j}\\)$\n",
    "\n",
    "4. **重复训练**：\n",
    "   - 重复上述步骤，将多个训练样本输入神经网络，进行前向传播、损失计算、反向传播和参数更新，直到满足停止训练的条件，如达到最大迭代次数或损失足够小。\n",
    "\n",
    "这些步骤循环进行，直到神经网络的参数收敛到一组使损失最小化的值，模型就训练完成了。这就是前馈神经网络的训练过程，它通过反向传播和梯度下降来学习适合任务的权重和偏置，以便进行预测和分类。\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74d4823498abb86a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fully Connected Layer 全连接层\n",
    "全连接层是神经网络中最基本的层，也是最常用的层。全连接层的每个神经元都与上一层的所有神经元相连，因此也称为密集层（Dense Layer）。全连接层的输入是一个向量，输出也是一个向量，输出的维度取决于该层神经元的数量。\n",
    "\n",
    "`torch.nn.Linear(in_features, out_features, bias=True)`\n",
    "- in_features：输入特征维度\n",
    "- out_features：输出特征维度\n",
    "- bias：是否使用偏置，默认使用\n",
    "\n",
    "nn.Linear()的输入和输出都是二维张量，一般形状为[batch_size, size]，其中batch_size表示输入样本的数量，size表示每个样本的特征维度。该层的输出形状为[batch_size, out_features]。该函数使用线性变换：$y = xA^T + b$，其中$A$和$b$分别是该层的权重和偏置，$x$是该层的输入。\n",
    "\n",
    "#### 计算公式\n",
    "对于 `nn.Linear(m, n)` 这个层，它的权重矩阵的维度是 `(n, m)`，偏置向量的维度是 `(n,)`。这里的 `m` 和 `n` 分别对应于输入特征和输出特征的维度。\n",
    "$$\\[ \\text{output} = \\text{tensor} \\times \\text{weights}^T + \\text{bias} \\]$$\n",
    "\n",
    "其中：\n",
    "- `tensor` 是输入张量，维度为 `(1, m)`。\n",
    "- `weights` 是权重矩阵，维度为 `(n, m)`。\n",
    "- `bias` 是偏置向量，维度为 `(n,)`。\n",
    "- 输出 `output` 的维度为 `(1, n)`。\n",
    "\n",
    "在 PyTorch 中，这个过程是自动完成的，你不需要手动进行矩阵乘法和加偏置。当你调用 `layer(tensor)` 时，PyTorch 会自动执行这些操作并给出输出。\n",
    "\n",
    "要注意的是，由于权重和偏置是在层初始化时随机生成的（除非你手动设置了固定的值），每次运行程序时得到的输出可能会有所不同。在你提供的代码中，通过设置随机数种子 `torch.manual_seed(0)`，确保了每次运行时权重和偏置的初始值是相同的，从而保证了输出的一致性。\n",
    "\n",
    "---\n",
    "\n",
    "#### 代码示例1；简单调用nn.Linear()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e66250070c051be"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.6514,  1.3005, -0.8323]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "# FCL示例\n",
    "\n",
    "# 设置随机数种子\n",
    "torch.manual_seed(0)\n",
    "# 创建一个全连接层，输入特征维度为4，输出特征维度为3\n",
    "layer = nn.Linear(4, 3)\n",
    "# 创建一个4维的输入张量\n",
    "tensor = torch.FloatTensor([[1, 2, 3, 4]])\n",
    "# 通过全连接层得到输出\n",
    "output = layer(tensor)\n",
    "print(output) \n",
    "\n",
    "# 输入size：[1, 4]、\n",
    "# 输出size：[1, 3]、"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T15:43:37.310086400Z",
     "start_time": "2024-02-01T15:43:37.300613200Z"
    }
   },
   "id": "154ea7f25a4ac2a7",
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 代码示例2: 完整的FNN模型示例\n",
    "创建一个二分类数据集，首先使用train_test_split函数将数据集划分为训练集和测试集，然后将数据集转换为PyTorch的Tensor，最后定义前馈神经网络模型、损失函数和优化器。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "938e5cc89191d0f8"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([80, 2]) torch.Size([80])\n"
     ]
    }
   ],
   "source": [
    "# 创建一个虚拟数据集\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(100, 2)  # 特征向量，100个样本，每个样本有2个特征\n",
    "y = (X[:, 0] + X[:, 1] > 1).astype(int)  # 标签，根据特征之和大于1或不大于1进行二分类, astype(int)将布尔值转换为整数\n",
    "\n",
    "# 划分数据集为训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 转换为PyTorch的Tensor\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "y_train = torch.FloatTensor(y_train)\n",
    "X_test = torch.FloatTensor(X_test)\n",
    "y_test = torch.FloatTensor(y_test)\n",
    "\n",
    "print(X_train.shape, y_train.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T15:13:38.544284300Z",
     "start_time": "2024-02-01T15:13:38.525347900Z"
    }
   },
   "id": "71f6bc05f1503ebd",
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "该前馈神经网络包括：\n",
    "- 输入层：2个特征\n",
    "    - Linear全连接线性变换：维度变化2-->4\n",
    "    - ReLU激活函数\n",
    "- 隐藏层：4个神经元\n",
    "    - Linear全连接线性变换：维度变化4-->1\n",
    "    - Sigmoid激活函数\n",
    "- 输出层：1个神经元\n",
    "- 二元交叉熵损失函数\n",
    "- Adam优化器，用于后向传播时更新参数。注意，pytorch会自动计算梯度，用于后向传播，并不需要单独编写反向传播算法。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc36170b3183a185"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 定义前馈神经网络模型\n",
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) # 输入层到隐藏层的线性变换\n",
    "        self.relu = nn.ReLU() # 隐藏层的激活函数\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size) # 隐藏层到输出层的线性变换\n",
    "        self.sigmoid = nn.Sigmoid() # 输出层的激活函数\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# 初始化模型和优化器\n",
    "input_size = 2\n",
    "hidden_size = 4\n",
    "output_size = 1\n",
    "model = FeedForwardNN(input_size, hidden_size, output_size)\n",
    "criterion = nn.BCELoss()  # 二分类问题使用二元交叉熵损失\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01) # Adam优化器,用作梯度下降来更新参数"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-01T15:15:47.705231300Z",
     "start_time": "2024-02-01T15:15:47.680312500Z"
    }
   },
   "id": "ff1fdd8d60f62a26",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 0.0101\n",
      "Epoch [200/1000], Loss: 0.0097\n",
      "Epoch [300/1000], Loss: 0.0093\n",
      "Epoch [400/1000], Loss: 0.0089\n",
      "Epoch [500/1000], Loss: 0.0085\n",
      "Epoch [600/1000], Loss: 0.0081\n",
      "Epoch [700/1000], Loss: 0.0077\n",
      "Epoch [800/1000], Loss: 0.0074\n",
      "Epoch [900/1000], Loss: 0.0070\n",
      "Epoch [1000/1000], Loss: 0.0066\n",
      "Test Accuracy: 95.00%\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    # 前向传播\n",
    "    # 清空梯度\n",
    "    optimizer.zero_grad()\n",
    "    # 训练集输入模型进行预测\n",
    "    outputs = model(X_train)\n",
    "    # 计算损失\n",
    "    loss = criterion(outputs, y_train.view(-1, 1))\n",
    "    \n",
    "    # 反向传播和参数更新\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 每100个epoch打印一次损失值\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# 在测试集上评估模型， 不需要计算梯度，减少内存消耗\n",
    "with torch.no_grad():\n",
    "    predicted = model(X_test) # 预测值\n",
    "    predicted = (predicted > 0.5).float() # 将预测值大于0.5的转换为1，否则转换为0\n",
    "    accuracy = (predicted == y_test.view(-1, 1)).float().mean()\n",
    "    print(f'Test Accuracy: {accuracy.item()*100:.2f}%')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-02T18:44:38.332502800Z",
     "start_time": "2024-02-02T18:44:37.921287900Z"
    }
   },
   "id": "c395c038b7909324",
   "execution_count": 23
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
