{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Markov Process\n",
    "### INCLUDES\n",
    "1. Markov Chain\n",
    "2. Hidden Markov Model (HMM)\n",
    "3. Rewards\n",
    "4. Value Iteration\n",
    "5. Policy Iteration"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "45acd62fd339cf3c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Markov Chain\n",
    "马尔可夫链（Markov Chain）是一种数学模型，它用来描述一个系统随时间进展的状态转换过程。在马尔可夫链中，系统在任何时间点的状态只依赖于它在前一个时间点的状态，这种特性称为“无记忆性”或“马尔可夫性质”。换句话说，在给定当前状态的情况下，系统的未来状态与它的历史状态无关。\n",
    "\n",
    "一个马尔可夫链可以通过以下几个要素定义：\n",
    "\n",
    "1. **状态空间**：这是系统可能所处的所有状态的集合，可以是有限的也可以是无限的。\n",
    "\n",
    "2. **转移矩阵**：对于一个有限状态空间的马尔可夫链，转移矩阵 $\\( P \\)$ 表示了从任一状态 $\\( i \\)$ 转移到另一状态 $\\( j \\)$ 的概率。转移矩阵的每一行之和必须为1，因为它们表示了概率分布。\n",
    "\n",
    "3. **初始状态分布**：这是系统在时间开始时各个状态的概率分布。\n",
    "\n",
    "如果用 $\\( P_{ij} \\)$ 表示从状态 $\\( i \\)$ 转移到状态 $\\( j \\)$ 的概率，那么对于有限状态空间的马尔可夫链，转移矩阵 $\\( P \\)$ 可以表示为：\n",
    "\n",
    "$$\\[\n",
    "P = \\begin{bmatrix}\n",
    "P_{11} & P_{12} & \\cdots & P_{1n} \\\\\n",
    "P_{21} & P_{22} & \\cdots & P_{2n} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "P_{n1} & P_{n2} & \\cdots & P_{nn}\n",
    "\\end{bmatrix}\n",
    "\\]$$\n",
    "\n",
    "其中，矩阵的每一行的元素之和为1：\n",
    "\n",
    "$$\\[\n",
    "\\sum_{j=1}^{n} P_{ij} = 1, \\quad \\forall i \\in \\{1, 2, \\ldots, n\\}\n",
    "\\]$$\n",
    "\n",
    "在马尔可夫链的演化过程中，系统的状态会根据转移矩阵中的概率进行随机转移。随着时间的推移，系统可能会达到一个稳态分布，这时各个状态的概率不再随时间变化。\n",
    "\n",
    "马尔可夫链在多个领域有广泛应用，如经济学、物理学、生物信息学、计算机科学（例如搜索引擎的网页排名算法），以及社会科学等。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71d74799a55d53ac"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 离散马尔可夫链\n",
    "\n",
    "在离散马尔可夫链中，状态转换发生在固定的、离散的时间点上。通常这些时间点可以是序列中的步骤或周期，例如，每一天、每一轮游戏等。离散马尔可夫链的特点是使用一个转移矩阵来描述从一个时间点到下一个时间点状态转换的概率。如果用 $\\( P_{ij} \\)$ 表示从状态 $\\( i \\)$ 转移到状态 $\\( j \\)$ 的概率，那么在下一个时间点 $\\( t+1 \\)$ 状态为 $\\( j \\)$ 的概率 $\\( P_{j}(t+1) \\)$ 可以通过所有可能的当前状态 $\\( i \\)$ 的概率 $\\( P_{i}(t) \\)$ 与相应的转移概率 $\\( P_{ij} \\)$ 相乘后求和得到：\n",
    "\n",
    "$$\\[ P_{j}(t+1) = \\sum_{i} P_{ij} P_{i}(t) \\]$$\n",
    "\n",
    "### 连续马尔可夫链\n",
    "\n",
    "连续马尔可夫链的状态转换可以在任意时刻发生，转换事件的发生遵循某种概率分布，通常是指数分布或泊松分布。在连续马尔可夫链中，我们关注的是在给定时间间隔内从状态 $\\( i \\)$ 转移到状态 $\\( j \\)$ 的概率，而不是在固定时间点的转换。连续马尔可夫链使用率矩阵（也称为生成矩阵或者Q矩阵）来描述状态之间的转换速率，其中每个元素 $\\( Q_{ij} \\)$ 表示单位时间内从状态 $\\( i \\)$ 转移到不同状态 $\\( j \\)$ 的速率。状态 $\\( i \\)$ 留在原地的速率由 $\\( -Q_{ii} \\)$ 给出，它等于所有从状态 $\\( i \\)$ 到其他状态转移速率的和的相反数：\n",
    "\n",
    "$$\\[ Q_{ii} = - \\sum_{j \\neq i} Q_{ij} \\]$$\n",
    "\n",
    "对于连续马尔可夫链，状态 $\\( j \\)$ 在时间 $\\( t \\)$ 的概率 $\\( P_{j}(t) \\)$ 由以下微分方程描述：\n",
    "\n",
    "$$\\[ \\frac{dP_{j}(t)}{dt} = \\sum_{i} Q_{ij} P_{i}(t) \\]$$\n",
    "\n",
    "离散马尔可夫链通常用于模型时间序列中以固定时间间隔记录的事件，而连续马尔可夫链则适用于事件发生时间是连续分布的情况。例如，在生物学中，连续马尔可夫链可以用来模拟化学反应过程中分子状态的随机转换；在金融学中，离散马尔可夫链可以用来预测股票市场的每日收盘价。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8738fadea849286b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Hidden Markov Model (HMM)\n",
    "隐马尔可夫模型（Hidden Markov Model，HMM）是一种统计模型，它假设系统能够被视为一个马尔可夫过程，但其中的状态不直接可见（它们是“隐”状态）。然而，状态的变化可以通过观察到的序列（也称为观测序列）中的一些可见现象来间接推断。HMM 在许多时间序列数据的分析中特别有用，如语音识别、手写识别、生物信息学（如蛋白质结构预测）和金融市场分析等。\n",
    "\n",
    "HMM 由以下几个基本要素组成：\n",
    "\n",
    "1. **状态集合（States）**：HMM 包含一个有限集合的隐状态 $\\( S = \\{S_1, S_2, ..., S_N\\} \\)$。在任何时间点，模型都处于这些状态中的某一个，但是这个状态不是直接可见的。\n",
    "\n",
    "2. **观测集合（Observations）**：每个隐状态可以生成一个观察，观察集合 $\\( V = \\{v_1, v_2, ..., v_M\\} \\)$ 是所有可能观察的集合。\n",
    "\n",
    "3. **转移概率矩阵（Transition probabilities）**：矩阵 $\\( A = \\{a_{ij}\\} \\)$ 包含了从状态 $\\( i \\)$ 转移到状态 $\\( j \\)$ 的概率 $\\( a_{ij} \\)$。对于任何状态 $\\( i \\)$，概率之和为1，即 $\\( \\sum_{j=1}^{N} a_{ij} = 1 \\)$\n",
    "\n",
    "4. **观测概率分布（Observation probability distribution）**：在状态 $\\( i \\)$ 下观测到 $\\( v_k \\)$ 的概率用矩阵 $\\( B = \\{b_j(k)\\} \\)$ 表示，其中 $\\( b_j(k) \\)$ 是在状态 $\\( j \\)$ 下观测到第 $\\( k \\)$ 个观测的概率。\n",
    "\n",
    "5. **初始状态概率（Initial state probabilities）**：向量 $\\( \\pi = \\{\\pi_i\\} \\)$ 表示在时间序列的开始处于状态 $\\( i \\)$ 的概率。\n",
    "\n",
    "HMM 的主要问题包括：\n",
    "\n",
    "- **评估问题（Evaluation）**：给定模型参数和观测序列，如何有效计算序列出现的概率？这通常用前向算法或后向算法解决。\n",
    "\n",
    "- **解码问题（Decoding）**：给定模型参数和观测序列，如何找到最有可能产生这些观测的隐状态序列？这通常使用维特比算法（Viterbi Algorithm）来解决。\n",
    "\n",
    "- **学习问题（Learning）**：如何调整模型参数以最好地解释观测数据？这通常通过使用Baum-Welch算法（也称为前向-后向算法）来解决。\n",
    "\n",
    "隐马尔可夫模型的强大之处在于其能够利用观测数据来推断隐状态的概率模型，即使这些状态不是直接可观测的。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ec03aab118b658c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Markov Decision Process (MDP)\n",
    "### 1. Basic Concepts\n",
    "这段话概述了一个关于复杂决策制定的讨论主题，特别是它将要探讨马尔可夫决策过程（Markov Decision Processes, MDPs）和强化学习（Reinforcement Learning, RL）的基础知识。这些是在机器学习和人工智能领域用于解决序列决策问题的关键概念和方法。\n",
    "\n",
    "让我逐一解释这些概念：\n",
    "\n",
    "1. **马尔可夫决策过程（Markov Decision Processes）**：MDP是一种数学框架，用于模拟决策者在一定状态空间内做出一系列决策的情景。在MDP中，决策者的每个行动都会导致状态变化，并伴随着一个即时奖励。\n",
    "\n",
    "\n",
    "2. **序列决策问题（Sequential Decision Problems）**：涉及连续做出一系列决策的问题，每个决策都依赖于当前的状态和先前的决策。这类问题的特点是需要考虑每个决策如何影响未来的结果。\n",
    "\n",
    "3. **奖励、效用和策略（Rewards, Utility, and Policies）**：\n",
    "   - **奖励（Rewards）**：智能体在从一个状态转移到另一个状态时所获得的即时回报。\n",
    "   - **效用（Utility）**：通常指长期的累积奖励，它反映了某个策略或一系列行动的总体价值。\n",
    "   - **策略（Policies）**：智能体在每个状态下应该采取哪个行动的指导规则。\n",
    "\n",
    "3. **值迭代（Value Iteration）**：是一种动态规划方法，用于计算MDP的最优策略。它通过迭代计算每个状态的价值函数，直到收敛到稳定的价值函数，该函数代表在该状态下的最大期望效用。\n",
    "\n",
    "4. **策略迭代（Policy Iteration）**：另一种动态规划方法，它交替进行策略评估和策略改进步骤，直到找到最优策略。\n",
    "\n",
    "MDP与强化学习（Reinforcement Learning）密切相关，强化学习是一种机器学习方法，用于解决序列决策问题。在强化学习中，智能体通过与环境的交互学习，以最大化长期奖励。MDP提供了强化学习问题的数学基础，它定义了智能体与环境之间的交互方式，以及智能体如何根据环境的反馈做出决策。两者区别在于，MDP通常假设智能体已经知道环境的动态特性，比如状态转移概率和奖励函数，而强化学习则更侧重于智能体如何从与环境的交互中学习这些特性，即智能体对环境的动态特性一无所知。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3cef1d2dec967073"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### MDP Framework\n",
    "\n",
    "- **s**: 状态（State） - 这是MDP中的一个状态，代表系统或环境在特定时间点的状态。\n",
    "\n",
    "- **$\\( s_{start} \\)$**: 起始状态（Starting State） - 这是决策过程开始时系统或环境所处的状态。\n",
    "\n",
    "- **Actions(s)**: 可能的行动（Actions） - 对于状态 \\( s \\) 而言，这是智能体可以选择的所有可能行动的集合。\n",
    "\n",
    "- **$\\( P(s' \\mid s, a) \\)$** (或 **$\\( T(s, a, s') \\)$**): 转移概率（Transition Probability） - 如果智能体在状态 $\\( s \\)$ 采取行动 $\\( a \\)$，到达状态 $\\( s' \\)$ 的概率。这个概率是MDP中的核心部分，描述了行动对状态转换的影响。\n",
    "\n",
    "- **Reward(s)**, **R(s)**, **r(s)**: 状态奖励（Reward for the State） - 在状态 $\\( s \\)$ 下智能体可以获得的奖励（可以是正的也可以是负的）。奖励函数定义了智能体在某状态下进行某行动的即时回报。\n",
    "\n",
    "- **Goal(s)**: 目标状态（Goal State） - 表示是否在过程结束时达到了目标，通常目标状态与更高的奖励相关联。\n",
    "\n",
    "- **$\\( U_h([s_1, s_2, \\ldots]) \\)$**: 效用或价值函数（Utility or Value Function） - 表示状态序列的总效用或价值，通常是序列中接收到的奖励的总和或经过折扣的总和。\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc931b56555c1cde"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Policy\n",
    "\n",
    "- **策略（Policy）**，用 π 表示，是一个从状态到行动的映射，指定了在每个状态下应该采取什么行动。例如，π(s) 表示在状态 s 下应该采取的行动。\n",
    "\n",
    "- 策略执行的随机性：当从初始状态出发执行给定的策略时，由于环境的随机性（也就是说环境是随机或者说是“随机的”），可能会导致不同的环境历史路径。这意味着即使是相同的策略，在不同的执行中也可能产生不同的结果。\n",
    "\n",
    "- **最优策略（Optimal Policy）**，用 π* 表示，是指在所有可能的策略中，能够产生最高期望效用（expected utility）的策略。期望效用通常是指从当前状态开始，按照策略 π* 所能获得的奖励的长期累积值，考虑到了概率和时间的价值（如果有折扣因子的话）。\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c7caaf3f45775441"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Utility \n",
    "这张图片讨论了在马尔可夫决策过程（MDP）中，根据时间范围不同，效用（Utility）如何随时间变化，特别是在有限时间范围（Finite Horizon）和无限时间范围（Infinite Horizon）的情境下的差异。\n",
    "\n",
    "1. **有限时间范围（Finite Horizon）**：\n",
    "   - 在这种情境下，存在一个固定的时间点 $\\( N \\)$，之后的事件都不再重要。这意味着策略的优化只考虑到时间 $\\( N \\)$ 为止。\n",
    "   - 对于所有 $\\( k \\)$，效用函数 $\\( U_h([s_0, s_1, ..., s_{N+k}]) \\)$ 与 $\\( U_h([s_0, s_1, ..., s_N]) \\)$ 相同，这表示超过时间 $\\( N \\)$ 的状态不会影响效用计算。\n",
    "   - 这通常导致非平稳最优策略（non-stationary optimal policies），其中最优策略依赖于特定的时间点，因为随着时间的推移，策略可能会改变。\n",
    "\n",
    "2. **无限时间范围（Infinite Horizon）**：\n",
    "   - 在这种情境下，策略被认为是平稳的（stationary），这意味着最优策略不会随时间变化，即在任何状态下采取的最优行动都是恒定的。\n",
    "   - 这并不意味着所有状态序列都是无限的，而是说没有固定的截止时间。无限时间范围允许策略优化考虑长远的效用最大化。\n",
    "   - 如果两个状态序列 $\\( [s_0, s_1, s_2, ...] \\)$ 和 $\\( [s'_0, s'_1, s'_2, ...] \\)$ 从相同的状态 $\\( s_0 = s'_0 \\)$ 开始，那么这两个序列应该按照相同的方式进行偏好排序，因为时间在状态上的位置并不重要。\n",
    "\n",
    "根据时间范围的不同来定义和计算Utility，这直接影响了最优策略的确定和策略的性质（是否平稳）。在有限时间范围内，策略和效用计算必须考虑时间的流逝，而在无限时间范围内，策略是时间不变的，效用是基于长期的预期回报来计算的。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aab77e2f4022ccad"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Rewards\n",
    "在马尔可夫决策过程（MDP）中，根据不同的情境和需求，对奖励的处理方式有几种不同的概念：\n",
    "\n",
    "#### 加性奖励（Additive Rewards）\n",
    "加性奖励是一种最简单的奖励结构，它假设一个序列中每个状态的奖励对总效用的贡献是独立的，而且每个奖励的重要性是相等的。如果一个状态序列是 $\\( [s_0, s_1, s_2, \\ldots] \\)$，那么这个序列的总效用 $\\( U \\)$ 就是序列中所有状态的奖励 $\\( R \\)$ 的和：\n",
    "\n",
    "$$ U([s_0, s_1, s_2, \\ldots]) = R(s_0) + R(s_1) + R(s_2) + \\ldots $$\n",
    "\n",
    "这种方法通常在有限时间范围（finite horizon）的情况下使用，即当我们只关注在固定时间步长内累积的奖励时。\n",
    "\n",
    "\n",
    "#### 无限时间范围奖励（Infinite Horizon Rewards）\n",
    "无限时间范围奖励是指当一个策略在无限的时间内执行时，智能体所获得的总奖励。在没有贴现的情况下，如果奖励是正数，总奖励可能会趋向于无限大。因此，通常在无限时间范围的情况下会使用贴现奖励来确保总效用是有界的。如果使用贴现，即使在无限时间范围内，总效用也会收敛到一个有限值，这允许我们对长期效用进行合理的计算和比较。\n",
    "\n",
    "在实际应用中，这些概念帮助我们设计策略，以最大化即时奖励和长期收益之间的平衡。选择哪种奖励方法取决于应用场景、目标以及对即时和未来奖励的相对重视程度。\n",
    "这张图片讨论了在无限时间范围内的奖励（infinite horizon rewards）的问题，并提供了三种解决方案：\n",
    "\n",
    "1.**贴现奖励（Discounted Rewards）**：\n",
    "在无限时间范围（infinite horizon）的情境中，为了避免无限累积的奖励导致的数学问题（例如，总效用无限大），通常会引入一个贴现因子 $\\( \\gamma \\)$（其中 $\\( 0 \\leq \\gamma < 1 \\)$）。这样，随着时间的推移，奖励的现值会减少。这反映了未来奖励相对于即时奖励的减少价值，也符合经济学中的时间价值概念。贴现奖励计算如下：\n",
    "\n",
    "$$ U([s_0, s_1, s_2, \\ldots]) = R(s_0) + \\gamma R(s_1) + \\gamma^2 R(s_2) + \\ldots = \\sum_{t=0}^{\\infty} \\gamma^t R(s_t) $$\n",
    "\n",
    "在这个公式中，$\\( \\gamma^t \\)$ 表示在时间步长 $\\( t \\)$ 的奖励被贴现了 $\\( t \\)$ 次。\n",
    "如果所有奖励都被限制在 $\\( \\pm R_{\\text{max}} \\)$ 范围内，那么这个级数可以被上限为一个几何级数（geometric sequence）的和，其总和为：\n",
    "     $$ \\frac{R_{\\text{max}}}{1 - \\gamma} $$\n",
    "\n",
    "2.**适当策略（Proper Policies）**：\n",
    "   - 如果我们采取适当的策略，也就是说最终一定会访问到终止状态，那么即使在没有贴现的情况下，加性奖励也是有限的。这是因为一旦访问到终止状态，后续就不会有新的奖励加入。\n",
    "\n",
    "3.**平均奖励（Average Reward）**：\n",
    "   - 另一种方法是比较每个时间步的平均奖励。这种方法通常用于策略评估，特别是在奖励结构为固定或周期性时。\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1baa6b8fbd04dd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "这张图片讨论了在无限时间范围内的奖励（infinite horizon rewards）的问题，并提供了三种解决方案：\n",
    "\n",
    "1. **贴现奖励（Discounted Rewards）**：\n",
    "   - 使用贴现奖励可以保证即使在无限时间范围内，效用值也是有限的。如果贴现因子 \\( \\gamma \\) 小于 1，并且奖励有一个最大值 \\( R_{\\text{max}} \\)，那么效用的计算可以通过一个无穷级数求和来完成：\n",
    "     $$ U_h([s_0, s_1, s_2, \\ldots]) = \\sum_{t=0}^{\\infty} \\gamma^t R(s_t) $$\n",
    "   - 如果所有奖励都被限制在 $\\( \\pm R_{\\text{max}} \\)$ 范围内，那么这个级数可以被上限为一个几何级数（geometric sequence）的和，其总和为：\n",
    "     $$ \\frac{R_{\\text{max}}}{1 - \\gamma} $$\n",
    "   - 这说明了无论状态序列如何，贴现奖励的总和将是有限的，因此我们可以比较不同策略的效用。\n",
    "\n",
    "2. **适当策略（Proper Policies）**：\n",
    "   - 如果我们采取适当的策略，也就是说最终一定会访问到终止状态，那么即使在没有贴现的情况下，加性奖励也是有限的。这是因为一旦访问到终止状态，后续就不会有新的奖励加入。\n",
    "\n",
    "3. **平均奖励（Average Reward）**：\n",
    "   - 另一种方法是比较每个时间步的平均奖励。这种方法通常用于策略评估，特别是在奖励结构为固定或周期性时。\n",
    "\n",
    "图片强调了在处理无限时间范围奖励时必须考虑的问题，以及如何通过不同的方法来确保奖励的可比较性和合理性。在实践中，选择哪种方法取决于具体的应用场景和策略评估的要求。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "161f268f0e1edf4e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Example Problem\n",
    "一个决策游戏，其中包含两种策略：“停留（stay）”和“退出（quit）”。游戏规则如下：\n",
    "\n",
    "- 每一轮游戏，你可以选择“停留”或“退出”。\n",
    "- 如果选择“退出”，游戏立即结束，并且你获得£10。\n",
    "- 如果选择“停留”，你在那一轮获得£4，并且接下来会掷一个六面骰子。\n",
    "  - 如果骰子掷出的结果是1或2，游戏结束。\n",
    "  - 如果掷出的结果是3到6，游戏继续进行下一轮。\n",
    "\n",
    "问题是要求计算如果遵循“停留”策略和“退出”策略时的期望效用（expected utility）。\n",
    "\n",
    "要计算“停留”策略的期望效用，我们需要考虑每一轮游戏继续的概率。掷出1或2结束游戏的概率是2/6，因此每一轮游戏继续的概率是4/6。只要游戏继续，每轮就可以获得£4。这是一个无限时间范围问题，但由于每一轮结束游戏的概率是固定的，我们可以计算出平均每轮获得的期望收益。\n",
    "\n",
    "遵循“退出”策略的期望效用很简单，就是£10，因为这是一个确定的数额，不涉及任何随机性。\n",
    "\n",
    "现在，让我们计算“停留”策略的期望效用。\n",
    "\n",
    "遵循“停留”策略时，期望效用大约是£12。这个结果是通过计算每轮获得£4的期望值的几何级数得到的。(Sum=4/(1-4/6)=12)\n",
    "\n",
    "相比之下，如果我们遵循“退出”策略，期望效用则是固定的£10。\n",
    "\n",
    "综合比较两种策略，“停留”策略具有更高的期望效用。因此，如果只从期望效用的角度来看，选择“停留”是一个更好的策略。当然，这不考虑玩家对风险的偏好，有些玩家可能会因为风险规避而选择“退出”策略。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b2eb0edf1cf1d958"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. Value Iteration\n",
    "值迭代（Value Iteration）是一种在强化学习和马尔可夫决策过程（MDP）中用于寻找最优策略的算法。该算法的目标是计算出每个状态的最优价值函数，即从该状态开始并遵循最优策略可以获得的最大期望回报。\n",
    "\n",
    "值迭代算法基于贝尔曼最优性方程，该方程提供了一个递归的方式来更新状态的价值。具体来说，对于MDP中的每一个状态 $\\( s \\)$，贝尔曼最优性方程定义了最优价值函数 $\\( V^*(s) \\)$如下：\n",
    "\n",
    "$$\n",
    "V^*(s) = \\max_a \\sum_{s'} P(s' \\mid s, a) [R(s, a, s') + \\gamma V^*(s')]\n",
    "$$\n",
    "\n",
    "其中：\n",
    "- $\\( a \\)$ 是在状态$ \\( s \\)$ 可能采取的行动。\n",
    "- $\\( s' \\)$ 是行动 $\\( a \\)$ 可能导致的下一个状态。\n",
    "- $\\( P(s' \\mid s, a) \\)$ 是从状态 $\\( s \\)$ 采取行动 $\\( a \\)$ 转移到状态 $\\( s' \\)$ 的概率。\n",
    "- $\\( R(s, a, s') \\)$ 是从状态 $\\( s \\)$ 采取行动 $\\( a \\)$ 转移到状态 $\\( s' \\)$ 时获得的即时奖励。\n",
    "- $\\( \\gamma \\)$ 是贴现因子，用于计算未来奖励的当前价值。\n",
    "- $\\( V^*(s') \\)$ 是状态 $\\( s' \\)$ 的最优价值函数。\n",
    "\n",
    "值迭代通过以下步骤进行：\n",
    "\n",
    "1. **初始化**：为所有状态 $\\( s \\)$ 设置一个初始价值，通常是0。\n",
    "\n",
    "2. **迭代更新**：对每个状态 $\\( s \\)$，计算贝尔曼最优性方程右侧的表达式，并更新 $\\( s \\)$ 的价值为这个表达式的结果。这个过程被重复进行，直到价值函数的变化小于一个预定的阈值，这通常表示收敛到最优价值函数。\n",
    "\n",
    "3. **终止**：当价值函数的变化小于阈值时，算法停止迭代。此时，我们认为价值函数已经收敛到最优解。\n",
    "\n",
    "4. **输出最优策略**：一旦价值函数收敛，我们可以通过选择在每个状态 $\\( s \\)$ 都能获得最大价值的行动来确定最优策略。\n",
    "\n",
    "值迭代算法的优势在于它的简单性和直接性，它能够在不需要模型的全面了解的情况下，通过迭代改进直接逼近最优解。这使它成为寻找最优策略的强大工具，特别是在状态空间较大或模型未完全知晓时。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "402ad7555d95a697"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bellman Equation\n",
    "贝尔曼等式（Bellman Equation）是动态规划（Dynamic Programming）中的一个核心概念，它为解决序列决策问题提供了一个递归分解的方法。贝尔曼等式通常用于强化学习（Reinforcement Learning）和动态规划中，以描述在特定状态下的最优策略。\n",
    "\n",
    "在强化学习的背景下，贝尔曼等式描述了一个策略下的状态价值函数（Value Function）或动作价值函数（Action-Value Function），它们分别表示在给定状态下遵循特定策略的期望回报（Expected Return），或在给定状态和动作下遵循特定策略的期望回报。\n",
    "\n",
    "对于状态价值函数 $\\( V^\\pi(s) \\)$，贝尔曼等式表达为：\n",
    "$$\\[ V^\\pi(s) = \\sum_{a} \\pi(a|s) \\sum_{s',r} P(s',r|s,a) \\left[ r + \\gamma V^\\pi(s') \\right] \\]$$\n",
    "这里，$\\( \\pi(a|s) \\)$ 是在状态 $\\( s \\)$ 下采取动作 $\\( a \\)$ 的策略概率，$\\( P(s',r|s,a) \\)$ 是从状态 $\\( s \\)$ 通过动作 $\\( a \\)$转移到状态 $\\( s' \\)$ 并得到奖励 $\\( r \\)$ 的概率，$\\( \\gamma \\)$ 是折扣因子，表示未来奖励的当前价值。\n",
    "\n",
    "对于动作价值函数 $\\( Q^\\pi(s, a) \\)$，贝尔曼等式则表达为：\n",
    "$$\\[ Q^\\pi(s, a) = \\sum_{s', r} P(s', r|s, a) \\left[ r + \\gamma \\sum_{a'} \\pi(a'|s') Q^\\pi(s', a') \\right] \\]$$\n",
    "\n",
    "最优策略（Optimal Policy）是指在所有策略中能够为每个状态获得最大期望回报的策略。对于最优策略 $\\( \\pi^* \\)$，其对应的最优状态价值函数 $\\( V^*(s) \\)$ 和最优动作价值函数 $\\( Q^*(s, a) \\)$ 分别满足以下贝尔曼最优等式：\n",
    "\n",
    "对于 $\\( V^*(s) \\)$：\n",
    "$$\\[ V^*(s) = \\max_a \\sum_{s', r} P(s', r|s, a) \\left[ r + \\gamma V^*(s') \\right] \\]$$\n",
    "\n",
    "对于 $\\( Q^*(s, a) \\)$：\n",
    "$$\\[ Q^*(s, a) = \\sum_{s', r} P(s', r|s, a) \\left[ r + \\gamma \\max_{a'} Q^*(s', a') \\right] \\]$$\n",
    "\n",
    "贝尔曼最优等式通过选择最大化期望回报的动作来计算最优价值函数，从而指导最优策略的形成。在实际应用中，这些等式可以通过各种算法，如值迭代（Value Iteration）或策略迭代（Policy Iteration），来近似求解最优策略和价值函数。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d90c7050dcca98f8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. Policy Iteration\n",
    "策略迭代（Policy Iteration）,这是一种在马尔可夫决策过程（MDP）中找到最优策略的方法。策略迭代算法包括两个主要步骤：策略评估（Policy Evaluation）和策略改进（Policy Improvement），并且这两个步骤会交替进行直到策略收敛。\n",
    "\n",
    "1. **策略评估**：对于给定的策略 $\\( \\pi_i \\)$，计算其对应的状态价值函数 $\\( U_i \\)$。状态价值函数 $\\( U^{\\pi_i} \\)$ 描述了遵循策略 $\\( \\pi_i \\)$ 时，从每个状态开始的期望回报。在这一步中，算法会评估当前策略的效果，即如果该策略被执行，每个状态的效用（或价值）是多少。\n",
    "\n",
    "2. **策略改进**：在评估了策略 $\\( \\pi_i \\)$ 后，算法会寻找一个新的策略 $\\( \\pi_{i+1} \\)$，该策略会在每个状态上采用能够产生最大期望效用（MEU）的动作。这一步通常涉及到对每个可能的动作进行一步前瞻（one-step look-ahead），基于当前的效用函数 $\\( U_i \\)$ 来计算每个动作的期望效用，并选择最优的动作来更新策略。\n",
    "\n",
    "算法会不断重复这两步，直到在策略改进步骤中不再有任何改变，即找到了最优策略。这时候，状态的效用不再改变，算法结束。这种算法的一个重要思想是：如果一个动作明显优于其他动作，那么状态的确切效用值并不需要非常精确，因为最优动作的选择将不受影响。策略迭代可能需要多次策略评估和改进，但每次都会得到一个保证不比前一个策略差的策略。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd356bc529facfa3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Policy Iteration vs Value Iteration\n",
    "\n",
    "策略迭代（Policy Iteration）和值迭代（Value Iteration）都是用来求解马尔可夫决策过程（MDP）中最优策略的动态规划算法。它们都是基于贝尔曼最优方程，但是在计算过程中有所不同。\n",
    "\n",
    "- **效率**：值迭代在计算上通常更高效，因为它不需要在每次迭代中都进行策略评估直至收敛。\n",
    "- **收敛性**：策略迭代可能在更少的迭代次数后收敛，但每次迭代更耗时。值迭代每次迭代更快，但可能需要更多的迭代次数。\n",
    "- **实现复杂度**：策略迭代在实现上可能更为复杂，因为需要两个不同的步骤。值迭代实现起来通常更简单，只需要一个迭代步骤。\n",
    "\n",
    "总的来说，选择策略迭代还是值迭代取决于具体应用场景和对效率的要求。在一些情况下，策略迭代可能因为在早期迭代就找到了较好的策略而更有优势，而值迭代则因其实现的简洁性和在某些问题上的计算效率而受到青睐。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7dddcf8433b07566"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
