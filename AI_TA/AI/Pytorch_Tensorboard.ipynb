{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# TensorBoard\n",
    "TensorBoard是一个可视化工具，用于查看神经网络训练过程中的各种指标，如损失和准确率。TensorBoard还可以可视化计算图，以及在训练过程中的图像、音频和文本数据。TensorBoard是TensorFlow自带的工具，可以直接使用。同时也集成到了PyTorch中，可以通过PyTorch的接口使用TensorBoard。\n",
    "在PyTorch框架中使用TensorBoard进行可视化和模型评估是一个很好的方式，可以帮助你更直观地理解模型的训练过程和性能。下面我会简要介绍如何在PyTorch中使用TensorBoard进行这些任务。\n",
    "\n",
    "### 安装TensorBoard\n",
    "\n",
    "首先，确保你已经安装了TensorBoard。如果还没有安装，可以通过pip安装：\n",
    "\n",
    "```bash\n",
    "pip install tensorboard\n",
    "```\n",
    "\n",
    "### 使用TensorBoard\n",
    "\n",
    "1. **导入必要的库**\n",
    "\n",
    "   在你的Python脚本或Jupyter笔记本中，首先导入必要的库：\n",
    "\n",
    "   ```python\n",
    "   import torch\n",
    "   from torch.utils.tensorboard import SummaryWriter\n",
    "   ```\n",
    "\n",
    "2. **创建`SummaryWriter`实例**\n",
    "\n",
    "   `SummaryWriter`是TensorBoard在PyTorch中的主要接口，用于记录和导出事件文件（包含日志信息）。你可以在你的代码中创建一个`SummaryWriter`实例：\n",
    "\n",
    "   ```python\n",
    "   writer = SummaryWriter('runs/你的实验名')\n",
    "   ```\n",
    "\n",
    "   这将创建一个目录（如果不存在的话），用于存储所有的TensorBoard日志文件。\n",
    "\n",
    "3. **记录数据**\n",
    "\n",
    "   在训练过程中，你可以使用`writer`来记录你感兴趣的信息，比如损失、准确率、特定层的权重和偏差等。这里有几个常用的函数：\n",
    "\n",
    "   - `writer.add_scalar('标签', 数值, 步数)`：用于记录标量值，如损失或准确率。\n",
    "   - `writer.add_histogram('标签', tensor, 步数)`：用于记录参数或梯度分布。\n",
    "   - `writer.add_image('标签', 图像, 步数)`：用于记录图像数据，例如输入图像或特征图。\n",
    "   - `writer.add_graph(model, 输入数据)`：用于可视化模型架构。\n",
    "\n",
    "   例如，记录训练过程中的损失：\n",
    "\n",
    "   ```python\n",
    "   for epoch in range(num_epochs):\n",
    "       for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "           # 运行你的训练步骤\n",
    "           loss = train_step(data, targets)\n",
    "           \n",
    "           # 记录损失\n",
    "           writer.add_scalar('训练损失', loss, epoch * len(train_loader) + batch_idx)\n",
    "   ```\n",
    "\n",
    "4. **启动TensorBoard**\n",
    "\n",
    "   训练过程中或训练完成后，你可以通过命令行启动TensorBoard来查看记录的数据：\n",
    "\n",
    "   ```bash\n",
    "   tensorboard --logdir=runs\n",
    "   ```\n",
    "\n",
    "   然后按照命令行中显示的URL（通常是http://localhost:6006）在浏览器中打开TensorBoard界面。\n",
    "\n",
    "5. **分析TensorBoard界面**\n",
    "\n",
    "   在TensorBoard界面中，你可以看到不同的选项卡，如Scalars（标量）、Graphs（图结构）、Distributions（分布）等，用于查看不同类型的数据。\n",
    "\n",
    "通过这些步骤，你可以在PyTorch中有效地使用TensorBoard来可视化训练过程和评估模型性能。这不仅可以帮助你理解模型在训练过程中的行为，还能帮助你发现潜在的问题和改进点。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "392170de3d56b1f9"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1105a6272e18832"
  },
  {
   "cell_type": "markdown",
   "source": [
    "在实际操作中，是否使用每个批次（batch）还是每个周期（epoch）来记录损失（loss）和准确率（accuracy），通常取决于你的具体需求和场景。以下是两种方法的一些考虑因素：\n",
    "\n",
    "#### 每个批次记录（Batch-level Logging）：\n",
    "- **细粒度观察**：记录每个批次的损失和准确率可以提供关于模型训练过程的更细粒度的视图。这对于识别和诊断训练过程中的问题非常有用，例如损失的波动或异常值。\n",
    "- **快速反馈**：在训练的早期阶段，查看每个批次的性能可以更快地提供反馈，有助于快速调整超参数或早期发现问题。\n",
    "- **大型数据集**：对于非常大的数据集，一个epoch可能需要很长时间才能完成。在这种情况下，按批次记录可以更频繁地了解模型性能。\n",
    "\n",
    "#### 每个周期记录（Epoch-level Logging）：\n",
    "- **趋势分析**：记录每个周期的平均损失和准确率可以提供关于模型性能随时间变化的整体趋势。这有助于评估模型是否在改进，以及它是否已经收敛。\n",
    "- **简洁清晰**：每个周期记录生成的数据量少于每个批次记录，这使得趋势更易于观察和分析，尤其是在训练多个周期时。\n",
    "- **资源效率**：记录和存储较少的数据点（每个周期的数据而不是每个批次的数据）可以降低存储和计算开销。\n",
    "\n",
    "#### 综合考虑：\n",
    "- 在实践中，两种方法可以根据需要结合使用。例如，你可以在训练初期按批次记录，以快速诊断和调整，然后在训练稳定后转为按周期记录，以减少日志数据量并集中关注长期趋势。\n",
    "- 另外，也可以同时记录每个批次和每个周期的指标，但选择在TensorBoard中展示哪些数据，以便于分析和避免信息过载。\n",
    "\n",
    "选择哪种方法取决于你的具体目标、数据集的大小以及训练过程中资源的可用性。在一些情况下，可能需要更频繁的反馈来快速调整模型，而在其他情况下，关注长期趋势可能更为重要。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "55934d7b588e83d8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 示例：使用TensorBoard可视化LeNet-5训练过程\n",
    "1. 定义`train`函数前，使用`SummaryWriter`创建一个`writer`实例：\n",
    "   ```python\n",
    "   writer = SummaryWriter('path')\n",
    "   ```\n",
    "2. 在`train`,`test`的循环中，添加`writer.add_scalar`记录训练和测试的损失和准确率：\n",
    "   ```python\n",
    "   # train\n",
    "   for batch_idx, (data, target) in enumerate(train_loader):\n",
    "       # 训练过程\n",
    "       ...\n",
    "       # batch logging\n",
    "       writer.add_scalar('Training/Loss', loss.item(), epoch * len(train_loader) + batch_idx)\n",
    "       writer.add_scalar('Training/Accuracy', accuracy(output, target), epoch * len(train_loader) + batch_idx)\n",
    "   \n",
    "    # test\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "   \n",
    "            # 测试过程\n",
    "            ...\n",
    "            # epoch logging\n",
    "            writer.add_scalar('Test/Loss', loss.item(), epoch)\n",
    "            writer.add_scalar('Test/Accuracy', accuracy(output, target), epoch)\n",
    "            # batch logging  \n",
    "            writer.add_scalar('Test/Loss batch', loss.item(), epoch * len(test_loader) + batch_idx)\n",
    "            writer.add_scalar('Test/Accuracy batch', accuracy(output, target), epoch * len(test_loader) + batch_idx)\n",
    "   \n",
    "\n",
    "   ```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1ff67a9a789284d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "在TensorBoard中，确保每次训练运行时步数能够连续递增，或者为每次运行设置不同的`run`名称，你可以采取以下措施：\n",
    "\n",
    "1. **为每次运行设置不同的`run`目录**：\n",
    "   - 当你初始化`SummaryWriter`时，可以为每次运行指定不同的目录名。这样，TensorBoard会将每次运行视为不同的实验。\n",
    "   ```python\n",
    "   from torch.utils.tensorboard import SummaryWriter\n",
    "   import datetime\n",
    "\n",
    "   # 使用当前时间作为run的名称，确保每次都是唯一的\n",
    "   current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "   writer = SummaryWriter('runs/experiment_' + current_time)\n",
    "   ```\n",
    "\n",
    "2. **确保步数连续递增**：\n",
    "   - 如果你要在同一个`run`目录下继续之前的训练，你需要记录上次训练结束时的步数，并从该步数继续。\n",
    "   - 通常，你可以在训练的每个epoch结束时保存模型的状态和当前的步数。当从检查点恢复时，加载这个步数并从它继续。\n",
    "   ```python\n",
    "   # 假设你已经有了一个检查点\n",
    "   checkpoint = torch.load('path_to_checkpoint.pth')\n",
    "   model.load_state_dict(checkpoint['model_state_dict'])\n",
    "   optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "   start_epoch = checkpoint['epoch']\n",
    "   global_step = checkpoint['global_step']\n",
    "\n",
    "   for epoch in range(start_epoch, num_epochs):\n",
    "       for batch_idx, (data, target) in enumerate(train_loader):\n",
    "           # ...\n",
    "           global_step += 1\n",
    "           # 使用global_step作为步数\n",
    "           writer.add_scalar('Training Loss', loss.item(), global_step)\n",
    "           writer.add_scalar('Training Accuracy', accuracy(output, target), global_step)\n",
    "   ```\n",
    "\n",
    "   在这个例子中，`global_step`在所有epoch和batch之间保持连续。\n",
    "\n",
    "采取这些措施可以帮助你在TensorBoard中清晰地区分不同的运行，并确保在一个运行内部，步数是单调递增的，从而避免图形显示问题。"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4230f651ca6abc02"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.308878\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.357819\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.087530\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.288389\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.136461\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.208771\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.152856\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.067399\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.329961\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.023598\n",
      "\n",
      "Test set: Average loss: 0.0564, Average Accuracy: 98.16%\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.036860\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.023307\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.151743\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.026296\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.011592\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.028307\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.139935\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.074571\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.189150\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.010960\n",
      "\n",
      "Test set: Average loss: 0.0541, Average Accuracy: 98.24%\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.008609\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.004536\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.050133\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.016177\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.175095\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.002296\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.051115\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.039617\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.081546\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.023049\n",
      "\n",
      "Test set: Average loss: 0.0426, Average Accuracy: 98.55%\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.078853\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.005461\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.040452\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.002405\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.029224\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.002713\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.005829\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.008995\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.021312\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.006257\n",
      "\n",
      "Test set: Average loss: 0.0389, Average Accuracy: 98.84%\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.023885\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.019887\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.022842\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.010413\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.010124\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.002277\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.072885\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.058520\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.106796\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.067619\n",
      "\n",
      "Test set: Average loss: 0.0368, Average Accuracy: 98.85%\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.009945\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.016614\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.006845\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.021554\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.001611\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.009117\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.088430\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.002296\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.004694\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.034483\n",
      "\n",
      "Test set: Average loss: 0.0367, Average Accuracy: 98.84%\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.019109\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.018454\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.023608\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.005821\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.034637\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.034109\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.000223\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.103464\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.021798\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.005004\n",
      "\n",
      "Test set: Average loss: 0.0318, Average Accuracy: 99.01%\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.049060\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.036078\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.043560\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.009380\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.040058\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.003042\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.056799\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.001317\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.014534\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.039248\n",
      "\n",
      "Test set: Average loss: 0.0378, Average Accuracy: 98.90%\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.026870\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.004530\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.000431\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.020067\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.045622\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.019514\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.000484\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.002776\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.012138\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.007831\n",
      "\n",
      "Test set: Average loss: 0.0375, Average Accuracy: 98.92%\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.007339\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.004152\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.039509\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.002337\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.000267\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.001009\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.081097\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.008696\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.000686\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.000251\n",
      "\n",
      "Test set: Average loss: 0.0396, Average Accuracy: 98.86%\n",
      "\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.002812\n",
      "Train Epoch: 11 [6400/60000 (11%)]\tLoss: 0.030947\n",
      "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 0.016663\n",
      "Train Epoch: 11 [19200/60000 (32%)]\tLoss: 0.003367\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.000356\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 0.000605\n",
      "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 0.045170\n",
      "Train Epoch: 11 [44800/60000 (75%)]\tLoss: 0.002508\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.000795\n",
      "Train Epoch: 11 [57600/60000 (96%)]\tLoss: 0.001027\n",
      "\n",
      "Test set: Average loss: 0.0450, Average Accuracy: 98.83%\n",
      "\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.004648\n",
      "Train Epoch: 12 [6400/60000 (11%)]\tLoss: 0.001096\n",
      "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 0.000913\n",
      "Train Epoch: 12 [19200/60000 (32%)]\tLoss: 0.012603\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.003309\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 0.019141\n",
      "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 0.000717\n",
      "Train Epoch: 12 [44800/60000 (75%)]\tLoss: 0.008685\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.049354\n",
      "Train Epoch: 12 [57600/60000 (96%)]\tLoss: 0.001117\n",
      "\n",
      "Test set: Average loss: 0.0481, Average Accuracy: 98.84%\n",
      "\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.007407\n",
      "Train Epoch: 13 [6400/60000 (11%)]\tLoss: 0.000532\n",
      "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 0.004212\n",
      "Train Epoch: 13 [19200/60000 (32%)]\tLoss: 0.003342\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.020269\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 0.008425\n",
      "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 0.019223\n",
      "Train Epoch: 13 [44800/60000 (75%)]\tLoss: 0.002651\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.000318\n",
      "Train Epoch: 13 [57600/60000 (96%)]\tLoss: 0.014476\n",
      "\n",
      "Test set: Average loss: 0.0580, Average Accuracy: 98.72%\n",
      "\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.000191\n",
      "Train Epoch: 14 [6400/60000 (11%)]\tLoss: 0.001304\n",
      "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 0.003678\n",
      "Train Epoch: 14 [19200/60000 (32%)]\tLoss: 0.000459\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.003273\n",
      "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 0.000610\n",
      "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 0.000066\n",
      "Train Epoch: 14 [44800/60000 (75%)]\tLoss: 0.077352\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.000130\n",
      "Train Epoch: 14 [57600/60000 (96%)]\tLoss: 0.000316\n",
      "\n",
      "Test set: Average loss: 0.0390, Average Accuracy: 99.08%\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn, torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "# LeNet-5示例\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "\n",
    "        # 定义第卷积层C1, 输入通道为1，输出通道为6，卷积核大小为5(6个5x5的卷积核)\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
    "        # 定义池化层S2，最大池化，窗口大小为2\n",
    "        self.max_pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # 定义第卷积层C3, 输入通道为6，输出通道为16，卷积核大小为5(16个5x5的卷积核)\n",
    "        self.conv3 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        # 定义池化层S4，最大池化，窗口大小为2\n",
    "        self.max_pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # 定义第卷积层C5，可以视为全连接层, 输入通道为16*5*5，输出通道为120。\n",
    "        self.fc5 = nn.Linear(16 * 5 * 5, 120)\n",
    "        # 定义全连接层F6，输入维度为120，输出维度为84\n",
    "        self.fc6 = nn.Linear(120, 84)\n",
    "        # 定义输出层，输入维度为84，输出维度为10\n",
    "        self.fc7 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 通过C1后使用ReLU激活函数\n",
    "        x = F.relu(self.conv1(x))\n",
    "        # 通过S2\n",
    "        x = self.max_pool2(x)\n",
    "        # 通过C3后使用ReLU激活函数\n",
    "        x = F.relu(self.conv3(x))\n",
    "        # 通过S4\n",
    "        x = self.max_pool4(x)\n",
    "        # 展平特征图\n",
    "        # x.view(-1, 16*5*5) 这行代码的作用是将前面卷积层和池化层处理后的多维特征图展平为一维向量。这一步是必要的，因为全连接层（如 self.fc5）期望其输入是一维向量形式，而不是多维特征图。\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        # 通过C5后使用ReLU激活函数\n",
    "        x = F.relu(self.fc5(x))\n",
    "        # 通过F6后使用ReLU激活函数\n",
    "        x = F.relu(self.fc6(x))\n",
    "        # 输出层\n",
    "        x = self.fc7(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 加载数据集\n",
    "batch_size = 64\n",
    "\n",
    "# MNIST 数据集的转换器，首先将数据转换为张量，然后标准化\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),  # LeNet-5 需要 32x32 的输入\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# 下载并加载训练集\n",
    "train_dataset = datasets.MNIST(root='C:\\\\Users\\\\yhb\\\\MscProject\\\\AI_TA\\\\data', train=True, download=False, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 下载并加载测试集\n",
    "test_dataset = datasets.MNIST(root='C:\\\\Users\\\\yhb\\\\MscProject\\\\AI_TA\\\\data', train=False, download=False, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# 初始化 SummaryWriter\n",
    "# 使用当前时间作为run的名称，确保每次都是唯一的\n",
    "current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "writer = SummaryWriter('runs/experiment_' + current_time)\n",
    "\n",
    "# 计算准确率的函数\n",
    "def accuracy(output, target):\n",
    "    # 获取预测最高分的索引\n",
    "    pred = output.argmax(dim=1, keepdim=True)\n",
    "    correct = pred.eq(target.view_as(pred)).sum().item()\n",
    "    return correct / len(target)\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # 添加TensorBoard日志记录，传入正确的accuracy函数调用\n",
    "        # 这里是batch logging，即每个batch结束后记录一次\n",
    "        writer.add_scalar('Training/Loss', loss.item(), epoch * len(train_loader) + batch_idx)\n",
    "        writer.add_scalar('Training/Accuracy', accuracy(output, target), epoch * len(train_loader) + batch_idx)\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(\n",
    "                f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
    "\n",
    "def test(model, device, test_loader, epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            # 分别使用epoch和epoch * len(test_loader) + batch_idx作为步数\n",
    "            # step=epoch：每个epoch结束后记录一次\n",
    "            # step=epoch * len(test_loader) + batch_idx：每个batch结束后记录一次\n",
    "            \n",
    "            # epoch logging，即每个epoch结束后记录一次\n",
    "            writer.add_scalar('Test/Loss', loss.item(), epoch)\n",
    "            writer.add_scalar('Test/Accuracy', accuracy(output, target), epoch)\n",
    "            # batch logging\n",
    "            writer.add_scalar('Test/Loss batch', loss.item(), epoch * len(test_loader) + batch_idx)\n",
    "            writer.add_scalar('Test/Accuracy batch', accuracy(output, target), epoch * len(test_loader) + batch_idx)\n",
    "            \n",
    "            # 计算总的正确预测数\n",
    "            correct += accuracy(output, target) * len(data)\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    \n",
    "    # 添加总的测试准确率到TensorBoard\n",
    "    # 这个准确率是整个测试集上的准确率，即每个epoch结束后的准确率\n",
    "    writer.add_scalar('Average Test Accuracy', test_accuracy, epoch)\n",
    "  \n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Average Accuracy: {test_accuracy:.2f}%\\n')\n",
    "\n",
    "# 设置设备并实例化模型，定义损失函数和优化器，开始训练和测试\n",
    "device = torch.device(\"cuda\")\n",
    "model = LeNet5().to(device)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "for epoch in range(1, 15): # 训练 15 次\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader, epoch)\n",
    "\n",
    "# 关闭 SummaryWriter\n",
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-08T20:08:31.912226900Z",
     "start_time": "2024-02-08T20:05:52.998607100Z"
    }
   },
   "id": "8b76111db7b1c9da",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 查看TensorBoard\n",
    "在命令行中输入以下命令启动TensorBoard：\n",
    "```bash\n",
    "tensorboard --logdir=absolute_path_to_your_runs\n",
    "```\n",
    "然后在浏览器中打开TensorBoard界面，你将看到类似下面的界面：\n",
    "\n",
    "注意，在使用`writer.add_scalar`记录损失和准确率时，第一个参数`tag`是用于标识这个数据的名称，tag一般格式为`'分类/名称'`，这样可以方便地在TensorBoard中查找和筛选数据。例如，`'Training/Loss'`表示训练过程中的损失，`'Test/Accuracy'`表示测试过程中的准确率。这样在web界面显示图像时，会根据父类进行分类，方便查看。\n",
    "\n",
    "<img src=\"./images/img_29.png\">"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8da7af148ac76ee"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
