{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Text Analytics Coursework: PART 1--Tweet Emotion Classification"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f81c9c24b8d0fcb8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use HuggingFace's datasets library to access the Emotion dataset\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.svm import SVC\n",
    "import string\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import BertTokenizer\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel, TfidfModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Part 1: Tweet Emotion Classification\n",
    "# 1. Load the dataset\n",
    "Use the TweetEval dataset from HuggingFace's datasets library to load the Emotion dataset. The dataset has three splits: train, validation, and test. Load the train, validation, and test splits into separate variables.\n",
    "The labels in the dataset are: [0,1,2,3] which correspond to the emotions: [anger, joy, optimism, sadness]."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6252accd86db369a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "cache_dir = \"./NLP_data_cache\"\n",
    "\n",
    "train_dataset = load_dataset(\n",
    "    \"tweet_eval\",\n",
    "    name=\"emotion\",\n",
    "    split=\"train\",\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "print(f\"Training dataset with {len(train_dataset)} instances loaded\")\n",
    "\n",
    "\n",
    "val_dataset = load_dataset(\n",
    "    \"tweet_eval\",\n",
    "    name=\"emotion\",\n",
    "    split=\"validation\",\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "print(f\"Development/validation dataset with {len(val_dataset)} instances loaded\")\n",
    "\n",
    "\n",
    "test_dataset = load_dataset(\n",
    "    \"tweet_eval\",\n",
    "    name=\"emotion\",\n",
    "    split=\"test\",\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "print(f\"Test dataset with {len(test_dataset)} instances loaded\")\n",
    "\n",
    "# Access the input text and target labels like this...\n",
    "train_texts = train_dataset['text']\n",
    "train_labels = train_dataset['label']\n",
    "\n",
    "val_texts = val_dataset['text']\n",
    "val_labels = val_dataset['label']\n",
    "\n",
    "test_texts = test_dataset['text']\n",
    "test_labels = test_dataset['label']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a2e07d5fa647a0e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_texts[13], train_labels[13]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7b6b08563b6f098",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Non-Neural Classifier\n",
    "## 2.1. Preprocessing\n",
    "### 2.1.1. Text Cleaning\n",
    "\n",
    "The text cleaning step includes removing URLs, @mentions and #hashtags, converting to lowercase, removing punctuation. Lexical annotation and word form reduction by the result of lexical annotation. These steps help to reduce the noise and diversity of text data and improve the performance of text analysis."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c35458438f1c250"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 确保已下载nltk的wordnet和stopwords数据包\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english')) # 加载停用词\n",
    "\n",
    "# 映射nltk的词性标签到wordnet的词性标签。用于词形还原\n",
    "def nltk_pos_tagger(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text, flags=re.MULTILINE) # 移除URLs\n",
    "    text = re.sub(r'\\@\\w+|\\#','', text) # 移除@mentions和hashtags\n",
    "    text = text.lower() # 转换为小写\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation)) # 移除标点符号\n",
    "    \n",
    "    tokens = word_tokenize(text) # 分词\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words and len(word) > 2] # 删除停用词和短词(长度小于等于2)\n",
    "\n",
    "    nltk_tagged = nltk.pos_tag(filtered_tokens)   # 词性标注\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_pos_tagger(x[1])), nltk_tagged) # 转换词性标注\n",
    "    \n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)\n",
    "\n",
    "\n",
    "# 应用词形还原到你的文本数据\n",
    "train_texts_lemmatized = [lemmatize_text(text) for text in train_texts]\n",
    "val_texts_lemmatized = [lemmatize_text(text) for text in val_texts]\n",
    "test_texts_lemmatized = [lemmatize_text(text) for text in test_texts]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c949cf8e7d335f29",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_texts_lemmatized[0:3]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca0de61066bfb547",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1.2. Vectorization\n",
    "Use the TF-IDF vectorizer to convert the text data into numerical form."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "903b32a374981d9a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# TF-IDF向量化\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1,2))\n",
    "\n",
    "# 将训练和验证数据集合并进行向量化，这样可以提高模型的泛化能力\n",
    "X_all_lemmatized = train_texts_lemmatized + val_texts_lemmatized\n",
    "y_all = train_labels + val_labels\n",
    "\n",
    "# 向量化处理\n",
    "x_train_tfidf = tfidf_vectorizer.fit_transform(X_all_lemmatized)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_texts_lemmatized)\n",
    "\n",
    "y_train = y_all\n",
    "\n",
    "# 分割数据集，这里重新分割是因为我们合并了训练集和验证集进行统一的向量化\n",
    "# x_train_tfidf, x_val_tfidf, y_train, y_val = train_test_split(\n",
    "#     X_all_tfidf, y_all, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "857654fdcd34ab16",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2. Classification Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c1177f1a72a234a3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# grid search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10,100],\n",
    "    'gamma': [1, 0.1, 0.01],\n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(SVC(class_weight='balanced'), param_grid, cv=5, n_jobs=-1,verbose=True,scoring='f1_weighted')\n",
    "grid_search.fit(x_train_tfidf, y_train)\n",
    "\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "print(\"Best cross-validation score: \", grid_search.best_score_)\n",
    "print(\"Best estimator: \", grid_search.best_estimator_)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ee718ffb715490e",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 训练SVM模型\n",
    "# C是正则化参数，C越大，正则化越弱，则模型越容易过拟合；gamma是rbf核函数的系数，gamma越大，模型越容易过拟合\n",
    "svm_model = SVC(random_state=42, kernel='rbf', C=1, gamma=1,class_weight='balanced')\n",
    "svm_model.fit(x_train_tfidf, y_train)\n",
    "\n",
    "# 预测验证集\n",
    "y_train_pred = svm_model.predict(x_train_tfidf)\n",
    "# y_val_pred = svm_model.predict(x_val_tfidf)\n",
    "y_test_pred = svm_model.predict(X_test_tfidf)\n",
    "\n",
    "print(\"Train Accuracy: \", accuracy_score(y_train, y_train_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_train, y_train_pred))\n",
    "\n",
    "# # 性能评估\n",
    "# print(\"Validation Accuracy: \", accuracy_score(y_val, y_val_pred))\n",
    "# print(\"Classification Report:\\n\", classification_report(y_val, y_val_pred))\n",
    "\n",
    "# 测试集性能评估\n",
    "print(\"Test Accuracy: \", accuracy_score(test_labels, y_test_pred))\n",
    "print(\"Test Classification Report:\\n\", classification_report(test_labels, y_test_pred))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b045535a2733497",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# cross validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(svm_model, x_train_tfidf, y_all, cv=5, scoring='f1_weighted',n_jobs=-1)\n",
    "print(\"Cross-validation scores: \", scores)\n",
    "print(\"Mean CV F1-score: \", scores.mean())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2bf99963208c5a34",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3 Improving the Model\n",
    "Add Lexicon Features, using NLTK's VADER sentiment analysis tool to extract sentiment scores from the text data. Add these sentiment scores as additional features to the TF-IDF vectorized features."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c036b60517662b82"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# 实例化情感分析器\n",
    "analyser = SentimentIntensityAnalyzer()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91105e8ed4698f48",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "# 初始化VADER情感分析器\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "X_all_lemmatized = train_texts_lemmatized + val_texts_lemmatized\n",
    "y_all = train_labels + val_labels\n",
    "\n",
    "# 计算情感分数 # compound是综合得分\n",
    "x_all_sentiments = [sia.polarity_scores(text)['compound'] for text in X_all_lemmatized]\n",
    "x_test_sentiments = [sia.polarity_scores(text)['compound'] for text in test_texts_lemmatized]\n",
    "\n",
    "x_all_sentiments = np.array(x_all_sentiments).reshape(-1, 1)\n",
    "x_test_sentiments = np.array(x_test_sentiments).reshape(-1, 1)\n",
    "\n",
    "# 合并情感分数和TF-IDF特征\n",
    "# 向量化处理\n",
    "X_all_tfidf = tfidf_vectorizer.fit_transform(X_all_lemmatized)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_texts_lemmatized)\n",
    "\n",
    "X_all = hstack([X_all_tfidf, x_all_sentiments])\n",
    "X_test = hstack([X_test_tfidf, x_test_sentiments])\n",
    "\n",
    "x_train_augmented = X_all\n",
    "y_train = y_all\n",
    "\n",
    "# 分割数据集\n",
    "# x_train_augmented, x_val_augmented, y_train, y_val = train_test_split(\n",
    "#     X_all, y_all, test_size=0.2, random_state=42)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8adee85aab060caf",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 训练SVM模型\n",
    "svm_model_vader = SVC(random_state=42, kernel='linear', C=1, gamma=0.1,class_weight='balanced')\n",
    "svm_model_vader.fit(x_train_augmented, y_train)\n",
    "\n",
    "# 预测验证集\n",
    "y_train_pred = svm_model_vader.predict(x_train_augmented)\n",
    "# y_val_pred = svm_model_vader.predict(x_val_augmented)\n",
    "y_test_pred = svm_model_vader.predict(X_test)\n",
    "\n",
    "print(\"Train Accuracy: \", accuracy_score(y_train, y_train_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_train, y_train_pred))\n",
    "\n",
    "# # 性能评估\n",
    "# print(\"Validation Accuracy: \", accuracy_score(y_val, y_val_pred))\n",
    "# print(\"Classification Report:\\n\", classification_report(y_val, y_val_pred))\n",
    "\n",
    "# 测试集性能评估\n",
    "print(\"Test Accuracy: \", accuracy_score(test_labels, y_test_pred))\n",
    "print(\"Test Classification Report:\\n\", classification_report(test_labels, y_test_pred))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e07ed242dfca142c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 查看分类错误的样本\n",
    "test_df = test_dataset.to_pandas()\n",
    "test_df['pred'] = y_test_pred\n",
    "test_df['gold_label'] = test_labels\n",
    "mistakes = test_df[test_df['pred'] != test_df['gold_label']]\n",
    "mistakes\n",
    "# mistakes.to_csv('./NLP_data_cache/mistakes.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "920502ae44c4dac0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "test_texts[12], test_labels[12]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5118a2a9b112dc56",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Insights\n",
    "\n",
    "Comparing the performance before and after adding the sentiment score feature, it is found that the performance of the classifier has improved. This indicates that the sentiment score feature is useful for the sentiment classification task.\n",
    "\n",
    "This is shown by the significant improvement in Recall for category 1 (Joy) and Precision for category 2 (Optimise). This indicates that the model is better able to recognize these two categories. This may be due to the fact that the sentiment score provides a way to directly quantify the sentiment tendency of the text, helping the model to capture some of the sentiment features that are difficult to express explicitly through the text content alone. This result suggests that for text categorization tasks, combining content analysis and quantitative analysis of sentiment tendencies can effectively improve the prediction accuracy and generalization ability of the model.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5dead10f21f220a6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Deep Learning Classifier\n",
    "## 3.1. Preprocessing\n",
    "### 3.1.1. Length of Texts\n",
    "We found that the maximum length of the text data is 36. As the hist plot can show the distribution of the length of the text data, we can see that most of the text data is less than 25 words. Therefore, we can set the maximum length of the text data to 25."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33b712192277da6c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 查看样本长度的分布\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_texts_len = [len(text.split()) for text in train_texts]\n",
    "val_texts_len = [len(text.split()) for text in val_texts]\n",
    "test_texts_len = [len(text.split()) for text in test_texts]\n",
    "\n",
    "plt.hist(train_texts_len, bins=30, alpha=0.5, label='train')\n",
    "plt.hist(val_texts_len, bins=30, alpha=0.5, label='val')\n",
    "plt.hist(test_texts_len, bins=30, alpha=0.5, label='test')\n",
    "\n",
    "# 找出最长长度\n",
    "max_len = max(max(train_texts_len), max(val_texts_len), max(test_texts_len))\n",
    "max_len"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ebb004858fd6dfaa",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1.2. Tokenization, Encoding, Padding&Truncating and Tensorization\n",
    "Use BERT tokenizer to tokenize the text data. Then convert the tokenized text data into vocabulary index. Finally, pad and tensorize the text data and generate the DataLoader."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "633cee5533952ae7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, DistilBertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "## 注意：使用不同模型要使用对应的tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "\n",
    "def encode_texts(texts):\n",
    "    return tokenizer.batch_encode_plus(\n",
    "        texts,\n",
    "        add_special_tokens=True,  # 添加特殊标记，例如[CLS]和[SEP]\n",
    "        max_length=25,  # 设定最大序列长度\n",
    "        padding='max_length',  # 进行填充以达到相同长度\n",
    "        truncation=True,  # 截断超过最大长度的部分\n",
    "        return_attention_mask=True,  # 返回attention_mask（区分填充和非填充部分）\n",
    "        return_token_type_ids=False,  # 不返回token_type_ids(用于区分两个句子，如问答，本模型不需要)\n",
    "        return_tensors='pt'  # 返回PyTorch张量\n",
    "    )\n",
    "\n",
    "# 对训练、验证和测试数据进行编码\n",
    "encoded_train = encode_texts(train_texts)\n",
    "encoded_val = encode_texts(val_texts)\n",
    "encoded_test = encode_texts(test_texts)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dbd3d27ae9653dac",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}  # 确保不修改原始数据\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# 创建Dataset\n",
    "train_dataset = TextDataset(encoded_train, train_labels)\n",
    "val_dataset = TextDataset(encoded_val, val_labels)\n",
    "test_dataset = TextDataset(encoded_test, test_labels)\n",
    "\n",
    "# 创建DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7b20cde4beb4484",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2. Training the Model\n",
    "Download the pre-trained BERT model and fine-tune it on the emotion classification task."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b53adb1f4012f78d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 使用Focal Loss作为损失函数\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=3.0, alpha=None):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma \n",
    "        self.alpha = alpha \n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        inputs: logits\n",
    "        targets: 真实标签\n",
    "        \"\"\"\n",
    "        BCE_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)  # 计算概率\n",
    "        F_loss = (1 - pt) ** self.gamma * BCE_loss\n",
    "        \n",
    "        if self.alpha is not None:\n",
    "            at = self.alpha[targets]\n",
    "            F_loss = at * F_loss\n",
    "        \n",
    "        return F_loss.mean()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "73ce7d36290fea8c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader, optimizer, scheduler, device, epochs):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_f1score = []  \n",
    "    val_f1score = []\n",
    "    \n",
    "    # 如果需要使用带权重的交叉熵损失函数：\n",
    "    # 类别的样本数量(训练集）\n",
    "    class_counts = torch.tensor([1400, 708, 294, 855])\n",
    "    # 计算每个类别的权重（以最小类别样本数的倒数作为基础）\n",
    "    weights = class_counts.min().float() / class_counts.float()\n",
    "    weights = weights.to(device)\n",
    "    custom_loss_function = CrossEntropyLoss(weight=weights) # 使用带权重的交叉熵损失\n",
    "    # custom_loss_function = FocalLoss(alpha=weights, gamma=3.0) # 使用Focal Loss\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        all_train_preds = []  \n",
    "        all_train_labels = []\n",
    "        \n",
    "        # for batch in tqdm(train_dataloader, desc=\"Training\"):\n",
    "        for batch in train_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()} \n",
    "            # 这行代码遍历从DataLoader迭代器返回的每个批次数据中的所有键值对。这里的batch是一个字典，它的键（k）是字符串，比如'input_ids'、'attention_mask'和'labels'，值（v）是对应的数据张量。{k: v.to(device) for k, v in batch.items()}这部分代码的作用是：\n",
    "            # 对于字典中的每个键值对，将值（即张量）移动到指定的device（例如，GPU或CPU）。\n",
    "            \n",
    "            #使用BERT内置损失函数进行反向传播\n",
    "            outputs = model(**batch)  # **batch将字典解包为关键字参数,等价于model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], labels=batch['labels'])   \n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            \n",
    "            # # # 使用自定义损失函数进行反向传播\n",
    "            # outputs = model(**batch)\n",
    "            # logits = outputs.logits  # 获取模型的logits\n",
    "            # # 计算自定义损失\n",
    "            # loss = custom_loss_function(logits, batch['labels'].to(device))\n",
    "            # # 使用自定义损失进行反向传播\n",
    "            # loss.backward()\n",
    "            \n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # 梯度裁剪（防止梯度爆炸）\n",
    "            \n",
    "            optimizer.step()\n",
    "            # scheduler.step() # 更新学习率\n",
    "            model.zero_grad() \n",
    "            \n",
    "            # 打印当前学习率\n",
    "            # print('Learning rate: ', optimizer.param_groups[0]['lr'])\n",
    "            \n",
    "            # train_preds = torch.argmax(logits, dim=1).flatten() # 使用bert损失函数时，需要注释掉这行代码\n",
    "            train_preds = torch.argmax(outputs.logits, dim=1).flatten()\n",
    "            \n",
    "            all_train_preds.extend(train_preds.cpu().numpy())\n",
    "            all_train_labels.extend(batch['labels'].cpu().numpy())\n",
    "        \n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        train_f1 = f1_score(all_train_labels, all_train_preds, average='macro')\n",
    "        train_f1score.append(train_f1)\n",
    "        \n",
    "        # 验证过程\n",
    "        model.eval()\n",
    "        total_val_loss = 0 # 每个epoch都会重新计算验证集的损失\n",
    "        val_accuracy = []\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        \n",
    "        # for batch in tqdm(val_dataloader, desc=\"Validating\"):\n",
    "        for batch in val_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}  \n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch) \n",
    "                \n",
    "            loss = outputs.loss\n",
    "            total_val_loss += loss.item()\n",
    "            \n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1).flatten()\n",
    "            labels = batch['labels']  # 获取标签进行准确度计算\n",
    "            # 将当前批次的预测和标签追加到列表中\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            val_accuracy.append(accuracy_score(labels.cpu().numpy(), preds.cpu().numpy()))\n",
    "        \n",
    "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        # 计算f1-score\n",
    "        val_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "        val_f1score.append(val_f1)\n",
    "        \n",
    "        # 使用验证集的Macro F1来更新学习率\n",
    "        scheduler.step(val_f1)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss} | Val Loss: {avg_val_loss} | Val Accuracy: {np.mean(val_accuracy)}')\n",
    "        print(classification_report(all_labels, all_preds))\n",
    "        \n",
    "        # 保存最终模型\n",
    "        # if epoch == epochs - 1:\n",
    "        # torch.save(model.state_dict(), './NLP_data_cache/bert_model.pth')\n",
    "        \n",
    "    return train_losses, val_losses, train_f1score, val_f1score,model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ead208038845bd0a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def test(model, test_dataloader, device):\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "    test_accuracy = []\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    error_records = []  # 用于存储错误分类的记录\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    with torch.no_grad():  # 禁用梯度计算\n",
    "        for batch in test_dataloader:\n",
    "            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}  # 保持labels之外的数据移至设备\n",
    "            labels = batch['labels'].cpu().numpy()\n",
    "            outputs = model(**inputs)  # 进行预测\n",
    "            preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()  # 获取预测结果\n",
    "            \n",
    "            test_accuracy.append(accuracy_score(labels, preds))\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "            \n",
    "            # 检查每个预测，并记录错误分类的实例\n",
    "            input_ids = inputs['input_ids'].cpu().numpy()\n",
    "            for input_id, pred, label in zip(input_ids, preds, labels):\n",
    "                if pred != label:\n",
    "                    decoded_sentence = tokenizer.decode(input_id, skip_special_tokens=True)\n",
    "                    error_records.append([decoded_sentence, pred, label])\n",
    "\n",
    "    avg_accuracy = np.mean(test_accuracy)\n",
    "    print(f\"Test Accuracy: {avg_accuracy}\")\n",
    "    print(classification_report(all_labels, all_preds))\n",
    "\n",
    "    # 将错误记录保存到CSV文件\n",
    "    df_errors = pd.DataFrame(error_records, columns=['Text', 'Predicted Label', 'True Label'])\n",
    "    df_errors.to_csv('model_errors.csv', index=False)\n",
    "\n",
    "    print(f\"Saved {len(error_records)} error records to model_errors.csv\")\n",
    "\n",
    "    return all_preds, all_labels, avg_accuracy, error_records"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "881e9aecc538aa47",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Programming\\anaconda3\\envs\\mytorch\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 | Train Loss: 1.2706126290209152 | Val Loss: 1.2672943472862244 | Val Accuracy: 0.4296875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      1.00      0.60       160\n",
      "           1       0.00      0.00      0.00        97\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        89\n",
      "\n",
      "    accuracy                           0.43       374\n",
      "   macro avg       0.11      0.25      0.15       374\n",
      "weighted avg       0.18      0.43      0.26       374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Programming\\anaconda3\\envs\\mytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "E:\\Programming\\anaconda3\\envs\\mytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "E:\\Programming\\anaconda3\\envs\\mytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/200 | Train Loss: 1.265683980549083 | Val Loss: 1.2456627388795216 | Val Accuracy: 0.4296875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      1.00      0.60       160\n",
      "           1       0.00      0.00      0.00        97\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        89\n",
      "\n",
      "    accuracy                           0.43       374\n",
      "   macro avg       0.11      0.25      0.15       374\n",
      "weighted avg       0.18      0.43      0.26       374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Programming\\anaconda3\\envs\\mytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "E:\\Programming\\anaconda3\\envs\\mytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "E:\\Programming\\anaconda3\\envs\\mytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/200 | Train Loss: 1.2593302796868717 | Val Loss: 1.2581024865309398 | Val Accuracy: 0.4296875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      1.00      0.60       160\n",
      "           1       0.00      0.00      0.00        97\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.00      0.00      0.00        89\n",
      "\n",
      "    accuracy                           0.43       374\n",
      "   macro avg       0.11      0.25      0.15       374\n",
      "weighted avg       0.18      0.43      0.26       374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Programming\\anaconda3\\envs\\mytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "E:\\Programming\\anaconda3\\envs\\mytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "E:\\Programming\\anaconda3\\envs\\mytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/200 | Train Loss: 1.2544980902297824 | Val Loss: 1.225640445947647 | Val Accuracy: 0.43631628787878785\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.97      0.60       160\n",
      "           1       0.00      0.00      0.00        97\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.47      0.09      0.15        89\n",
      "\n",
      "    accuracy                           0.44       374\n",
      "   macro avg       0.23      0.26      0.19       374\n",
      "weighted avg       0.30      0.44      0.29       374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Programming\\anaconda3\\envs\\mytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "E:\\Programming\\anaconda3\\envs\\mytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "E:\\Programming\\anaconda3\\envs\\mytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/200 | Train Loss: 1.222224698931563 | Val Loss: 1.221544623374939 | Val Accuracy: 0.4339488636363636\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.75      0.59       160\n",
      "           1       1.00      0.02      0.04        97\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.33      0.46      0.38        89\n",
      "\n",
      "    accuracy                           0.44       374\n",
      "   macro avg       0.45      0.31      0.25       374\n",
      "weighted avg       0.55      0.44      0.35       374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Programming\\anaconda3\\envs\\mytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "E:\\Programming\\anaconda3\\envs\\mytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "E:\\Programming\\anaconda3\\envs\\mytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/200 | Train Loss: 1.1734308127094717 | Val Loss: 1.1962232838074367 | Val Accuracy: 0.44554924242424243\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.82      0.62       160\n",
      "           1       0.00      0.00      0.00        97\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.32      0.40      0.36        89\n",
      "\n",
      "    accuracy                           0.45       374\n",
      "   macro avg       0.21      0.31      0.24       374\n",
      "weighted avg       0.29      0.45      0.35       374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Programming\\anaconda3\\envs\\mytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "E:\\Programming\\anaconda3\\envs\\mytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "E:\\Programming\\anaconda3\\envs\\mytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/200 | Train Loss: 1.108599541234035 | Val Loss: 1.1657480945189793 | Val Accuracy: 0.47679924242424243\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.70      0.65       160\n",
      "           1       0.64      0.09      0.16        97\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.34      0.65      0.44        89\n",
      "\n",
      "    accuracy                           0.48       374\n",
      "   macro avg       0.39      0.36      0.31       374\n",
      "weighted avg       0.50      0.48      0.42       374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Programming\\anaconda3\\envs\\mytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "E:\\Programming\\anaconda3\\envs\\mytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "E:\\Programming\\anaconda3\\envs\\mytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/200 | Train Loss: 1.0286188359354056 | Val Loss: 1.1284282803535461 | Val Accuracy: 0.5378787878787878\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.78      0.67       160\n",
      "           1       0.59      0.39      0.47        97\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.40      0.44      0.42        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.40      0.40      0.39       374\n",
      "weighted avg       0.50      0.54      0.51       374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Programming\\anaconda3\\envs\\mytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "E:\\Programming\\anaconda3\\envs\\mytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "E:\\Programming\\anaconda3\\envs\\mytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/200 | Train Loss: 0.9417120709138758 | Val Loss: 1.174361377954483 | Val Accuracy: 0.5196496212121212\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.84      0.66       160\n",
      "           1       0.68      0.26      0.37        97\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.39      0.40      0.40        89\n",
      "\n",
      "    accuracy                           0.52       374\n",
      "   macro avg       0.40      0.37      0.36       374\n",
      "weighted avg       0.50      0.52      0.47       374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Programming\\anaconda3\\envs\\mytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "E:\\Programming\\anaconda3\\envs\\mytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "E:\\Programming\\anaconda3\\envs\\mytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/200 | Train Loss: 0.8927012471591725 | Val Loss: 1.153243288397789 | Val Accuracy: 0.5430871212121212\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.84      0.68       160\n",
      "           1       0.58      0.29      0.39        97\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.47      0.46      0.47        89\n",
      "\n",
      "    accuracy                           0.55       374\n",
      "   macro avg       0.40      0.40      0.38       374\n",
      "weighted avg       0.51      0.55      0.50       374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Programming\\anaconda3\\envs\\mytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "E:\\Programming\\anaconda3\\envs\\mytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "E:\\Programming\\anaconda3\\envs\\mytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/200 | Train Loss: 0.8405897506311828 | Val Loss: 1.3604069451491039 | Val Accuracy: 0.47017045454545453\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.49      0.60       160\n",
      "           1       0.58      0.20      0.29        97\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.33      0.89      0.48        89\n",
      "\n",
      "    accuracy                           0.47       374\n",
      "   macro avg       0.42      0.39      0.34       374\n",
      "weighted avg       0.56      0.47      0.45       374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Programming\\anaconda3\\envs\\mytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "E:\\Programming\\anaconda3\\envs\\mytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "E:\\Programming\\anaconda3\\envs\\mytorch\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/200 | Train Loss: 0.8055095284008512 | Val Loss: 1.1509084701538086 | Val Accuracy: 0.5338541666666666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.65      0.68       160\n",
      "           1       0.44      0.41      0.43        97\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.42      0.63      0.51        89\n",
      "\n",
      "    accuracy                           0.53       374\n",
      "   macro avg       0.39      0.42      0.40       374\n",
      "weighted avg       0.52      0.53      0.52       374\n",
      "Epoch 13/200 | Train Loss: 0.7341706568703932 | Val Loss: 1.2229396750529606 | Val Accuracy: 0.5248579545454545\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.69      0.67       160\n",
      "           1       0.51      0.23      0.31        97\n",
      "           2       0.40      0.07      0.12        28\n",
      "           3       0.40      0.70      0.51        89\n",
      "\n",
      "    accuracy                           0.53       374\n",
      "   macro avg       0.49      0.42      0.40       374\n",
      "weighted avg       0.54      0.53      0.50       374\n",
      "Epoch 14/200 | Train Loss: 0.7138678104269738 | Val Loss: 1.2297234038511913 | Val Accuracy: 0.5170454545454545\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.56      0.63       160\n",
      "           1       0.43      0.47      0.45        97\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.42      0.65      0.51        89\n",
      "\n",
      "    accuracy                           0.52       374\n",
      "   macro avg       0.39      0.42      0.40       374\n",
      "weighted avg       0.52      0.52      0.51       374\n",
      "Epoch 15/200 | Train Loss: 0.6592404760566413 | Val Loss: 1.2636485695838928 | Val Accuracy: 0.5234375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.54      0.62       160\n",
      "           1       0.45      0.54      0.49        97\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.42      0.65      0.51        89\n",
      "\n",
      "    accuracy                           0.52       374\n",
      "   macro avg       0.40      0.43      0.41       374\n",
      "weighted avg       0.53      0.52      0.51       374\n",
      "Epoch 16/200 | Train Loss: 0.6293439158037597 | Val Loss: 1.2088777720928192 | Val Accuracy: 0.5520833333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.64      0.67       160\n",
      "           1       0.47      0.54      0.50        97\n",
      "           2       0.00      0.00      0.00        28\n",
      "           3       0.46      0.60      0.52        89\n",
      "\n",
      "    accuracy                           0.55       374\n",
      "   macro avg       0.41      0.44      0.42       374\n",
      "weighted avg       0.53      0.55      0.54       374\n",
      "Epoch 17/200 | Train Loss: 0.5876338075773389 | Val Loss: 1.2255999545256298 | Val Accuracy: 0.5688920454545454\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.69      0.68       160\n",
      "           1       0.46      0.47      0.47        97\n",
      "           2       0.42      0.29      0.34        28\n",
      "           3       0.54      0.54      0.54        89\n",
      "\n",
      "    accuracy                           0.57       374\n",
      "   macro avg       0.52      0.50      0.51       374\n",
      "weighted avg       0.57      0.57      0.57       374\n",
      "Epoch 18/200 | Train Loss: 0.565858429553462 | Val Loss: 1.280510405699412 | Val Accuracy: 0.5598958333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.60      0.66       160\n",
      "           1       0.44      0.65      0.52        97\n",
      "           2       0.33      0.21      0.26        28\n",
      "           3       0.56      0.51      0.53        89\n",
      "\n",
      "    accuracy                           0.56       374\n",
      "   macro avg       0.52      0.49      0.49       374\n",
      "weighted avg       0.58      0.56      0.56       374\n",
      "Epoch 19/200 | Train Loss: 0.512796516073685 | Val Loss: 1.3042391141255696 | Val Accuracy: 0.5743371212121212\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.70      0.69       160\n",
      "           1       0.45      0.64      0.53        97\n",
      "           2       0.50      0.07      0.12        28\n",
      "           3       0.62      0.45      0.52        89\n",
      "\n",
      "    accuracy                           0.58       374\n",
      "   macro avg       0.56      0.47      0.46       374\n",
      "weighted avg       0.59      0.58      0.56       374\n",
      "Epoch 20/200 | Train Loss: 0.48328932959075066 | Val Loss: 1.4033346225817998 | Val Accuracy: 0.548058712121212\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.57      0.64       160\n",
      "           1       0.48      0.47      0.48        97\n",
      "           2       0.26      0.25      0.25        28\n",
      "           3       0.48      0.67      0.56        89\n",
      "\n",
      "    accuracy                           0.55       374\n",
      "   macro avg       0.49      0.49      0.48       374\n",
      "weighted avg       0.57      0.55      0.55       374\n",
      "Epoch 21/200 | Train Loss: 0.45827291146212934 | Val Loss: 1.371959388256073 | Val Accuracy: 0.5570549242424242\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.59      0.65       160\n",
      "           1       0.46      0.60      0.52        97\n",
      "           2       0.26      0.21      0.24        28\n",
      "           3       0.52      0.56      0.54        89\n",
      "\n",
      "    accuracy                           0.56       374\n",
      "   macro avg       0.49      0.49      0.49       374\n",
      "weighted avg       0.57      0.56      0.56       374\n",
      "Epoch 22/200 | Train Loss: 0.42550284386265513 | Val Loss: 1.5193363229433696 | Val Accuracy: 0.5170454545454545\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.61      0.64       160\n",
      "           1       0.52      0.31      0.39        97\n",
      "           2       0.23      0.46      0.31        28\n",
      "           3       0.47      0.61      0.53        89\n",
      "\n",
      "    accuracy                           0.52       374\n",
      "   macro avg       0.47      0.50      0.47       374\n",
      "weighted avg       0.55      0.52      0.52       374\n",
      "Epoch 23/200 | Train Loss: 0.4029981825573772 | Val Loss: 1.5027819971243541 | Val Accuracy: 0.5428503787878788\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.59      0.64       160\n",
      "           1       0.48      0.51      0.49        97\n",
      "           2       0.24      0.25      0.25        28\n",
      "           3       0.48      0.60      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.48      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 24/200 | Train Loss: 0.3901992123208794 | Val Loss: 1.7155300378799438 | Val Accuracy: 0.5078125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.53      0.61       160\n",
      "           1       0.56      0.29      0.38        97\n",
      "           2       0.20      0.46      0.28        28\n",
      "           3       0.45      0.73      0.56        89\n",
      "\n",
      "    accuracy                           0.51       374\n",
      "   macro avg       0.49      0.50      0.46       374\n",
      "weighted avg       0.58      0.51      0.51       374\n",
      "Epoch 25/200 | Train Loss: 0.3849558028958592 | Val Loss: 1.7429705262184143 | Val Accuracy: 0.5063920454545454\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.45      0.57       160\n",
      "           1       0.49      0.40      0.44        97\n",
      "           2       0.35      0.39      0.37        28\n",
      "           3       0.39      0.75      0.52        89\n",
      "\n",
      "    accuracy                           0.51       374\n",
      "   macro avg       0.50      0.50      0.48       374\n",
      "weighted avg       0.58      0.51      0.51       374\n",
      "Epoch 26/200 | Train Loss: 0.35124780719771104 | Val Loss: 1.5761850029230118 | Val Accuracy: 0.5364583333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.63      0.65       160\n",
      "           1       0.54      0.36      0.43        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.48      0.63      0.55        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.48      0.49      0.47       374\n",
      "weighted avg       0.56      0.54      0.54       374\n",
      "Epoch 27/200 | Train Loss: 0.34116894335431214 | Val Loss: 1.5913508236408234 | Val Accuracy: 0.5767045454545454\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.86      0.70       160\n",
      "           1       0.56      0.29      0.38        97\n",
      "           2       0.40      0.29      0.33        28\n",
      "           3       0.61      0.48      0.54        89\n",
      "\n",
      "    accuracy                           0.58       374\n",
      "   macro avg       0.54      0.48      0.49       374\n",
      "weighted avg       0.57      0.58      0.55       374\n",
      "Epoch 28/200 | Train Loss: 0.32382534743816244 | Val Loss: 1.6354424953460693 | Val Accuracy: 0.5205965909090909\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.53      0.61       160\n",
      "           1       0.41      0.61      0.49        97\n",
      "           2       0.20      0.21      0.21        28\n",
      "           3       0.53      0.51      0.52        89\n",
      "\n",
      "    accuracy                           0.52       374\n",
      "   macro avg       0.47      0.46      0.46       374\n",
      "weighted avg       0.56      0.52      0.53       374\n",
      "Epoch 29/200 | Train Loss: 0.26702109870373036 | Val Loss: 1.5749314030011494 | Val Accuracy: 0.5364583333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.66      0.66       160\n",
      "           1       0.44      0.41      0.43        97\n",
      "           2       0.25      0.39      0.31        28\n",
      "           3       0.56      0.51      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.48      0.49      0.48       374\n",
      "weighted avg       0.55      0.54      0.54       374\n",
      "Epoch 30/200 | Train Loss: 0.24998933222948336 | Val Loss: 1.5537173002958298 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.65      0.65       160\n",
      "           1       0.46      0.47      0.46        97\n",
      "           2       0.21      0.25      0.23        28\n",
      "           3       0.56      0.51      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.47      0.47       374\n",
      "weighted avg       0.55      0.54      0.54       374\n",
      "Epoch 31/200 | Train Loss: 0.2615687942095831 | Val Loss: 1.580722247560819 | Val Accuracy: 0.5364583333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.64      0.65       160\n",
      "           1       0.45      0.47      0.46        97\n",
      "           2       0.20      0.29      0.23        28\n",
      "           3       0.57      0.51      0.54        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.47       374\n",
      "weighted avg       0.55      0.54      0.54       374\n",
      "Epoch 32/200 | Train Loss: 0.2672104653628433 | Val Loss: 1.5826354026794434 | Val Accuracy: 0.5338541666666666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.64      0.66       160\n",
      "           1       0.46      0.45      0.46        97\n",
      "           2       0.18      0.25      0.21        28\n",
      "           3       0.53      0.52      0.53        89\n",
      "\n",
      "    accuracy                           0.53       374\n",
      "   macro avg       0.46      0.47      0.46       374\n",
      "weighted avg       0.55      0.53      0.54       374\n",
      "Epoch 33/200 | Train Loss: 0.2567360001963143 | Val Loss: 1.5890485892693202 | Val Accuracy: 0.53125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.66      0.66       160\n",
      "           1       0.45      0.40      0.42        97\n",
      "           2       0.21      0.32      0.26        28\n",
      "           3       0.54      0.51      0.52        89\n",
      "\n",
      "    accuracy                           0.53       374\n",
      "   macro avg       0.46      0.47      0.47       374\n",
      "weighted avg       0.54      0.53      0.54       374\n",
      "Epoch 34/200 | Train Loss: 0.25967830916245777 | Val Loss: 1.6033301850159962 | Val Accuracy: 0.53125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.64      0.65       160\n",
      "           1       0.46      0.42      0.44        97\n",
      "           2       0.21      0.32      0.25        28\n",
      "           3       0.53      0.52      0.52        89\n",
      "\n",
      "    accuracy                           0.53       374\n",
      "   macro avg       0.47      0.48      0.47       374\n",
      "weighted avg       0.55      0.53      0.54       374\n",
      "Epoch 35/200 | Train Loss: 0.24236687455399364 | Val Loss: 1.6106889148553212 | Val Accuracy: 0.5286458333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.64      0.66       160\n",
      "           1       0.46      0.42      0.44        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.52      0.51      0.51        89\n",
      "\n",
      "    accuracy                           0.53       374\n",
      "   macro avg       0.46      0.47      0.46       374\n",
      "weighted avg       0.54      0.53      0.54       374\n",
      "Epoch 36/200 | Train Loss: 0.25180351690334435 | Val Loss: 1.6300451010465622 | Val Accuracy: 0.5494791666666666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.62      0.65       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.24      0.32      0.28        28\n",
      "           3       0.53      0.62      0.57        89\n",
      "\n",
      "    accuracy                           0.55       374\n",
      "   macro avg       0.49      0.50      0.49       374\n",
      "weighted avg       0.56      0.55      0.55       374\n",
      "Epoch 37/200 | Train Loss: 0.24792187390666381 | Val Loss: 1.6271886428197224 | Val Accuracy: 0.5442708333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.62      0.65       160\n",
      "           1       0.48      0.45      0.47        97\n",
      "           2       0.23      0.32      0.27        28\n",
      "           3       0.54      0.58      0.56        89\n",
      "\n",
      "    accuracy                           0.55       374\n",
      "   macro avg       0.48      0.49      0.49       374\n",
      "weighted avg       0.56      0.55      0.55       374\n",
      "Epoch 38/200 | Train Loss: 0.24353352627333472 | Val Loss: 1.639914184808731 | Val Accuracy: 0.5352746212121212\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.64      0.65       160\n",
      "           1       0.48      0.41      0.44        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.54      0.56      0.55        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.47       374\n",
      "weighted avg       0.55      0.54      0.54       374\n",
      "Epoch 39/200 | Train Loss: 0.2379821015923631 | Val Loss: 1.6239046603441238 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.65      0.66       160\n",
      "           1       0.47      0.41      0.44        97\n",
      "           2       0.20      0.29      0.24        28\n",
      "           3       0.54      0.56      0.55        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.47       374\n",
      "weighted avg       0.55      0.54      0.54       374\n",
      "Epoch 40/200 | Train Loss: 0.24926421835142024 | Val Loss: 1.6257427434126537 | Val Accuracy: 0.5416666666666666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.29      0.24        28\n",
      "           3       0.53      0.55      0.54        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.47       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 41/200 | Train Loss: 0.24093149197013938 | Val Loss: 1.6361864109834034 | Val Accuracy: 0.5338541666666666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.62      0.65       160\n",
      "           1       0.47      0.44      0.46        97\n",
      "           2       0.20      0.29      0.24        28\n",
      "           3       0.52      0.55      0.54        89\n",
      "\n",
      "    accuracy                           0.53       374\n",
      "   macro avg       0.47      0.48      0.47       374\n",
      "weighted avg       0.55      0.53      0.54       374\n",
      "Epoch 42/200 | Train Loss: 0.23254401952612633 | Val Loss: 1.6298957268397014 | Val Accuracy: 0.5364583333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.65      0.66       160\n",
      "           1       0.47      0.42      0.44        97\n",
      "           2       0.21      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.47       374\n",
      "weighted avg       0.55      0.54      0.54       374\n",
      "Epoch 43/200 | Train Loss: 0.23361593820885115 | Val Loss: 1.6386229594548543 | Val Accuracy: 0.5286458333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.62      0.65       160\n",
      "           1       0.47      0.44      0.46        97\n",
      "           2       0.19      0.29      0.23        28\n",
      "           3       0.52      0.53      0.52        89\n",
      "\n",
      "    accuracy                           0.53       374\n",
      "   macro avg       0.46      0.47      0.46       374\n",
      "weighted avg       0.55      0.53      0.54       374\n",
      "Epoch 44/200 | Train Loss: 0.24953272741507082 | Val Loss: 1.6389496078093846 | Val Accuracy: 0.5364583333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.64      0.66       160\n",
      "           1       0.48      0.43      0.45        97\n",
      "           2       0.19      0.29      0.23        28\n",
      "           3       0.53      0.54      0.54        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.47       374\n",
      "weighted avg       0.55      0.54      0.54       374\n",
      "Epoch 45/200 | Train Loss: 0.24720928072929382 | Val Loss: 1.6399854520956676 | Val Accuracy: 0.5416666666666666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.64      0.66       160\n",
      "           1       0.48      0.42      0.45        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.55      0.56      0.56        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.48      0.49      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 46/200 | Train Loss: 0.23609864507235734 | Val Loss: 1.638762836654981 | Val Accuracy: 0.5364583333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.64      0.66       160\n",
      "           1       0.47      0.43      0.45        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.47       374\n",
      "weighted avg       0.55      0.54      0.54       374\n",
      "Epoch 47/200 | Train Loss: 0.23086003951874434 | Val Loss: 1.6395959158738453 | Val Accuracy: 0.5338541666666666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.62      0.65       160\n",
      "           1       0.47      0.44      0.46        97\n",
      "           2       0.20      0.29      0.24        28\n",
      "           3       0.52      0.55      0.54        89\n",
      "\n",
      "    accuracy                           0.53       374\n",
      "   macro avg       0.47      0.48      0.47       374\n",
      "weighted avg       0.55      0.53      0.54       374\n",
      "Epoch 48/200 | Train Loss: 0.23660721863601722 | Val Loss: 1.632921536763509 | Val Accuracy: 0.5338541666666666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.64      0.66       160\n",
      "           1       0.47      0.43      0.45        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.52      0.52        89\n",
      "\n",
      "    accuracy                           0.53       374\n",
      "   macro avg       0.47      0.48      0.47       374\n",
      "weighted avg       0.55      0.53      0.54       374\n",
      "Epoch 49/200 | Train Loss: 0.23336777218854896 | Val Loss: 1.6408956150213878 | Val Accuracy: 0.5364583333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.64      0.66       160\n",
      "           1       0.47      0.42      0.45        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.54      0.54      0.54        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.47       374\n",
      "weighted avg       0.55      0.54      0.54       374\n",
      "Epoch 50/200 | Train Loss: 0.2398956263912659 | Val Loss: 1.6407306293646495 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 51/200 | Train Loss: 0.23583795703655364 | Val Loss: 1.6415363649527233 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 52/200 | Train Loss: 0.231891114776041 | Val Loss: 1.6415480722983677 | Val Accuracy: 0.5364583333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.47      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.47       374\n",
      "weighted avg       0.55      0.54      0.54       374\n",
      "Epoch 53/200 | Train Loss: 0.22256451465335547 | Val Loss: 1.6412186821301777 | Val Accuracy: 0.5364583333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.47      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.47       374\n",
      "weighted avg       0.55      0.54      0.54       374\n",
      "Epoch 54/200 | Train Loss: 0.24331373977018336 | Val Loss: 1.6414216061433156 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 55/200 | Train Loss: 0.23170985544429107 | Val Loss: 1.6419627765814464 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 56/200 | Train Loss: 0.24486655789409198 | Val Loss: 1.6426010479529698 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 57/200 | Train Loss: 0.23392746254217392 | Val Loss: 1.6421458572149277 | Val Accuracy: 0.5364583333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.64      0.66       160\n",
      "           1       0.47      0.43      0.45        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.47       374\n",
      "weighted avg       0.55      0.54      0.54       374\n",
      "Epoch 58/200 | Train Loss: 0.24065410864411615 | Val Loss: 1.6423622965812683 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 59/200 | Train Loss: 0.2319248733932481 | Val Loss: 1.642770955959956 | Val Accuracy: 0.5364583333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.64      0.66       160\n",
      "           1       0.47      0.43      0.45        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.47       374\n",
      "weighted avg       0.55      0.54      0.54       374\n",
      "Epoch 60/200 | Train Loss: 0.2464643269046849 | Val Loss: 1.643875444928805 | Val Accuracy: 0.5364583333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.64      0.66       160\n",
      "           1       0.47      0.43      0.45        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.47       374\n",
      "weighted avg       0.55      0.54      0.54       374\n",
      "Epoch 61/200 | Train Loss: 0.23053631975370295 | Val Loss: 1.643221452832222 | Val Accuracy: 0.5364583333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.64      0.66       160\n",
      "           1       0.47      0.43      0.45        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.47       374\n",
      "weighted avg       0.55      0.54      0.54       374\n",
      "Epoch 62/200 | Train Loss: 0.22602903397352087 | Val Loss: 1.6427330821752548 | Val Accuracy: 0.5364583333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.64      0.66       160\n",
      "           1       0.47      0.43      0.45        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.47       374\n",
      "weighted avg       0.55      0.54      0.54       374\n",
      "Epoch 63/200 | Train Loss: 0.23850738826920004 | Val Loss: 1.6433089127143223 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 64/200 | Train Loss: 0.22632922473199227 | Val Loss: 1.644855300585429 | Val Accuracy: 0.5364583333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.64      0.66       160\n",
      "           1       0.47      0.43      0.45        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.47       374\n",
      "weighted avg       0.55      0.54      0.54       374\n",
      "Epoch 65/200 | Train Loss: 0.23801152156118086 | Val Loss: 1.644935816526413 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 66/200 | Train Loss: 0.2413052763132488 | Val Loss: 1.6443366507689159 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 67/200 | Train Loss: 0.2280224054029175 | Val Loss: 1.6445164382457733 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 68/200 | Train Loss: 0.23743168286540928 | Val Loss: 1.6454419245322545 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 69/200 | Train Loss: 0.23035498923969036 | Val Loss: 1.6457778463761012 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 70/200 | Train Loss: 0.2359917842801295 | Val Loss: 1.6455650826295216 | Val Accuracy: 0.5364583333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.47      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.47       374\n",
      "weighted avg       0.55      0.54      0.54       374\n",
      "Epoch 71/200 | Train Loss: 0.2266345649738522 | Val Loss: 1.6450414508581161 | Val Accuracy: 0.5364583333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.47      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.47       374\n",
      "weighted avg       0.55      0.54      0.54       374\n",
      "Epoch 72/200 | Train Loss: 0.2444556342328296 | Val Loss: 1.644893487294515 | Val Accuracy: 0.5364583333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.47      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.47       374\n",
      "weighted avg       0.55      0.54      0.54       374\n",
      "Epoch 73/200 | Train Loss: 0.25150107256337706 | Val Loss: 1.6444209069013596 | Val Accuracy: 0.5364583333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.47      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.47       374\n",
      "weighted avg       0.55      0.54      0.54       374\n",
      "Epoch 74/200 | Train Loss: 0.22911340706780844 | Val Loss: 1.6444928447405498 | Val Accuracy: 0.5364583333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.47      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.47       374\n",
      "weighted avg       0.55      0.54      0.54       374\n",
      "Epoch 75/200 | Train Loss: 0.23813322106120632 | Val Loss: 1.6443509211142857 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 76/200 | Train Loss: 0.24743007375475237 | Val Loss: 1.6454846610625584 | Val Accuracy: 0.5364583333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.47      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.47       374\n",
      "weighted avg       0.55      0.54      0.54       374\n",
      "Epoch 77/200 | Train Loss: 0.22932411916553974 | Val Loss: 1.6458349575599034 | Val Accuracy: 0.5364583333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.47      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.47       374\n",
      "weighted avg       0.55      0.54      0.54       374\n",
      "Epoch 78/200 | Train Loss: 0.23972924681855182 | Val Loss: 1.6460611869891484 | Val Accuracy: 0.5364583333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.47      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.47       374\n",
      "weighted avg       0.55      0.54      0.54       374\n",
      "Epoch 79/200 | Train Loss: 0.238931556104445 | Val Loss: 1.645840272307396 | Val Accuracy: 0.5338541666666666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.63      0.65       160\n",
      "           1       0.47      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.53       374\n",
      "   macro avg       0.47      0.48      0.47       374\n",
      "weighted avg       0.55      0.53      0.54       374\n",
      "Epoch 80/200 | Train Loss: 0.23350818824096053 | Val Loss: 1.6463936418294907 | Val Accuracy: 0.5364583333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.47      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.47       374\n",
      "weighted avg       0.55      0.54      0.54       374\n",
      "Epoch 81/200 | Train Loss: 0.22567941759731255 | Val Loss: 1.6467705468336742 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 82/200 | Train Loss: 0.22524053187054746 | Val Loss: 1.6472601344188054 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 83/200 | Train Loss: 0.22761601392252773 | Val Loss: 1.6465567251046498 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 84/200 | Train Loss: 0.2413731811969888 | Val Loss: 1.646002858877182 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 85/200 | Train Loss: 0.2337630497766476 | Val Loss: 1.6468391120433807 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 86/200 | Train Loss: 0.2374101162833326 | Val Loss: 1.6470864067475002 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 87/200 | Train Loss: 0.2302703613451883 | Val Loss: 1.6479063630104065 | Val Accuracy: 0.5364583333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.47      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.47       374\n",
      "weighted avg       0.55      0.54      0.54       374\n",
      "Epoch 88/200 | Train Loss: 0.23871529803556554 | Val Loss: 1.6483020881811778 | Val Accuracy: 0.5338541666666666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.63      0.65       160\n",
      "           1       0.47      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.53       374\n",
      "   macro avg       0.47      0.48      0.47       374\n",
      "weighted avg       0.55      0.53      0.54       374\n",
      "Epoch 89/200 | Train Loss: 0.22402602156587675 | Val Loss: 1.6478978991508484 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 90/200 | Train Loss: 0.2402720552522178 | Val Loss: 1.64789280295372 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 91/200 | Train Loss: 0.2256626962300609 | Val Loss: 1.6483946392933528 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 92/200 | Train Loss: 0.24254870940657222 | Val Loss: 1.6471497764190037 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 93/200 | Train Loss: 0.22805495735477 | Val Loss: 1.6475266218185425 | Val Accuracy: 0.5364583333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.64      0.66       160\n",
      "           1       0.47      0.43      0.45        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.47       374\n",
      "weighted avg       0.55      0.54      0.54       374\n",
      "Epoch 94/200 | Train Loss: 0.2410977283383117 | Val Loss: 1.6468940923611324 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 95/200 | Train Loss: 0.23230301110329582 | Val Loss: 1.6461843649546306 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 96/200 | Train Loss: 0.2304756542777314 | Val Loss: 1.645921731988589 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 97/200 | Train Loss: 0.238152980293129 | Val Loss: 1.6451427588860195 | Val Accuracy: 0.5364583333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.64      0.66       160\n",
      "           1       0.47      0.43      0.45        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.47       374\n",
      "weighted avg       0.55      0.54      0.54       374\n",
      "Epoch 98/200 | Train Loss: 0.2441179820281618 | Val Loss: 1.6455793331066768 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 99/200 | Train Loss: 0.2441340826963093 | Val Loss: 1.6465232918659847 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 100/200 | Train Loss: 0.2318473531335008 | Val Loss: 1.6473096708456676 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 101/200 | Train Loss: 0.2354938689546258 | Val Loss: 1.647285098830859 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 102/200 | Train Loss: 0.24557775463543685 | Val Loss: 1.6465991983811061 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 103/200 | Train Loss: 0.23312979381458432 | Val Loss: 1.6465504268805187 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 104/200 | Train Loss: 0.23229717182032034 | Val Loss: 1.6467511852582295 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 105/200 | Train Loss: 0.24977357356863863 | Val Loss: 1.6467158397038777 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 106/200 | Train Loss: 0.22935798026475251 | Val Loss: 1.6465483208497365 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 107/200 | Train Loss: 0.22863064940069236 | Val Loss: 1.6472640832265217 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 108/200 | Train Loss: 0.23916117423305325 | Val Loss: 1.6470441470543544 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 109/200 | Train Loss: 0.23461743502640256 | Val Loss: 1.647143393754959 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 110/200 | Train Loss: 0.2291939299480588 | Val Loss: 1.6472484320402145 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 111/200 | Train Loss: 0.2289452026872074 | Val Loss: 1.646821637948354 | Val Accuracy: 0.5338541666666666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.63      0.65       160\n",
      "           1       0.47      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.53       374\n",
      "   macro avg       0.47      0.48      0.47       374\n",
      "weighted avg       0.55      0.53      0.54       374\n",
      "Epoch 112/200 | Train Loss: 0.22175744511917525 | Val Loss: 1.6460045526425044 | Val Accuracy: 0.5338541666666666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.63      0.65       160\n",
      "           1       0.47      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.53       374\n",
      "   macro avg       0.47      0.48      0.47       374\n",
      "weighted avg       0.55      0.53      0.54       374\n",
      "Epoch 113/200 | Train Loss: 0.23287451500986137 | Val Loss: 1.647442489862442 | Val Accuracy: 0.5286458333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.62      0.64       160\n",
      "           1       0.46      0.44      0.45        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.53       374\n",
      "   macro avg       0.47      0.48      0.47       374\n",
      "weighted avg       0.55      0.53      0.54       374\n",
      "Epoch 114/200 | Train Loss: 0.2405370451217773 | Val Loss: 1.6478448311487834 | Val Accuracy: 0.5338541666666666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.63      0.65       160\n",
      "           1       0.47      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.53       374\n",
      "   macro avg       0.47      0.48      0.47       374\n",
      "weighted avg       0.55      0.53      0.54       374\n",
      "Epoch 115/200 | Train Loss: 0.23703408613801003 | Val Loss: 1.648383155465126 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 116/200 | Train Loss: 0.24710280305760748 | Val Loss: 1.6487864206234615 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 117/200 | Train Loss: 0.22051437644689692 | Val Loss: 1.649389425913493 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 118/200 | Train Loss: 0.23017158125545464 | Val Loss: 1.6491437604029973 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 119/200 | Train Loss: 0.2407249713718307 | Val Loss: 1.6496208955844243 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 120/200 | Train Loss: 0.23909407857732445 | Val Loss: 1.6502478023370106 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 121/200 | Train Loss: 0.23285232462427197 | Val Loss: 1.6508979399998982 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 122/200 | Train Loss: 0.23741724334803282 | Val Loss: 1.6509139637152355 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 123/200 | Train Loss: 0.22618263236740055 | Val Loss: 1.6515244444211323 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 124/200 | Train Loss: 0.22544476134227773 | Val Loss: 1.6508395622173946 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 125/200 | Train Loss: 0.2399933597474706 | Val Loss: 1.6500288794438045 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 126/200 | Train Loss: 0.2193362443689622 | Val Loss: 1.648571362098058 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 127/200 | Train Loss: 0.25085932531339283 | Val Loss: 1.6484759896993637 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 128/200 | Train Loss: 0.2481665363279628 | Val Loss: 1.6485147227843602 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 129/200 | Train Loss: 0.2308338541610568 | Val Loss: 1.6487995783487956 | Val Accuracy: 0.5364583333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.64      0.66       160\n",
      "           1       0.47      0.43      0.45        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.47       374\n",
      "weighted avg       0.55      0.54      0.54       374\n",
      "Epoch 130/200 | Train Loss: 0.24461347514799997 | Val Loss: 1.649529367685318 | Val Accuracy: 0.5364583333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.64      0.66       160\n",
      "           1       0.47      0.43      0.45        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.47       374\n",
      "weighted avg       0.55      0.54      0.54       374\n",
      "Epoch 131/200 | Train Loss: 0.23620086718423694 | Val Loss: 1.6495519280433655 | Val Accuracy: 0.5364583333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.64      0.66       160\n",
      "           1       0.47      0.43      0.45        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.47       374\n",
      "weighted avg       0.55      0.54      0.54       374\n",
      "Epoch 132/200 | Train Loss: 0.24186590401565328 | Val Loss: 1.6490030785401661 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 133/200 | Train Loss: 0.2402308776098139 | Val Loss: 1.649787187576294 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 134/200 | Train Loss: 0.24605964189943144 | Val Loss: 1.6504606157541275 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 135/200 | Train Loss: 0.2308469232829178 | Val Loss: 1.6507861365874608 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 136/200 | Train Loss: 0.2336676320170655 | Val Loss: 1.650867074728012 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 137/200 | Train Loss: 0.24362797282782256 | Val Loss: 1.65124615530173 | Val Accuracy: 0.5364583333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.47      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.47       374\n",
      "weighted avg       0.55      0.54      0.54       374\n",
      "Epoch 138/200 | Train Loss: 0.24377275711181118 | Val Loss: 1.650990789135297 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 139/200 | Train Loss: 0.23718508559406973 | Val Loss: 1.6513242771228154 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 140/200 | Train Loss: 0.2316147365844717 | Val Loss: 1.652200351158778 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 141/200 | Train Loss: 0.23036187934670962 | Val Loss: 1.6517943342526753 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 142/200 | Train Loss: 0.24412660976397058 | Val Loss: 1.6522432665030162 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 143/200 | Train Loss: 0.24284698857980616 | Val Loss: 1.6524860858917236 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 144/200 | Train Loss: 0.23742707891791476 | Val Loss: 1.6522070666154225 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 145/200 | Train Loss: 0.22206066117859355 | Val Loss: 1.652663101752599 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 146/200 | Train Loss: 0.23097403760195948 | Val Loss: 1.652232512831688 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 147/200 | Train Loss: 0.23017971955385863 | Val Loss: 1.6527359783649445 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 148/200 | Train Loss: 0.22973479695764243 | Val Loss: 1.652124101916949 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 149/200 | Train Loss: 0.2293706192981963 | Val Loss: 1.6525508016347885 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 150/200 | Train Loss: 0.21716451754464822 | Val Loss: 1.6528408825397491 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 151/200 | Train Loss: 0.22931712208425298 | Val Loss: 1.652622843782107 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 152/200 | Train Loss: 0.2410474587597099 | Val Loss: 1.6524894883235295 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 153/200 | Train Loss: 0.23860924822442672 | Val Loss: 1.6523293256759644 | Val Accuracy: 0.5364583333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.64      0.66       160\n",
      "           1       0.47      0.43      0.45        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.47       374\n",
      "weighted avg       0.55      0.54      0.54       374\n",
      "Epoch 154/200 | Train Loss: 0.24177324983711337 | Val Loss: 1.6532837202151616 | Val Accuracy: 0.5364583333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.64      0.66       160\n",
      "           1       0.47      0.43      0.45        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.47       374\n",
      "weighted avg       0.55      0.54      0.54       374\n",
      "Epoch 155/200 | Train Loss: 0.22731387155020938 | Val Loss: 1.6529127806425095 | Val Accuracy: 0.5364583333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.64      0.66       160\n",
      "           1       0.47      0.43      0.45        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.47       374\n",
      "weighted avg       0.55      0.54      0.54       374\n",
      "Epoch 156/200 | Train Loss: 0.24015416131884443 | Val Loss: 1.6529444058736165 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 157/200 | Train Loss: 0.21934176177955142 | Val Loss: 1.6533802598714828 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n",
      "Epoch 158/200 | Train Loss: 0.23067831974841801 | Val Loss: 1.6542885253826778 | Val Accuracy: 0.5390625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.64      0.66       160\n",
      "           1       0.48      0.44      0.46        97\n",
      "           2       0.20      0.32      0.25        28\n",
      "           3       0.53      0.53      0.53        89\n",
      "\n",
      "    accuracy                           0.54       374\n",
      "   macro avg       0.47      0.48      0.48       374\n",
      "weighted avg       0.56      0.54      0.55       374\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
    "from torch.optim import AdamW\n",
    "from transformers import BertForSequenceClassification, get_linear_schedule_with_warmup, \\\n",
    "    DistilBertForSequenceClassification, DistilBertConfig,BertConfig\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# BERT\n",
    "# 加载预训练的BERT模型，指定分类数为4（你的情况下是0, 1, 2, 3四种情感）\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)\n",
    "# config = BertConfig.from_pretrained('bert-base-uncased', hidden_size=360)\n",
    "# model = BertForSequenceClassification(config=config)\n",
    "# 创建一个更小的BERT配置\n",
    "config = BertConfig(\n",
    "    hidden_size=256*2,                # 减小隐藏层尺寸，原始值为768\n",
    "    num_hidden_layers=4*2,            # 减少隐藏层数，原始值为12\n",
    "    num_attention_heads=4*2,          # 减少注意力头数，原始值为12\n",
    "    intermediate_size=1024*2,         # 减小前馈网络的尺寸,原始值为4096\n",
    "    hidden_dropout_prob=0.2,        # 可以调整dropout，原始值为0.1\n",
    "    attention_probs_dropout_prob=0.1, # 可以调整attention dropout，原始值为0.1\n",
    "    num_labels=4\n",
    ")\n",
    "model = BertForSequenceClassification(config=config)\n",
    "\n",
    "# Distil BERT\n",
    "# 创建一个自定义的DistilBERT配置\n",
    "# config = DistilBertConfig(\n",
    "#     # n_layers=6,  # 默认为6，这里减少到4层\n",
    "#     # dim=256*2,  # 默认为768，这里减小到256(hidden_size，所有层的隐藏状态的尺寸)\n",
    "#     # hidden_dim=1024*2,  # 注意力层后面的前馈层的尺寸，默认为3072(4倍于dim)\n",
    "#     # n_heads=8,  # 默认为12\n",
    "#     dropout=0.2,  # 增加dropout比率，原始默认为0.1\n",
    "#     attention_dropout=0.1,  # 增加注意力层的dropout，原始默认为0.1\n",
    "#     num_labels=4  # 设置输出标签数，针对分类任务\n",
    "# )\n",
    "# 使用自定义配置初始化DistilBert模型\n",
    "# model = DistilBertForSequenceClassification(config=config)\n",
    "\n",
    "# # # 加载DistilBERT预训练模型，这里假设我们是做一个分类任务，类别数为4\n",
    "# model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=4)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.001)\n",
    "epochs = 200\n",
    "total_steps = len(train_loader) * epochs\n",
    "\n",
    "# 使用线性学习率调度器\n",
    "# scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=total_steps)\n",
    "\n",
    "# 使用余弦退火调整学习率\n",
    "# scheduler = CosineAnnealingLR(optimizer, T_max=total_steps)\n",
    "# scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=100, T_mult=1, eta_min=0) # T_0是第一个周期的迭代次数，T_mult是周期之间的倍数，eta_min是学习率的最小值\n",
    "\n",
    "# 使用 验证集\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=10, verbose=True)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "\n",
    "# 开始训练、验证和绘制损失曲线\n",
    "train_losses, val_losses, train_f1score, val_f1score, model = train(model, train_loader, val_loader, optimizer, scheduler, device, epochs)\n",
    "# 使用测试集进行最终评估\n",
    "test(model, test_loader, device)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-04-13T21:46:24.443430Z"
    }
   },
   "id": "cb203bd9b0a44883",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# 设置画板大小\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "# 绘制损失曲线图\n",
    "# plt.subplot(1, 2, 1)  # 1行2列的第1个\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "# plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs',fontsize=20)\n",
    "plt.ylabel('Loss', fontsize=20)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.legend()\n",
    "# 显示图表\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4574f1806f78405c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 绘制F1得分曲线图\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(train_f1score, label='Train F1 Score')\n",
    "plt.plot(val_f1score, label='Validation F1 Score')\n",
    "plt.title('Training and Validation F1 Score')\n",
    "plt.xlabel('Epochs',fontsize=20)\n",
    "plt.ylabel('Macro F1 Score', fontsize=20)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7276fb5e40298b98",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 储存当前模型valf1score列表\n",
    "import pickle\n",
    "\n",
    "valf1_distilbert_CosineLR = val_f1score\n",
    "with open('./NLP_data_cache/valf1_distilbert_CosineLR.pkl', 'wb') as f:\n",
    "    pickle.dump(valf1_distilbert_CosineLR, f)\n",
    "    \n",
    "# 读取valf1score列表\n",
    "with open('./NLP_data_cache/valf1_distilbert_CosineLR.pkl', 'rb') as f:\n",
    "    valf1_distilbert_CosineLR = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d051d9d6a32cd44a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(valf1_distilbert_CosineLR, label='DistilBERT with CosineAnnealingLR')\n",
    "plt.title('Validation F1 Score')\n",
    "plt.xlabel('Epochs',fontsize=20)\n",
    "plt.ylabel('Weighted F1 Score', fontsize=20)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "96e0c64c0fe34afc",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Find the joyful and optimistic topics\n",
    "## 4.1 Filter the Data\n",
    "Find the text data with the labels of joy and optimism."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f76ee373aebb0611"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 定义一个函数来筛选特定标签的文本\n",
    "def filter_texts_by_labels(texts, labels, target_labels):\n",
    "    filtered_texts = [text for text, label in zip(texts, labels) if label in target_labels]\n",
    "    filtered_labels = [label for label in labels if label in target_labels]\n",
    "    return filtered_texts, filtered_labels\n",
    "\n",
    "# 设置目标标签：1 (joy) 和 2 (optimism)\n",
    "target_labels = [1, 2]\n",
    "\n",
    "# 筛选训练集中的文本和标签\n",
    "train_texts_filtered, train_labels_filtered = filter_texts_by_labels(train_texts, train_labels, target_labels)\n",
    "\n",
    "# 筛选验证集中的文本和标签\n",
    "val_texts_filtered, val_labels_filtered = filter_texts_by_labels(val_texts, val_labels, target_labels)\n",
    "\n",
    "# 筛选测试集中的文本和标签\n",
    "test_texts_filtered, test_labels_filtered = filter_texts_by_labels(test_texts, test_labels, target_labels)\n",
    "\n",
    "# 应用词形还原到你的文本数据\n",
    "train_texts_lemmatized = [lemmatize_text(text) for text in train_texts_filtered]\n",
    "val_texts_lemmatized = [lemmatize_text(text) for text in val_texts_filtered]\n",
    "test_texts_lemmatized = [lemmatize_text(text) for text in test_texts_filtered]\n",
    "docs= train_texts_lemmatized + val_texts_lemmatized + test_texts_lemmatized\n",
    "len(docs)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28719d0c7cdf705a",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.2. Vectorization and Topic Modeling\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f2c98ec1ebc6fa1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "docs_tokenized = [doc.split() for doc in docs]  # 分词\n",
    "dictionary = Dictionary(docs_tokenized)\n",
    "\n",
    "# BOW\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs_tokenized]\n",
    "\n",
    "# TF-IDF\n",
    "tfidf = TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "790402f5f9304477",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 构建LDA模型\n",
    "# 使用词袋表示训练LDA\n",
    "# lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=5, passes=30) # num_topics是主题数, passes是迭代次数\n",
    "\n",
    "# 或者使用TF-IDF表示训练LDA\n",
    "lda_model_tfidf = LdaModel(corpus=corpus_tfidf, id2word=dictionary, num_topics=5, passes=30)\n",
    "\n",
    "topics = lda_model_tfidf.print_topics()\n",
    "for topic in topics:\n",
    "    print(topic)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be7fb75a0bad5104",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 评估模型\n",
    "# 使用文档词袋表示计算一致性得分\n",
    "coherence_model_lda = CoherenceModel(model=lda_model_tfidf, texts=docs_tokenized, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "\n",
    "print(f'Coherence Score: {coherence_lda}')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "468d852e68599149",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 假设lda_model是你的LDA模型实例\n",
    "num_topics = 5  # 你的模型的主题数量\n",
    "topn = 10  # 你希望从每个主题中提取的关键词数量\n",
    "\n",
    "topic_keywords = {}  # 初始化一个空字典来存储关键词及其权重\n",
    "\n",
    "# 循环遍历每个主题\n",
    "for topic_id in range(num_topics):\n",
    "    # 使用show_topic获取当前主题的前N个重要单词及其权重\n",
    "    for word, weight in lda_model_tfidf.show_topic(topic_id, topn=topn):\n",
    "        # 为了生成词云，我们将每个单词的权重累加（如果单词在多个主题中出现）\n",
    "        if word in topic_keywords:\n",
    "            topic_keywords[word] += weight\n",
    "        else:\n",
    "            topic_keywords[word] = weight\n",
    "\n",
    "# 确保topic_keywords字典包含了你想要显示在词云中的关键词和累计权重\n",
    "# 生成词云\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(topic_keywords)\n",
    "\n",
    "# 显示词云\n",
    "plt.figure(figsize=(8, 5))  # 设置图形的显示大小\n",
    "plt.imshow(wordcloud, interpolation='bilinear')  # 使用双线性插值显示更平滑的图像\n",
    "plt.axis('off')  # 不显示坐标轴\n",
    "\n",
    "plt.savefig('./img/wordcloud.png')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b38edb4c1bf78d54",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
